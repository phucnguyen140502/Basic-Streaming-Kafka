[2024-12-17 16:09:19,629] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2024-12-17 16:13:25,591] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 16:13:25,606] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mongodb-source-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 16:13:25,609] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:13:25,610] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:13:25,629] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=651, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:13:25,645] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=651, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:13:25,647] INFO [mongodb-source-connector|worker] Stopping connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 16:13:25,647] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 16:13:25,648] INFO [mongodb-source-connector|worker] Scheduled shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 16:13:25,650] INFO [mongodb-source-connector|worker] Stopping MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:82)
[2024-12-17 16:13:25,651] INFO [mongodb-source-connector|worker] Stopped MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:86)
[2024-12-17 16:13:25,678] INFO [mongodb-source-connector|worker] Completed shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 16:13:25,686] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 16:13:25,698] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 16:13:25,699] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 651 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1049, connectorIds=[mysql-sink-connector], taskIds=[mysql-sink-connector-0], revokedConnectorIds=[mongodb-source-connector], revokedTaskIds=[mongodb-source-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:13:25,702] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1049 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:13:25,702] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:13:25,703] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:13:25,704] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:13:25,713] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=652, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:13:25,719] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=652, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:13:25,720] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 652 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1050, connectorIds=[mysql-sink-connector], taskIds=[mysql-sink-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:13:25,722] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 16:13:25,722] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 1049 is behind group assignment 1050, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 16:13:25,737] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 1050 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 16:13:25,738] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1050 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:13:25,739] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:13:47,246] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 16:13:47,247] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 16:13:47,249] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:13:47,249] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:13:47,259] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=653, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:13:47,267] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=653, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:13:47,270] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 16:13:47,271] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 16:13:47,273] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 16:13:47,276] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 16:13:47,278] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 16:13:47,279] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 16:13:47,296] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 16:13:47,297] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-fea05d39-86eb-4a60-bb5d-06978755f8b3 sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 16:13:47,303] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 16:13:47,303] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 16:13:47,357] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 16:13:47,357] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:13:47,357] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:13:47,357] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 16:13:47,375] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 16:13:47,379] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 16:13:47,381] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 16:13:47,384] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 653 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1052, connectorIds=[], taskIds=[], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:13:47,385] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1052 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:13:47,386] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:13:47,387] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:13:47,387] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:13:47,391] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=654, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:13:47,396] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=654, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:13:47,397] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 654 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1052, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:13:47,398] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1052 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:13:47,398] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:17:27,135] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mongodb-source-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 16:17:27,155] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:17:27,156] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:17:27,162] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=655, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:17:27,180] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=655, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:17:27,181] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 655 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1053, connectorIds=[mongodb-source-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:17:27,182] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1053 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:17:27,184] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mongodb-source-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 16:17:27,187] INFO [mongodb-source-connector|worker] Creating connector mongodb-source-connector of type io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 16:17:27,192] INFO [mongodb-source-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-17 16:17:27,204] INFO [mongodb-source-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [_id, name, age, address, email, phoneNumber, profession, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:17:27,213] INFO [mongodb-source-connector|worker] Instantiated connector mongodb-source-connector with version 3.0.4.Final of type class io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 16:17:27,227] INFO [mongodb-source-connector|worker] Finished creating connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 16:17:27,228] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:17:27,232] INFO [mongodb-source-connector|worker] Successfully started MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:67)
[2024-12-17 16:17:27,241] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mongodb-source-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 16:17:27,243] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:17:27,244] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:17:27,248] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=656, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:17:27,253] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=656, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:17:27,254] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 656 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1055, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:17:27,255] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1055 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:17:27,257] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 16:17:27,258] INFO [mongodb-source-connector|task-0] Creating task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 16:17:27,260] INFO [mongodb-source-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 16:17:27,265] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [_id, name, age, address, email, phoneNumber, profession, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:17:27,266] INFO [mongodb-source-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mongodb.MongoDbConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 16:17:27,285] INFO [mongodb-source-connector|task-0] Instantiated task mongodb-source-connector-0 with version 3.0.4.Final of type io.debezium.connector.mongodb.MongoDbConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 16:17:27,285] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:17:27,288] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:17:27,288] INFO [mongodb-source-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 16:17:27,288] INFO [mongodb-source-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 16:17:27,288] INFO [mongodb-source-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 16:17:27,295] WARN [mongodb-source-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 16:17:27,296] WARN [mongodb-source-connector|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 16:17:27,297] INFO [mongodb-source-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 16:17:27,298] INFO [mongodb-source-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-17 16:17:27,300] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [_id, name, age, address, email, phoneNumber, profession, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:17:27,301] INFO [mongodb-source-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongodb-source-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-17 16:17:27,302] INFO [mongodb-source-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 16:17:27,323] INFO [mongodb-source-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-17 16:17:27,323] INFO [mongodb-source-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 16:17:27,323] INFO [mongodb-source-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 16:17:27,323] INFO [mongodb-source-connector|task-0] Kafka startTimeMs: 1734427047323 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 16:17:27,326] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:17:27,327] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-17 16:17:27,328] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [_id, name, age, address, email, phoneNumber, profession, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:17:27,331] INFO [mongodb-source-connector|task-0] Starting MongoDbConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:245)
[2024-12-17 16:17:27,332] INFO [mongodb-source-connector|task-0]    connector.class = io.debezium.connector.mongodb.MongoDbConnector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    collection.include.list = test.customers (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.unwrap.delete.handling.mode = drop (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.unwrap.array.encoding = array (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    mongodb.connection.string = ******** (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms = unwrap,extractNew,convertDate (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.unwrap.add.source.fields = db,collection (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    topic.prefix = fullfillment (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.convertDate.format = yyyy-MM-dd HH:mm:ss (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    mongodb.server.selection.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,333] INFO [mongodb-source-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    mongodb.connect.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    mongodb.auth.source = admin (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    transforms.convertDate.type = org.apache.kafka.connect.transforms.TimestampConverter$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    transforms.extractNew.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    transforms.extractNew.whitelist = _id,name,age,address,email,phoneNumber,profession,createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    task.class = io.debezium.connector.mongodb.MongoDbConnectorTask (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    transforms.convertDate.field = createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,334] INFO [mongodb-source-connector|task-0]    name = mongodb-source-connector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,340] INFO [mongodb-source-connector|task-0]    transforms.convertDate.target.type = string (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,340] INFO [mongodb-source-connector|task-0]    database.include.list = test (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,341] INFO [mongodb-source-connector|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,341] INFO [mongodb-source-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-17 16:17:27,341] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,342] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,343] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 16:17:27,347] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,348] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,353] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,354] INFO [mongodb-source-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2024-12-17 16:17:27,354] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,355] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,355] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,377] INFO [mongodb-source-connector|task-0] Found previous partition offset MongoDbPartition [sourcePartition={server_id=fullfillment}]: {sec=1734386093, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwQTFBRDAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2NDY1NkM2NTc0NjUwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwQTE5MDQ5NkY2QjQ1OTk5NjQwNDEwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:527)
[2024-12-17 16:17:27,379] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = SignalProcessor (io.debezium.util.Threads:270)
[2024-12-17 16:17:27,382] WARN [mongodb-source-connector|task-0] Found a not connector specific implementation io.debezium.snapshot.lock.NoLockingSupport for lock mode no_locking_support (io.debezium.snapshot.SnapshotLockProvider:82)
[2024-12-17 16:17:27,383] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,384] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,384] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,385] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-17 16:17:27,386] INFO [mongodb-source-connector|task-0] Found existing offset for at {sec=1734386093, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwQTFBRDAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2NDY1NkM2NTc0NjUwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwQTE5MDQ5NkY2QjQ1OTk5NjQwNDEwMDAwMDQAAA==} (io.debezium.connector.mongodb.connection.MongoDbConnection:202)
[2024-12-17 16:17:27,386] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,393] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@1c733c29, com.mongodb.Jep395RecordCodecProvider@63e6c603, com.mongodb.KotlinCodecProvider@354d2291]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@22720990}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-17 16:17:27,394] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-17 16:17:27,414] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,422] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=25093065, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Tue Dec 17 16:17:22 ICT 2024, lastUpdateTimeNanos=133858276077249} (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,423] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,427] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,428] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-17 16:17:27,429] INFO [mongodb-source-connector|task-0] Exception in monitor thread while connecting to server linux-ip-147:27017 (org.mongodb.driver.cluster:76)
com.mongodb.MongoSocketException: linux-ip-147: Temporary failure in name resolution
	at com.mongodb.ServerAddress.getSocketAddresses(ServerAddress.java:221)
	at com.mongodb.internal.connection.ServerAddressWithResolver.getSocketAddresses(ServerAddressWithResolver.java:68)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:100)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:211)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:196)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:156)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: linux-ip-147: Temporary failure in name resolution
	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:934)
	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1543)
	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:852)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at com.mongodb.ServerAddress.getSocketAddresses(ServerAddress.java:213)
	... 7 more
[2024-12-17 16:17:27,931] INFO [mongodb-source-connector|task-0] Exception in monitor thread while connecting to server linux-ip-147:27017 (org.mongodb.driver.cluster:76)
com.mongodb.MongoSocketException: linux-ip-147
	at com.mongodb.ServerAddress.getSocketAddresses(ServerAddress.java:221)
	at com.mongodb.internal.connection.ServerAddressWithResolver.getSocketAddresses(ServerAddressWithResolver.java:68)
	at com.mongodb.internal.connection.SocketStream.initializeSocket(SocketStream.java:100)
	at com.mongodb.internal.connection.SocketStream.open(SocketStream.java:78)
	at com.mongodb.internal.connection.InternalStreamConnection.open(InternalStreamConnection.java:211)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.lookupServerDescription(DefaultServerMonitor.java:196)
	at com.mongodb.internal.connection.DefaultServerMonitor$ServerMonitorRunnable.run(DefaultServerMonitor.java:156)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: linux-ip-147
	at java.base/java.net.InetAddress$CachedAddresses.get(InetAddress.java:801)
	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1533)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1385)
	at java.base/java.net.InetAddress.getAllByName(InetAddress.java:1306)
	at com.mongodb.ServerAddress.getSocketAddresses(ServerAddress.java:213)
	... 7 more
[2024-12-17 16:17:37,327] INFO [mongodb-source-connector|task-0|offsets] Couldn't commit processed log positions with the source database due to a concurrent connector shutdown or restart (io.debezium.connector.common.BaseSourceTask:499)
[2024-12-17 16:17:37,416] ERROR [mongodb-source-connector|task-0] Error while attempting to Checking change stream: Timed out after 10000 ms while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=REPLICA_SET, servers=[{address=linux-ip-147:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: linux-ip-147}, caused by {java.net.UnknownHostException: linux-ip-147}}] (io.debezium.connector.mongodb.connection.MongoDbConnections:52)
com.mongodb.MongoTimeoutException: Timed out after 10000 ms while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=REPLICA_SET, servers=[{address=linux-ip-147:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: linux-ip-147}, caused by {java.net.UnknownHostException: linux-ip-147}}]
	at com.mongodb.internal.connection.BaseCluster.createTimeoutException(BaseCluster.java:380)
	at com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:125)
	at com.mongodb.internal.connection.AbstractMultiServerCluster.selectServer(AbstractMultiServerCluster.java:54)
	at com.mongodb.internal.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:116)
	at com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)
	at com.mongodb.client.internal.ClientSessionBinding.getReadConnectionSource(ClientSessionBinding.java:92)
	at com.mongodb.internal.operation.SyncOperationHelper.withReadConnectionSource(SyncOperationHelper.java:97)
	at com.mongodb.internal.operation.ChangeStreamOperation.execute(ChangeStreamOperation.java:185)
	at com.mongodb.internal.operation.ChangeStreamOperation.execute(ChangeStreamOperation.java:54)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:153)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.execute(ChangeStreamIterableImpl.java:212)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.cursor(ChangeStreamIterableImpl.java:187)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.lambda$isValidResumeToken$10(MongoDbConnection.java:219)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.execute(MongoDbConnection.java:105)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.isValidResumeToken(MongoDbConnection.java:215)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.validateLogPosition(MongoDbConnection.java:205)
	at io.debezium.connector.mongodb.MongoDbConnectorTask.validate(MongoDbConnectorTask.java:292)
	at io.debezium.connector.mongodb.MongoDbConnectorTask.start(MongoDbConnectorTask.java:137)
	at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:251)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.initializeAndStart(AbstractWorkerSourceTask.java:279)
	at org.apache.kafka.connect.runtime.WorkerTask.doStart(WorkerTask.java:176)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:79)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[2024-12-17 16:17:37,417] ERROR [mongodb-source-connector|task-0] WorkerSourceTask{id=mongodb-source-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
io.debezium.DebeziumException: Error while attempting to Checking change stream
	at io.debezium.connector.mongodb.connection.MongoDbConnections.lambda$eventSourcingErrorHandler$1(MongoDbConnections.java:53)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.execute(MongoDbConnection.java:111)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.isValidResumeToken(MongoDbConnection.java:215)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.validateLogPosition(MongoDbConnection.java:205)
	at io.debezium.connector.mongodb.MongoDbConnectorTask.validate(MongoDbConnectorTask.java:292)
	at io.debezium.connector.mongodb.MongoDbConnectorTask.start(MongoDbConnectorTask.java:137)
	at io.debezium.connector.common.BaseSourceTask.start(BaseSourceTask.java:251)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.initializeAndStart(AbstractWorkerSourceTask.java:279)
	at org.apache.kafka.connect.runtime.WorkerTask.doStart(WorkerTask.java:176)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.AbstractWorkerSourceTask.run(AbstractWorkerSourceTask.java:79)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.mongodb.MongoTimeoutException: Timed out after 10000 ms while waiting for a server that matches ReadPreferenceServerSelector{readPreference=primary}. Client view of cluster state is {type=REPLICA_SET, servers=[{address=linux-ip-147:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: linux-ip-147}, caused by {java.net.UnknownHostException: linux-ip-147}}]
	at com.mongodb.internal.connection.BaseCluster.createTimeoutException(BaseCluster.java:380)
	at com.mongodb.internal.connection.BaseCluster.selectServer(BaseCluster.java:125)
	at com.mongodb.internal.connection.AbstractMultiServerCluster.selectServer(AbstractMultiServerCluster.java:54)
	at com.mongodb.internal.binding.ClusterBinding.getReadConnectionSource(ClusterBinding.java:116)
	at com.mongodb.client.internal.ClientSessionBinding.getConnectionSource(ClientSessionBinding.java:128)
	at com.mongodb.client.internal.ClientSessionBinding.getReadConnectionSource(ClientSessionBinding.java:92)
	at com.mongodb.internal.operation.SyncOperationHelper.withReadConnectionSource(SyncOperationHelper.java:97)
	at com.mongodb.internal.operation.ChangeStreamOperation.execute(ChangeStreamOperation.java:185)
	at com.mongodb.internal.operation.ChangeStreamOperation.execute(ChangeStreamOperation.java:54)
	at com.mongodb.client.internal.MongoClientDelegate$DelegateOperationExecutor.execute(MongoClientDelegate.java:153)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.execute(ChangeStreamIterableImpl.java:212)
	at com.mongodb.client.internal.ChangeStreamIterableImpl.cursor(ChangeStreamIterableImpl.java:187)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.lambda$isValidResumeToken$10(MongoDbConnection.java:219)
	at io.debezium.connector.mongodb.connection.MongoDbConnection.execute(MongoDbConnection.java:105)
	... 16 more
[2024-12-17 16:17:37,418] INFO [mongodb-source-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:432)
[2024-12-17 16:17:37,418] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-17 16:17:37,430] INFO [mongodb-source-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 16:17:37,431] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:17:37,431] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:17:37,431] INFO [mongodb-source-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 16:17:37,432] INFO [mongodb-source-connector|task-0] App info kafka.producer for connector-producer-mongodb-source-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 16:21:01,604] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 16:21:01,633] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 16:21:01,654] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:21:01,655] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:21:01,673] INFO 172.30.2.207 - - [17/Dec/2024:09:21:01 +0000] "POST /connectors HTTP/1.1" 201 1279 "-" "curl/7.81.0" 109 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 16:21:01,677] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=657, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:21:01,689] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=657, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:21:01,691] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 657 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1056, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:21:01,701] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1056 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:21:01,702] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 16:21:01,704] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 16:21:01,707] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:21:01,722] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:21:01,735] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 16:21:01,740] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 16:21:01,741] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:21:01,748] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:21:01,757] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:21:01,790] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 16:21:01,799] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:21:01,799] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:21:01,806] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=658, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:21:01,811] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=658, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:21:01,812] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 658 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1058, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:21:01,812] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1058 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:21:01,813] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 16:21:01,813] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 16:21:01,815] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 16:21:01,816] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:21:01,816] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 16:21:01,819] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 16:21:01,819] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 16:21:01,819] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:21:01,820] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:21:01,820] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 16:21:01,820] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 16:21:01,820] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 16:21:01,822] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 16:21:01,823] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 16:21:01,824] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:21:01,825] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:21:01,826] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 16:21:01,827] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 16:21:01,834] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 16:21:01,834] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 16:21:01,834] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 16:21:01,835] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734427261834 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 16:21:01,837] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:21:01,838] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 16:21:01,839] ERROR [mysql-sink-connector|task-0] The 'collection.name.format' value is invalid: Warning: Using deprecated config option "table.name.format". (io.debezium.connector.jdbc.JdbcSinkConnectorTask:1969)
[2024-12-17 16:21:01,840] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 16:21:01,840] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,840] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,840] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    table.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,841] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,age,address,email,phoneNumber,profession,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,842] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:21:01,856] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 16:21:01,867] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 16:21:01,868] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 16:21:01,871] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 16:21:01,871] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 16:21:01,871] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 16:21:01,922] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 16:21:01,929] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@13b5c668 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@e76a97a5 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|4406692e, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@252a6320 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|7b4b3759, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|48084c64, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 16:21:02,060] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 16:21:02,063] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 16:21:02,071] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 16:21:02,073] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 16:21:02,073] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 16:21:02,075] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 16:21:02,086] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 16:21:02,087] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 16:21:02,093] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 16:21:02,100] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-d41881a6-ad62-4ab3-9f88-7542fa9af450 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 16:21:02,102] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 16:21:02,106] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=23, memberId='connector-consumer-mysql-sink-connector-0-d41881a6-ad62-4ab3-9f88-7542fa9af450', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 16:21:02,108] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 23: {connector-consumer-mysql-sink-connector-0-d41881a6-ad62-4ab3-9f88-7542fa9af450=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 16:21:02,111] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=23, memberId='connector-consumer-mysql-sink-connector-0-d41881a6-ad62-4ab3-9f88-7542fa9af450', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 16:21:02,112] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 16:21:02,113] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 16:21:02,116] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=36, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 16:25:24,412] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 16:25:24,416] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 16:25:24,419] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:24,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:24,430] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=659, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:24,441] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=659, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:24,443] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 16:25:24,443] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 16:25:24,446] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 16:25:24,448] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 16:25:24,450] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 16:25:24,454] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 16:25:24,463] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 16:25:24,465] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-d41881a6-ad62-4ab3-9f88-7542fa9af450 sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 16:25:24,475] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 16:25:24,477] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 16:25:24,753] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 16:25:24,754] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:25:24,754] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:25:24,754] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 16:25:24,759] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 16:25:24,762] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 16:25:24,765] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 16:25:24,766] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 659 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1060, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:24,766] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1060 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:24,767] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:25:24,767] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:24,768] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:24,771] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=660, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:24,774] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=660, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:24,776] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 660 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1060, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:24,777] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1060 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:24,777] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:25:31,295] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 16:25:31,306] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 16:25:31,330] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:31,331] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:31,336] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=661, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:31,345] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=661, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:31,346] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 661 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1061, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:31,347] INFO 172.30.2.207 - - [17/Dec/2024:09:25:31 +0000] "POST /connectors HTTP/1.1" 201 1279 "-" "curl/7.81.0" 88 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 16:25:31,347] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1061 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:31,350] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 16:25:31,352] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 16:25:31,354] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:25:31,362] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:25:31,363] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 16:25:31,364] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 16:25:31,370] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:25:31,384] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:25:31,386] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:25:31,397] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 16:25:31,417] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 16:25:31,421] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:31,422] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:31,426] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=662, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:31,431] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=662, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:31,432] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 662 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1065, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:31,433] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1065 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:31,434] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 16:25:31,435] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 16:25:31,437] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 16:25:31,439] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:25:31,439] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 16:25:31,440] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 16:25:31,441] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 16:25:31,441] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:25:31,442] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 16:25:31,442] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 16:25:31,442] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 16:25:31,443] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 16:25:31,446] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 16:25:31,450] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 16:25:31,452] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 16:25:31,455] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 16:25:31,461] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 16:25:31,465] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 16:25:31,483] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 16:25:31,484] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 16:25:31,485] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 16:25:31,485] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734427531484 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 16:25:31,490] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 16:25:31,492] ERROR [mysql-sink-connector|task-0] The 'collection.name.format' value is invalid: Warning: Using deprecated config option "table.name.format". (io.debezium.connector.jdbc.JdbcSinkConnectorTask:1969)
[2024-12-17 16:25:31,493] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 16:25:31,493] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,493] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,493] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,493] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,494] INFO [mysql-sink-connector|task-0]    table.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,age,address,email,phoneNumber,profession,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,495] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,496] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,496] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,496] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,496] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,506] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,506] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 16:25:31,506] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:25:31,532] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 16:25:31,539] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 16:25:31,540] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 16:25:31,541] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 16:25:31,542] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 16:25:31,542] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 16:25:31,565] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 16:25:31,569] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@249ea975 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@f9ab6be7 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|7c037a56, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@41f74bd1 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|4ad9bb5c, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|70fda017, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 16:25:31,673] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 16:25:31,675] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 16:25:31,682] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 16:25:31,684] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 16:25:31,685] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 16:25:31,690] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 16:25:31,697] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 16:25:31,698] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 16:25:31,700] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 16:25:31,704] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-a0c319d0-3231-4a80-897b-5d216d5478ce (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 16:25:31,704] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 16:25:31,707] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=25, memberId='connector-consumer-mysql-sink-connector-0-a0c319d0-3231-4a80-897b-5d216d5478ce', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 16:25:31,708] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 25: {connector-consumer-mysql-sink-connector-0-a0c319d0-3231-4a80-897b-5d216d5478ce=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 16:25:31,710] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=25, memberId='connector-consumer-mysql-sink-connector-0-a0c319d0-3231-4a80-897b-5d216d5478ce', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 16:25:31,710] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 16:25:31,711] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 16:25:31,713] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=101035, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 16:25:52,134] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 16:25:52,138] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 16:25:52,140] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:52,141] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:52,149] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=663, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:52,156] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=663, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:52,157] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 16:25:52,158] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 16:25:52,158] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 16:25:52,159] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 16:25:52,160] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 16:25:52,169] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 16:25:52,170] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-a0c319d0-3231-4a80-897b-5d216d5478ce sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 16:25:52,174] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 16:25:52,174] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 16:25:52,183] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 16:25:52,341] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 16:25:52,342] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:25:52,342] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 16:25:52,342] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 16:25:52,351] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 16:25:52,355] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 16:25:52,357] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 16:25:52,357] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 663 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1067, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:52,358] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1067 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:52,358] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:25:52,358] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:25:52,358] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:25:52,360] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=664, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:25:52,364] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=664, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:25:52,365] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 664 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1067, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:25:52,366] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1067 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:25:52,366] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:48:27,192] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 16:48:27,197] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mongodb-source-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 16:48:27,201] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:48:27,202] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:48:27,213] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=665, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:48:27,224] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=665, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:48:27,226] INFO [mongodb-source-connector|worker] Stopping connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 16:48:27,227] INFO [mongodb-source-connector|worker] Scheduled shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 16:48:27,229] INFO [mongodb-source-connector|worker] Stopping MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:82)
[2024-12-17 16:48:27,229] INFO [mongodb-source-connector|worker] Stopped MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:86)
[2024-12-17 16:48:27,231] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 16:48:27,244] INFO [mongodb-source-connector|worker] Completed shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 16:48:27,248] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 16:48:27,253] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 16:48:27,254] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 665 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1069, connectorIds=[], taskIds=[], revokedConnectorIds=[mongodb-source-connector], revokedTaskIds=[mongodb-source-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:48:27,256] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1069 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:48:27,256] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 16:48:27,256] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 16:48:27,256] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 16:48:27,259] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=666, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 16:48:27,266] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=666, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 16:48:27,266] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 666 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=1069, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 16:48:27,267] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 1069 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 16:48:27,268] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
