[2024-12-17 03:00:03,571] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:00:03,629] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:00:03,630] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:00:03,630] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:00:03,648] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=536, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:00:03,651] INFO 172.30.2.207 - - [16/Dec/2024:20:00:03 +0000] "POST /connectors HTTP/1.1" 201 1260 "-" "curl/7.81.0" 116 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:00:03,660] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=536, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:00:03,662] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 536 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=849, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:00:03,663] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 849 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:00:03,664] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:00:03,665] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:00:03,668] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:00:03,676] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:00:03,679] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:00:03,680] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:00:03,680] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:00:03,703] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:00:03,717] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:00:03,748] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:00:03,757] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:00:03,758] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:00:03,758] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:00:03,761] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=537, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:00:03,766] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=537, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:00:03,767] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 537 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=853, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:00:03,767] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 853 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:00:03,768] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:01:23,177] INFO 172.30.0.4 - - [16/Dec/2024:20:01:23 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 26 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:01:23,557] INFO 172.30.0.4 - - [16/Dec/2024:20:01:23 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:01:29,739] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:01:29,740] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:01:29,742] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:01:29,742] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:01:29,752] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=538, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:01:29,761] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=538, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:01:29,763] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:01:29,764] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:01:29,768] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:01:29,782] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:01:29,785] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:01:29,786] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 538 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=855, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:01:29,792] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 855 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:01:29,794] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:01:29,795] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:01:29,800] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:01:29,806] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=539, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:01:29,813] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=539, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:01:29,814] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 539 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=855, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:01:29,821] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 855 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:01:29,822] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:03:44,140] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:03:44,180] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:03:44,181] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:03:44,182] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:03:44,199] INFO 172.30.2.207 - - [16/Dec/2024:20:03:44 +0000] "POST /connectors HTTP/1.1" 201 1260 "-" "curl/7.81.0" 91 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:03:44,207] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=540, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:03:44,217] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=540, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:03:44,219] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 540 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=856, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:03:44,220] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 856 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:03:44,222] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:03:44,223] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:03:44,225] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:03:44,233] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,238] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:03:44,241] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:03:44,241] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:03:44,256] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:03:44,265] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,307] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:03:44,309] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:03:44,309] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:03:44,314] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=541, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:03:44,319] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=541, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:03:44,320] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 541 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=858, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:03:44,321] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 858 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:03:44,322] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:03:44,323] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:03:44,326] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:03:44,331] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,332] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:03:44,336] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:03:44,336] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:03:44,337] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:03:44,340] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:03:44,341] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:03:44,341] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:03:44,341] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:03:44,354] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:03:44,357] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:03:44,358] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:03:44,360] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:03:44,366] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,370] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:03:44,380] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:03:44,384] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:03:44,415] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:03:44,416] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:03:44,417] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:03:44,418] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734379424416 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:03:44,423] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:03:44,424] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:03:44,424] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:03:44,425] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:03:44,428] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:03:44,429] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,429] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,430] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,431] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,431] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,432] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,433] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,433] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,434] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,435] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,435] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,436] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,436] INFO [mysql-sink-connector|task-0]    insert.mode = upsert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,437] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,438] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,438] INFO [mysql-sink-connector|task-0]    primary.key.mode = record_key (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,439] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,440] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,440] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,441] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,441] INFO [mysql-sink-connector|task-0]    delete.enabled = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,442] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,443] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,443] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,444] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,444] INFO [mysql-sink-connector|task-0]    primary.key.fields = _id (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,445] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,445] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,446] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,464] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:03:44,470] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:03:44,471] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:03:44,471] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:03:44,472] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:03:44,472] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:03:44,505] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:03:44,508] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@98a74f6e [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@6458b346 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|3d02706e, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@59452fde [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|7f134e34, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|1b33b20b, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:03:44,635] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:03:44,638] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:03:44,652] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:03:44,655] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:03:44,655] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:03:44,659] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:03:44,659] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:03:44,659] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:03:44,667] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:03:44,668] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:03:44,672] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:03:44,673] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:03:44,673] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:03:44,674] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:03:44,679] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:03:44,683] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:03:44,683] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:03:44,687] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=542, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:03:44,691] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=542, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:03:44,691] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 542 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=860, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:03:44,692] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 860 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:03:44,692] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:03:44,693] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:03:44,695] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:03:44,700] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,704] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:03:44,704] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:03:44,704] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:03:44,705] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:03:44,705] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:03:44,705] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:03:44,706] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:03:44,706] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:03:44,707] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:03:44,707] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:03:44,708] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:03:44,713] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:03:44,715] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:03:44,724] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:03:44,724] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:03:44,766] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:03:44,766] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:03:44,766] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:03:44,766] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734379424766 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:03:44,772] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:03:44,772] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:03:44,774] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:03:44,775] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,776] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,776] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,776] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,777] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,777] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,777] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,778] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,778] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,778] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,778] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,779] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,779] INFO [mysql-sink-connector|task-0]    insert.mode = upsert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,780] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,780] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,780] INFO [mysql-sink-connector|task-0]    primary.key.mode = record_key (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,781] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,781] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,781] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,781] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,782] INFO [mysql-sink-connector|task-0]    delete.enabled = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,782] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,782] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,783] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,783] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,783] INFO [mysql-sink-connector|task-0]    primary.key.fields = _id (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,783] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,784] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,784] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:03:44,789] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:03:44,794] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:03:44,794] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:03:44,795] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:03:44,795] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:03:44,795] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:03:44,823] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:03:44,826] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@d23061b8 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@a6dce8f3 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|167ad5c1, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@7e3ecc95 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|332d92cd, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|7860e036, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:03:44,963] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:03:44,966] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:03:44,991] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:03:44,997] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:03:44,998] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:03:45,000] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:03:45,027] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:03:45,028] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:03:45,031] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:03:45,037] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-95ca6083-0956-494a-967a-f41b3624f14d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:03:45,038] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:03:45,042] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-mysql-sink-connector-0-95ca6083-0956-494a-967a-f41b3624f14d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:03:45,043] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 1: {connector-consumer-mysql-sink-connector-0-95ca6083-0956-494a-967a-f41b3624f14d=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:03:45,048] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-mysql-sink-connector-0-95ca6083-0956-494a-967a-f41b3624f14d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:03:45,048] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:03:45,049] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:03:45,053] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Found no committed offset for partition fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2024-12-17 03:03:45,064] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting offset for partition fullfillment.test.customers-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-17 03:03:45,140] ERROR [mysql-sink-connector|task-0] Failed to process record: Failed to process a sink record (io.debezium.connector.jdbc.JdbcSinkConnectorTask:136)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:152)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Cannot write to table mongodb_customers with no key fields defined.
	at io.debezium.connector.jdbc.JdbcChangeEventSink.getSqlStatement(JdbcChangeEventSink.java:361)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:222)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 14 more
[2024-12-17 03:03:54,769] ERROR [mysql-sink-connector|task-0] JDBC sink connector failure (io.debezium.connector.jdbc.JdbcSinkConnectorTask:119)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:152)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Cannot write to table mongodb_customers with no key fields defined.
	at io.debezium.connector.jdbc.JdbcChangeEventSink.getSqlStatement(JdbcChangeEventSink.java:361)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:222)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 14 more
[2024-12-17 03:03:54,778] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: JDBC sink connector failure (org.apache.kafka.connect.runtime.WorkerSinkTask:634)
org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:152)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Cannot write to table mongodb_customers with no key fields defined.
	at io.debezium.connector.jdbc.JdbcChangeEventSink.getSqlStatement(JdbcChangeEventSink.java:361)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:222)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 14 more
[2024-12-17 03:03:54,779] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:636)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	... 11 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:152)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Cannot write to table mongodb_customers with no key fields defined.
	at io.debezium.connector.jdbc.JdbcChangeEventSink.getSqlStatement(JdbcChangeEventSink.java:361)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:222)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 14 more
[2024-12-17 03:03:54,780] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:03:54,795] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:03:54,805] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:03:54,806] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-95ca6083-0956-494a-967a-f41b3624f14d sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:03:54,809] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:03:54,810] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:03:55,113] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:03:55,115] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:03:55,116] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:03:55,117] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:03:55,142] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:09:19,601] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2024-12-17 03:09:19,607] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2024-12-17 03:14:18,879] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:14:18,883] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:14:18,887] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:14:18,888] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:14:18,903] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=543, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:14:18,919] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=543, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:14:18,922] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:14:18,923] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:14:18,925] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:14:18,933] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:14:18,938] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:14:18,969] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:14:18,970] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 543 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=864, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:14:18,971] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 864 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:14:18,971] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:14:18,972] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:14:18,972] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:14:18,976] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=544, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:14:18,984] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=544, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:14:18,985] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 544 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=864, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:14:18,985] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 864 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:14:18,986] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:15:37,138] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:15:37,199] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:15:37,201] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:15:37,202] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:15:37,209] INFO 172.30.2.207 - - [16/Dec/2024:20:15:37 +0000] "POST /connectors HTTP/1.1" 201 1227 "-" "curl/7.81.0" 106 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:37,218] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=545, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:15:37,229] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=545, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:15:37,230] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 545 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=865, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:15:37,232] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 865 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:15:37,233] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:15:37,234] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:15:37,237] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:15:37,246] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:15:37,249] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:15:37,251] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:15:37,252] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:15:37,263] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:15:37,271] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:15:37,313] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:15:37,318] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:15:37,328] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:15:37,328] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:15:37,331] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=546, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:15:37,341] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=546, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:15:37,342] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 546 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=869, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:15:37,342] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 869 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:15:37,342] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:15:50,902] INFO 172.30.0.4 - - [16/Dec/2024:20:15:50 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:52,196] INFO 172.30.0.4 - - [16/Dec/2024:20:15:52 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:52,713] INFO 172.30.0.4 - - [16/Dec/2024:20:15:52 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:52,923] INFO 172.30.0.4 - - [16/Dec/2024:20:15:52 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:53,087] INFO 172.30.0.4 - - [16/Dec/2024:20:15:53 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:15:56,815] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:15:56,817] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:15:56,819] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:15:56,819] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:15:56,832] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=547, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:15:56,839] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=547, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:15:56,843] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:15:56,843] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:15:56,846] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:15:56,852] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:15:56,853] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:15:56,854] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 547 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=870, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:15:56,856] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 870 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:15:56,861] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:15:56,862] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:15:56,870] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:15:56,882] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=548, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:15:56,889] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=548, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:15:56,889] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 548 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=871, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:15:56,890] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:15:56,890] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 870 is behind group assignment 871, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:15:56,897] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 871 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:15:56,898] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 871 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:15:56,898] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:16:58,679] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:16:58,715] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:16:58,716] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:16:58,718] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:16:58,731] INFO 172.30.2.207 - - [16/Dec/2024:20:16:58 +0000] "POST /connectors HTTP/1.1" 201 1227 "-" "curl/7.81.0" 87 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:16:58,734] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=549, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:16:58,752] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=549, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:16:58,753] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 549 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=872, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:16:58,754] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 872 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:16:58,755] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:16:58,756] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:16:58,757] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:16:58,760] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,773] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:16:58,775] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:16:58,775] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:16:58,786] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:16:58,791] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,822] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:16:58,837] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:16:58,837] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:16:58,841] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=550, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:16:58,849] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=550, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:16:58,850] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 550 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=874, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:16:58,852] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 874 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:16:58,853] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:16:58,854] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:16:58,856] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:16:58,867] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,876] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:16:58,877] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:16:58,877] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:16:58,878] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:16:58,879] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:16:58,879] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:16:58,879] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:16:58,879] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:16:58,882] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:16:58,883] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:16:58,884] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:16:58,901] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:16:58,929] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:16:58,932] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,933] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:16:58,935] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:16:58,950] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:16:58,951] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:16:58,955] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:16:58,955] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734380218950 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:16:58,959] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:16:58,959] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:16:58,960] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:16:58,960] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:16:58,961] ERROR [mysql-sink-connector|task-0] When 'delete.enabled' is set to 'true', the 'primary.key.mode' option must be set to 'record_key'. (io.debezium.connector.jdbc.JdbcSinkConnectorTask:557)
[2024-12-17 03:16:58,963] WARN [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} After being scheduled for shutdown, task threw an uncaught exception. (org.apache.kafka.connect.runtime.WorkerTask:232)
org.apache.kafka.connect.errors.ConnectException: Error configuring an instance of JdbcSinkConnectorConfig; check the logs for details
	at io.debezium.connector.jdbc.JdbcSinkConnectorConfig.validate(JdbcSinkConnectorConfig.java:406)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.start(JdbcSinkConnectorTask.java:97)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:324)
	at org.apache.kafka.connect.runtime.WorkerTask.doStart(WorkerTask.java:176)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[2024-12-17 03:16:58,963] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:16:58,964] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:16:58,966] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:16:58,966] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:16:58,966] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:16:58,966] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:16:58,969] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:16:58,971] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:16:58,971] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:16:58,974] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=551, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:16:58,977] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=551, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:16:58,977] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 551 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=876, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:16:58,977] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 876 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:16:58,978] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:16:58,978] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:16:58,979] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:16:58,980] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,981] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:16:58,981] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:16:58,981] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:16:58,981] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:16:58,982] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:16:58,982] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:16:58,982] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:16:58,982] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:16:58,983] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:16:58,983] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:16:58,983] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:16:58,986] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:16:58,989] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:16:58,990] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:16:58,991] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:16:59,000] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:16:59,000] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:16:59,000] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:16:59,000] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734380219000 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:16:59,002] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:16:59,002] ERROR [mysql-sink-connector|task-0] When 'delete.enabled' is set to 'true', the 'primary.key.mode' option must be set to 'record_key'. (io.debezium.connector.jdbc.JdbcSinkConnectorTask:557)
[2024-12-17 03:16:59,003] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: Error configuring an instance of JdbcSinkConnectorConfig; check the logs for details
	at io.debezium.connector.jdbc.JdbcSinkConnectorConfig.validate(JdbcSinkConnectorConfig.java:406)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.start(JdbcSinkConnectorTask.java:97)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:324)
	at org.apache.kafka.connect.runtime.WorkerTask.doStart(WorkerTask.java:176)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:225)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
[2024-12-17 03:16:59,003] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:16:59,003] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:16:59,004] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:16:59,004] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:16:59,004] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:16:59,004] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:16:59,009] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:16:59,009] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:17:04,907] INFO 172.30.0.4 - - [16/Dec/2024:20:17:04 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 2036 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 18 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:17:20,043] INFO 172.30.0.4 - - [16/Dec/2024:20:17:20 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 2036 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:20:20,484] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:20:20,487] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:20:20,489] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:20:20,490] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:20:20,500] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=552, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:20:20,508] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=552, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:20:20,510] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:20:20,511] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:20:20,513] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:20:20,513] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:20:20,522] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:20:20,524] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:20:20,525] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 552 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=877, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:20:20,533] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 877 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:20:20,534] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:20:20,534] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:20:20,534] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:20:20,551] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=553, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:20:20,556] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=553, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:20:20,559] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 553 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=878, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:20:20,559] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:20:20,559] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 877 is behind group assignment 878, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:20:20,585] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 878 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:20:20,585] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 878 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:20:20,586] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:21:32,849] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:21:32,878] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:21:32,880] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:21:32,881] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:21:32,889] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=554, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:21:32,893] INFO 172.30.2.207 - - [16/Dec/2024:20:21:32 +0000] "POST /connectors HTTP/1.1" 201 1238 "-" "curl/7.81.0" 78 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:21:32,898] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=554, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:21:32,900] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 554 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=879, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:21:32,901] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 879 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:21:32,905] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:21:32,905] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:21:32,907] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:21:32,908] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:32,911] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:21:32,920] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:21:32,921] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:21:32,930] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:21:32,932] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:32,957] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:21:32,989] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:21:32,989] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:21:33,005] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=555, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:21:33,011] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=555, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:21:33,011] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 555 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=881, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:21:33,013] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 881 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:21:33,013] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:21:33,015] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:21:33,018] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:21:33,024] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:33,026] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:21:33,029] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:21:33,029] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:21:33,032] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:21:33,035] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:21:33,036] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:21:33,036] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:21:33,039] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:21:33,040] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:21:33,043] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:21:33,045] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:21:33,045] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:21:33,057] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:21:33,061] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:33,063] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:21:33,064] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:21:33,095] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:21:33,095] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:21:33,095] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:21:33,095] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734380493095 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:21:33,098] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:21:33,098] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:21:33,098] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:21:33,099] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:21:33,099] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:21:33,105] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:21:33,105] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:21:33,105] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:21:33,105] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:21:33,107] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:21:33,108] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:21:33,109] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:21:33,111] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=556, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:21:33,114] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=556, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:21:33,114] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 556 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=883, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:21:33,114] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 883 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:21:33,115] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:21:33,115] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:21:33,116] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:21:33,119] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:33,123] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:21:33,123] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:21:33,123] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:21:33,123] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:21:33,124] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:21:33,124] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:21:33,124] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:21:33,124] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:21:33,127] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:21:33,128] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:21:33,128] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:21:33,129] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:21:33,131] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted, createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:21:33,136] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:21:33,140] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:21:33,151] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:21:33,151] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:21:33,151] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:21:33,152] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734380493151 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:21:33,153] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:21:33,153] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:21:33,154] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:21:33,155] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,155] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,156] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,156] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,157] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,157] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,157] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,158] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,158] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,158] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,159] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,159] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,159] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,160] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,160] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = __deleted,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,160] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,161] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,161] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,162] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,163] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,163] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,163] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,164] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,164] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,164] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,165] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,165] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,165] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:21:33,176] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:21:33,183] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:21:33,184] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:21:33,184] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:21:33,184] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:21:33,185] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:21:33,224] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:21:33,227] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@44118cfa [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@770304bf [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|54562851, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@a7f5371f [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|2492c9e6, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|716984b0, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:21:33,353] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:21:33,355] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:21:33,362] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:21:33,364] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:21:33,364] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:21:33,372] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:21:33,387] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:21:33,389] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:21:33,391] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:21:33,396] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-dcb211c4-d43a-4253-8a77-817892bd7113 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:21:33,397] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:21:33,400] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=1, memberId='connector-consumer-mysql-sink-connector-0-dcb211c4-d43a-4253-8a77-817892bd7113', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:21:33,401] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 1: {connector-consumer-mysql-sink-connector-0-dcb211c4-d43a-4253-8a77-817892bd7113=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:21:33,404] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=1, memberId='connector-consumer-mysql-sink-connector-0-dcb211c4-d43a-4253-8a77-817892bd7113', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:21:33,404] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:21:33,405] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:21:33,407] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Found no committed offset for partition fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1508)
[2024-12-17 03:21:33,415] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting offset for partition fullfillment.test.customers-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-17 03:21:40,493] INFO 172.30.0.4 - - [16/Dec/2024:20:21:40 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:28:57,581] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:28:57,584] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:28:57,588] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:28:57,599] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:28:57,612] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=557, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:28:57,627] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=557, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:28:57,629] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:28:57,629] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:28:57,631] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:28:57,633] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:28:57,636] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:28:57,638] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:28:57,651] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:28:57,653] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-dcb211c4-d43a-4253-8a77-817892bd7113 sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:28:57,654] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:28:57,655] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:28:57,666] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:28:57,666] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:28:57,667] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:28:57,667] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:28:57,692] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:28:57,709] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:28:57,718] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:28:57,718] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 557 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=885, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:28:57,721] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 885 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:28:57,721] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:28:57,722] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:28:57,722] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:28:57,739] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=558, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:28:57,746] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=558, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:28:57,747] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 558 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=885, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:28:57,749] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 885 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:28:57,749] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:28:59,381] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:28:59,413] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:28:59,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:28:59,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:28:59,426] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=559, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:28:59,426] INFO 172.30.2.207 - - [16/Dec/2024:20:28:59 +0000] "POST /connectors HTTP/1.1" 201 1228 "-" "curl/7.81.0" 70 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:28:59,432] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=559, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:28:59,433] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 559 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=886, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:28:59,434] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 886 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:28:59,435] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:28:59,438] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:28:59,440] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:28:59,448] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:28:59,451] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:28:59,460] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:28:59,460] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:28:59,463] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:28:59,468] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:28:59,498] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:28:59,514] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:28:59,522] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:28:59,524] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:28:59,532] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=560, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:28:59,539] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=560, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:28:59,540] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 560 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=890, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:28:59,541] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 890 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:28:59,542] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:29:10,663] INFO 172.30.0.4 - - [16/Dec/2024:20:29:10 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 27 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:29:15,351] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:29:15,362] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:29:15,374] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:29:15,376] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:29:15,384] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=561, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:29:15,393] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=561, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:29:15,396] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:29:15,397] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:29:15,413] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:29:15,419] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:29:15,437] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:29:15,439] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 561 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=891, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:29:15,442] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 891 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:29:15,443] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:29:15,443] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:29:15,443] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:29:15,449] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=562, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:29:15,464] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=562, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:29:15,465] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 562 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=892, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:29:15,465] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:29:15,465] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 891 is behind group assignment 892, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:29:15,472] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 892 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:29:15,473] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 892 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:29:15,473] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:24,500] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:30:24,539] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:30:24,541] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:24,542] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:24,556] INFO 172.30.2.207 - - [16/Dec/2024:20:30:24 +0000] "POST /connectors HTTP/1.1" 201 1228 "-" "curl/7.81.0" 88 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:30:24,558] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=563, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:24,569] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=563, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:24,570] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 563 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=893, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:24,571] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 893 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:24,571] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:30:24,572] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:30:24,574] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:30:24,576] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:30:24,578] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:30:24,579] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:30:24,579] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:24,590] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:30:24,606] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:30:24,652] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:30:24,653] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:24,653] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:24,657] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=564, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:24,661] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=564, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:24,663] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 564 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=896, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:24,665] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 896 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:24,666] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:30:24,667] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:30:24,670] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:30:24,676] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:30:24,683] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:30:24,685] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:30:24,687] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:30:24,696] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:30:24,698] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:30:24,699] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:30:24,699] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:30:24,699] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:30:24,703] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:30:24,710] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:30:24,711] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:30:24,712] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:30:24,713] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:30:24,716] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:30:24,717] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:30:24,719] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:30:24,737] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:30:24,738] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:30:24,738] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:30:24,738] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734381024738 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:30:24,742] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:24,743] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:24,743] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:24,745] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:30:24,746] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=565, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:24,750] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=565, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:24,751] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:30:24,756] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:30:24,758] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,758] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,758] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,759] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,760] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,761] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,762] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:30:24,784] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:30:24,793] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:30:24,794] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:30:24,795] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:30:24,795] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:30:24,796] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:30:24,844] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:30:24,848] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@9b77185a [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@b7da70f6 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|4020b3aa, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@2018a954 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|c7a2471, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|10c9311e, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:30:24,967] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:30:24,969] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:30:24,978] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:30:24,980] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:30:24,980] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:30:24,983] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:30:24,984] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:30:24,984] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:30:24,986] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:30:24,986] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:30:24,989] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:30:24,989] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:30:24,989] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:30:24,990] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:30:24,996] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:30:24,999] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:30:25,000] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:30:25,000] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 565 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=897, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:25,001] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 897 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:25,001] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:25,002] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:25,003] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:25,005] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=566, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:25,007] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=566, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:25,008] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 566 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=897, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:25,009] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 897 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:25,009] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:30,112] INFO 172.30.0.4 - - [16/Dec/2024:20:30:30 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 21 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:30:37,882] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:30:37,884] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:30:37,890] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:37,890] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:37,931] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=567, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:37,943] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=567, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:37,950] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:30:37,950] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:30:37,962] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:30:37,985] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:30:37,993] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:30:37,993] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 567 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=899, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:37,995] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:30:38,000] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 898 is behind group assignment 899, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:30:38,015] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 899 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:30:38,016] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 899 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:38,016] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:30:38,017] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:30:38,017] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:30:38,021] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=568, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:30:38,028] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=568, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:30:38,029] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 568 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=899, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:30:38,029] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 899 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:30:38,029] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:33:13,567] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:33:13,596] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:33:13,609] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:33:13,613] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:33:13,627] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=569, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:33:13,641] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=569, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:33:13,642] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 569 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=900, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:33:13,644] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 900 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:33:13,645] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:33:13,646] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:33:13,648] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:33:13,663] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:33:13,648] INFO 172.30.2.207 - - [16/Dec/2024:20:33:13 +0000] "POST /connectors HTTP/1.1" 201 1228 "-" "curl/7.81.0" 113 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:13,671] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:33:13,676] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:33:13,676] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:33:13,685] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:33:13,691] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:33:13,725] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:33:13,732] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:33:13,739] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:33:13,740] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:33:13,743] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=570, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:33:13,746] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=570, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:33:13,747] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 570 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=904, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:33:13,749] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 904 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:33:13,749] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:33:29,112] INFO 172.30.0.4 - - [16/Dec/2024:20:33:29 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:31,415] INFO 172.30.0.4 - - [16/Dec/2024:20:33:31 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 10 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:31,718] INFO 172.30.0.4 - - [16/Dec/2024:20:33:31 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:32,059] INFO 172.30.0.4 - - [16/Dec/2024:20:33:32 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:32,233] INFO 172.30.0.4 - - [16/Dec/2024:20:33:32 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:32,391] INFO 172.30.0.4 - - [16/Dec/2024:20:33:32 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 11 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:32,549] INFO 172.30.0.4 - - [16/Dec/2024:20:33:32 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:33:53,338] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:33:53,343] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:33:53,346] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:33:53,346] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:33:53,353] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=571, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:33:53,365] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=571, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:33:53,366] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:33:53,366] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:33:53,372] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:33:53,381] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:33:53,382] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:33:53,382] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 571 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=906, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:33:53,388] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:33:53,389] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 905 is behind group assignment 906, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:33:53,401] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 906 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:33:53,402] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 906 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:33:53,402] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:33:53,403] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:33:53,404] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:33:53,407] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=572, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:33:53,414] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=572, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:33:53,415] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 572 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=906, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:33:53,416] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 906 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:33:53,416] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:35:31,648] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:35:31,677] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:35:31,680] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:35:31,682] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:35:31,686] INFO 172.30.2.207 - - [16/Dec/2024:20:35:31 +0000] "POST /connectors HTTP/1.1" 201 1228 "-" "curl/7.81.0" 69 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:35:31,694] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=573, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:35:31,705] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=573, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:35:31,706] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 573 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=907, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:35:31,707] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 907 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:35:31,708] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:35:31,708] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:35:31,710] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:35:31,713] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:31,715] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:35:31,733] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:35:31,733] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:35:31,739] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:35:31,743] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:31,787] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:35:31,797] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:35:31,797] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:35:31,801] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=574, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:35:31,809] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:35:31,810] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=574, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:35:31,814] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 574 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=909, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:35:31,814] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 909 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:35:31,815] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:35:31,819] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:35:31,823] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:35:31,827] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:31,833] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:35:31,834] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:35:31,834] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:35:31,835] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:35:31,837] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:35:31,838] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:35:31,838] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:35:31,839] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:35:31,840] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:35:31,842] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:35:31,842] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:35:31,843] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:35:31,847] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:31,853] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:35:31,861] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:35:31,878] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:35:31,879] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:35:31,879] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:35:31,879] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734381331879 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:35:31,882] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:35:31,882] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:35:31,882] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:35:31,883] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:35:31,884] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:35:31,885] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,885] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,885] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,886] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,886] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,886] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,887] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,887] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,888] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,888] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,888] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,889] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,889] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,889] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,890] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,890] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,890] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,890] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,891] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,891] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,892] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,892] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,892] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,893] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,893] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,894] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,894] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,895] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:31,903] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:35:31,907] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:35:31,908] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:35:31,908] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:35:31,908] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:35:31,909] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:35:31,934] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:35:31,939] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@2dead0b0 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@d06fe2c9 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|3eff7838, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@4b7fcf5e [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|26198ce1, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|20eb281a, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:35:32,058] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:35:32,061] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:35:32,070] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:35:32,073] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:35:32,073] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:35:32,076] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:35:32,076] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:35:32,077] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:35:32,083] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:35:32,083] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:35:32,085] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:35:32,085] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:35:32,085] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:35:32,086] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:35:32,091] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:35:32,093] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:35:32,093] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:35:32,096] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=575, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:35:32,099] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=575, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:35:32,100] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 575 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=911, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:35:32,100] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 911 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:35:32,100] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:35:32,101] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:35:32,102] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:35:32,104] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:32,104] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:35:32,105] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:35:32,105] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:35:32,106] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:35:32,107] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:35:32,107] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:35:32,108] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:35:32,109] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:35:32,112] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:35:32,113] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:35:32,115] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:35:32,116] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:35:32,120] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [createdAt]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:35:32,121] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:35:32,122] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:35:32,147] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:35:32,148] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:35:32,148] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:35:32,148] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734381332148 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:35:32,153] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:35:32,154] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:35:32,154] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,154] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,155] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,156] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,157] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,158] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,158] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,158] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,159] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,159] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:35:32,170] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:35:32,171] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:35:32,174] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:35:32,175] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:35:32,176] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:35:32,176] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:35:32,177] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:35:32,222] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:35:32,227] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@bb82623 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@42fab320 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|18557276, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@b4dfa7ba [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|77f789c8, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|6925e728, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:35:32,344] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:35:32,346] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:35:32,355] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:35:32,356] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:35:32,357] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:35:32,361] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:35:32,374] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:35:32,376] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:35:32,379] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:35:32,385] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-79d5c3c2-c8e4-49a8-b42e-9c56e47fda9b (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:35:32,386] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:35:32,396] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=3, memberId='connector-consumer-mysql-sink-connector-0-79d5c3c2-c8e4-49a8-b42e-9c56e47fda9b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:35:32,397] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 3: {connector-consumer-mysql-sink-connector-0-79d5c3c2-c8e4-49a8-b42e-9c56e47fda9b=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:35:32,400] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=3, memberId='connector-consumer-mysql-sink-connector-0-79d5c3c2-c8e4-49a8-b42e-9c56e47fda9b', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:35:32,401] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:35:32,401] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:35:32,412] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=17, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 03:35:37,220] INFO 172.30.0.4 - - [16/Dec/2024:20:35:37 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:35:46,215] INFO 172.30.0.4 - - [16/Dec/2024:20:35:46 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 13 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:35:47,495] INFO 172.30.0.4 - - [16/Dec/2024:20:35:47 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:35:48,023] INFO 172.30.0.4 - - [16/Dec/2024:20:35:48 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 5 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:35:53,373] INFO 172.30.0.4 - - [16/Dec/2024:20:35:53 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 10 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:36:59,691] INFO 172.30.0.4 - - [16/Dec/2024:20:36:59 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 24 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:37:00,226] INFO 172.30.0.4 - - [16/Dec/2024:20:37:00 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:41:50,776] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:41:50,781] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:41:50,791] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:41:50,793] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:41:50,807] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=576, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:41:50,817] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=576, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:41:50,821] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:41:50,822] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:41:50,823] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:41:50,823] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:41:50,824] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:41:50,835] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:41:50,848] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:41:50,850] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-79d5c3c2-c8e4-49a8-b42e-9c56e47fda9b sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:41:50,852] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:41:50,857] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:41:51,295] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:41:51,295] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:41:51,297] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:41:51,297] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:41:51,315] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:41:51,325] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:41:51,326] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:41:51,327] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 576 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=913, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:41:51,328] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 913 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:41:51,329] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:41:51,334] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:41:51,334] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:41:51,352] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=577, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:41:51,364] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=577, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:41:51,366] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 577 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=913, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:41:51,367] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 913 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:41:51,368] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:43:08,127] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:43:08,156] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:43:08,182] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:43:08,183] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:43:08,195] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=578, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:43:08,199] INFO 172.30.2.207 - - [16/Dec/2024:20:43:08 +0000] "POST /connectors HTTP/1.1" 201 1228 "-" "curl/7.81.0" 105 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:43:08,207] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=578, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:43:08,208] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 578 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=914, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:43:08,210] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 914 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:43:08,211] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:43:08,212] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:43:08,214] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:43:08,218] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,224] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:43:08,228] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:43:08,228] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:43:08,232] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:43:08,241] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,279] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:43:08,285] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:43:08,285] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:43:08,288] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=579, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:43:08,295] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=579, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:43:08,296] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 579 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=916, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:43:08,297] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 916 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:43:08,302] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:43:08,305] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:43:08,309] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:43:08,312] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,312] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:43:08,313] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:43:08,313] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:43:08,314] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:43:08,314] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:43:08,315] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:43:08,315] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:43:08,315] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:43:08,319] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:43:08,332] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:43:08,334] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:43:08,334] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:43:08,335] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:43:08,347] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,352] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:43:08,354] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:43:08,385] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:43:08,385] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:43:08,385] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:43:08,385] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734381788385 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:43:08,402] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:43:08,404] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:43:08,404] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:43:08,405] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:43:08,411] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:43:08,411] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,411] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,411] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,412] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,412] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,412] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,426] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,426] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,426] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,426] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = __deleted (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,427] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,428] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,428] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,428] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,438] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,438] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,438] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,439] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,439] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,439] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,439] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,439] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,490] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:43:08,497] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:43:08,497] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:43:08,498] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:43:08,498] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:43:08,498] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:43:08,529] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:43:08,537] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@d4d8d7c9 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@683cb9ee [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|2a0c154a, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@f4ec8984 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|3a89f4dd, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|3ee52e58, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:43:08,642] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:43:08,644] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:43:08,650] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:43:08,651] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:43:08,651] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:43:08,653] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:43:08,658] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:43:08,658] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:43:08,659] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:43:08,660] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:43:08,661] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:43:08,662] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:43:08,662] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:43:08,662] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:43:08,664] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:43:08,669] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:43:08,669] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:43:08,672] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=580, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:43:08,676] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=580, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:43:08,676] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 580 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=918, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:43:08,677] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 918 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:43:08,677] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:43:08,678] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:43:08,679] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:43:08,682] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,687] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:43:08,688] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:43:08,689] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:43:08,689] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:43:08,691] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:43:08,692] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:43:08,693] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:43:08,693] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:43:08,694] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:43:08,695] WARN [mysql-sink-connector|task-0] Configuration key blacklist is deprecated and may be removed in the future.  Please update your configuration to use exclude instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-17 03:43:08,695] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:43:08,696] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:43:08,700] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, dropFields]
	transforms.dropFields.blacklist = [__deleted]
	transforms.dropFields.exclude = []
	transforms.dropFields.include = []
	transforms.dropFields.negate = false
	transforms.dropFields.predicate = null
	transforms.dropFields.renames = []
	transforms.dropFields.replace.null.with.default = true
	transforms.dropFields.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.dropFields.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:43:08,705] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:43:08,711] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:43:08,729] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:43:08,730] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:43:08,730] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:43:08,730] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734381788730 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:43:08,732] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:43:08,733] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:43:08,733] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,734] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,734] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,734] INFO [mysql-sink-connector|task-0]    transforms = unwrap,dropFields (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,735] INFO [mysql-sink-connector|task-0]    transforms.dropFields.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,735] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,735] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,736] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,736] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,736] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,737] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,738] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,738] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,738] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,739] INFO [mysql-sink-connector|task-0]    transforms.dropFields.blacklist = __deleted (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,739] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,739] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,740] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,740] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,740] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,741] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,741] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,741] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,742] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,742] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,742] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,743] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,743] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:43:08,747] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:43:08,748] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:43:08,750] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:43:08,751] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:43:08,751] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:43:08,752] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:43:08,752] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:43:08,773] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:43:08,775] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@dd5538b1 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@c406b7e2 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|1a7dd43e, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@275e0527 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|70f2be1c, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|52776e04, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:43:08,920] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:43:08,923] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:43:08,941] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:43:08,943] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:43:08,944] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:43:08,946] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:43:08,956] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:43:08,957] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:43:08,968] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:43:08,972] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-6799e3de-c479-4487-9b95-0e529eba2ecb (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:43:08,973] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:43:08,976] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=5, memberId='connector-consumer-mysql-sink-connector-0-6799e3de-c479-4487-9b95-0e529eba2ecb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:43:08,977] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 5: {connector-consumer-mysql-sink-connector-0-6799e3de-c479-4487-9b95-0e529eba2ecb=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:43:08,980] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=5, memberId='connector-consumer-mysql-sink-connector-0-6799e3de-c479-4487-9b95-0e529eba2ecb', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:43:08,981] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:43:08,981] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:43:08,985] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 03:43:21,902] INFO 172.30.0.4 - - [16/Dec/2024:20:43:21 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 12 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:43:23,203] INFO 172.30.0.4 - - [16/Dec/2024:20:43:23 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 14 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:43:23,842] INFO 172.30.0.4 - - [16/Dec/2024:20:43:23 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:43:34,816] WARN [mysql-sink-connector|task-0] SQL Error: 1060, SQLState: 42S21 (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:145)
[2024-12-17 03:43:34,817] ERROR [mysql-sink-connector|task-0] Duplicate column name 'createdAt' (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:150)
[2024-12-17 03:43:34,835] ERROR [mysql-sink-connector|task-0] Failed to process record: Failed to process a sink record (io.debezium.connector.jdbc.JdbcSinkConnectorTask:136)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:43:38,735] ERROR [mysql-sink-connector|task-0] JDBC sink connector failure (io.debezium.connector.jdbc.JdbcSinkConnectorTask:119)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:43:38,737] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: JDBC sink connector failure (org.apache.kafka.connect.runtime.WorkerSinkTask:634)
org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:43:38,738] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:636)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	... 11 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:43:38,740] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:43:38,740] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:43:38,747] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:43:38,748] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-6799e3de-c479-4487-9b95-0e529eba2ecb sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:43:38,749] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:43:38,749] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:43:39,147] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:43:39,148] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:43:39,149] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:43:39,149] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:43:39,160] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:43:42,009] INFO 172.30.0.4 - - [16/Dec/2024:20:43:41 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 4731 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 15 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:46:57,724] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:46:57,729] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:46:57,734] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:46:57,736] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:46:57,751] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=581, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:46:57,761] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=581, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:46:57,763] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:46:57,763] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:46:57,763] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:46:57,769] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:46:57,780] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:46:57,781] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:46:57,782] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 581 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=920, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:46:57,785] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 920 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:46:57,790] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:46:57,790] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:46:57,790] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:46:57,794] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=582, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:46:57,800] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=582, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:46:57,801] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 582 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=920, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:46:57,802] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 920 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:46:57,802] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:47:14,383] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:47:14,402] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:47:14,412] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:47:14,413] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:47:14,432] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=583, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:47:14,435] INFO 172.30.2.207 - - [16/Dec/2024:20:47:14 +0000] "POST /connectors HTTP/1.1" 201 1253 "-" "curl/7.81.0" 85 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:47:14,442] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=583, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:47:14,443] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 583 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=921, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:47:14,446] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 921 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:47:14,446] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:47:14,447] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:47:14,449] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:47:14,455] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:47:14,457] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:47:14,458] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:47:14,458] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:47:14,466] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:47:14,479] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:47:14,524] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:47:14,531] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:47:14,544] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:47:14,545] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:47:14,563] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=584, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:47:14,572] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=584, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:47:14,573] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 584 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=925, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:47:14,574] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 925 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:47:14,575] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:47:20,515] INFO 172.30.0.4 - - [16/Dec/2024:20:47:20 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:47:25,110] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:47:25,112] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:47:25,113] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:47:25,114] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:47:25,121] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=585, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:47:25,129] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=585, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:47:25,131] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:47:25,132] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:47:25,134] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:47:25,136] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:47:25,138] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:47:25,139] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 585 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=927, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:47:25,141] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 927 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:47:25,142] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:47:25,142] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:47:25,142] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:47:25,150] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=586, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:47:25,154] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=586, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:47:25,155] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 586 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=927, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:47:25,155] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 927 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:47:25,156] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:23,397] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:50:23,426] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:50:23,427] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:23,428] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:23,438] INFO 172.30.2.207 - - [16/Dec/2024:20:50:23 +0000] "POST /connectors HTTP/1.1" 201 1253 "-" "curl/7.81.0" 75 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:23,449] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=587, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:23,459] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=587, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:23,460] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 587 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=928, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:23,461] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 928 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:23,461] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:50:23,462] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:50:23,466] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:50:23,468] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:50:23,471] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:50:23,474] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:50:23,475] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:23,486] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:50:23,501] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:50:23,580] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:50:23,598] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:23,598] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:23,603] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=588, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:23,611] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=588, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:23,612] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 588 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=931, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:23,617] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:50:23,618] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 931 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:23,618] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:50:23,619] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:50:23,622] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:50:23,625] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:50:23,625] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:50:23,626] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:50:23,626] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:50:23,627] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:50:23,627] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:50:23,628] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:50:23,628] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:50:23,631] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:50:23,636] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:50:23,638] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:50:23,639] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:50:23,640] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:50:23,641] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:50:23,642] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:50:23,674] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:50:23,675] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:50:23,675] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:50:23,675] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734382223674 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:50:23,677] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:23,678] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:23,678] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:23,681] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=589, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:23,685] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=589, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:23,686] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:50:23,686] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:50:23,687] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:50:23,687] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:50:23,687] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:50:23,688] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:50:23,692] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:50:23,695] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:50:23,704] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:50:23,704] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:50:23,705] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 589 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=932, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:23,705] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 932 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:23,705] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:23,705] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:23,706] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:23,707] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=590, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:23,713] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=590, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:23,713] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 590 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=932, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:23,713] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 932 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:23,713] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:27,674] INFO 172.30.0.4 - - [16/Dec/2024:20:50:27 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 16 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:29,959] INFO 172.30.0.4 - - [16/Dec/2024:20:50:29 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:30,037] INFO 172.30.0.4 - - [16/Dec/2024:20:50:30 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 7 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:30,099] INFO 172.30.0.4 - - [16/Dec/2024:20:50:30 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 4 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:30,173] INFO 172.30.0.4 - - [16/Dec/2024:20:50:30 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 6 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:50:35,360] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:50:35,361] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:50:35,362] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:35,362] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:35,376] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=591, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:35,386] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=591, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:35,387] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:50:35,387] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:50:35,389] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:50:35,390] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:50:35,394] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:50:35,394] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 591 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=934, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:35,399] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 934 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:35,399] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:50:35,399] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:50:35,399] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:50:35,411] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=592, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:50:35,419] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=592, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:50:35,419] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 592 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=934, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:50:35,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 934 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:50:35,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:53:12,426] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:53:12,450] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:53:12,462] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:53:12,465] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:53:12,477] INFO 172.30.2.207 - - [16/Dec/2024:20:53:12 +0000] "POST /connectors HTTP/1.1" 201 1253 "-" "curl/7.81.0" 85 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:53:12,477] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=593, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:53:12,489] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=593, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:53:12,490] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 593 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=935, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:53:12,491] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 935 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:53:12,492] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:53:12,494] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:53:12,496] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:53:12,504] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,506] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:53:12,513] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:53:12,513] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:53:12,528] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:53:12,530] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,567] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:53:12,572] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:53:12,572] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:53:12,575] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=594, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:53:12,580] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=594, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:53:12,581] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 594 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=937, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:53:12,582] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 937 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:53:12,587] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:53:12,588] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:53:12,591] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:53:12,599] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:53:12,603] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,613] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:53:12,613] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:53:12,614] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:53:12,614] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:53:12,621] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:53:12,627] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:53:12,628] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:53:12,629] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:53:12,632] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:53:12,636] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:53:12,637] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:53:12,644] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,647] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:53:12,649] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:53:12,680] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:53:12,681] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:53:12,681] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:53:12,682] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734382392680 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:53:12,692] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:53:12,693] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:53:12,693] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:53:12,693] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:53:12,697] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,698] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,699] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,email,age,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,700] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,701] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:12,721] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:53:12,728] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:53:12,729] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:53:12,730] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:53:12,731] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:53:12,732] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:53:12,775] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:53:12,780] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@82fe5d21 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@c8aede14 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|2d23a979, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@47e8019f [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|8f340c, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|49f24ddc, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:53:12,902] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:53:12,904] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:53:12,915] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:53:12,921] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:53:12,921] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:53:12,928] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:53:12,929] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:53:12,930] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:53:12,933] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:53:12,934] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:53:12,935] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:53:12,935] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:53:12,936] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:53:12,936] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:53:12,938] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:53:12,945] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:53:12,945] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:53:12,948] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=595, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:53:12,954] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=595, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:53:12,955] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 595 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=939, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:53:12,955] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 939 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:53:12,956] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:53:12,957] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:53:12,958] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:53:12,961] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,965] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:53:12,966] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:53:12,966] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:53:12,967] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:53:12,968] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:53:12,968] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:53:12,969] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:53:12,969] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:53:12,971] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:53:12,972] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:53:12,973] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:53:12,976] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:53:12,980] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:53:12,983] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:53:12,996] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:53:12,996] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:53:12,997] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:53:12,997] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734382392996 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:53:13,000] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:53:13,000] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,001] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,email,age,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,002] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,003] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:53:13,007] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:53:13,010] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:53:13,010] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:53:13,011] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:53:13,011] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:53:13,011] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:53:13,034] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:53:13,036] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@f3f71796 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@fe3ab87e [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|5b05269f, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@6650f4b7 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|1c0c2733, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|2e79a94, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:53:13,126] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:53:13,127] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:53:13,132] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:53:13,133] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:53:13,134] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:53:13,135] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:53:13,182] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:53:13,187] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:53:13,188] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:53:13,200] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-278a4392-8d45-42e5-a711-e40b87682f66 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:53:13,202] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:53:13,206] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=7, memberId='connector-consumer-mysql-sink-connector-0-278a4392-8d45-42e5-a711-e40b87682f66', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:53:13,208] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 7: {connector-consumer-mysql-sink-connector-0-278a4392-8d45-42e5-a711-e40b87682f66=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:53:13,211] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=7, memberId='connector-consumer-mysql-sink-connector-0-278a4392-8d45-42e5-a711-e40b87682f66', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:53:13,211] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:53:13,212] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:53:13,215] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 03:53:13,280] WARN [mysql-sink-connector|task-0] SQL Error: 1060, SQLState: 42S21 (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:145)
[2024-12-17 03:53:13,281] ERROR [mysql-sink-connector|task-0] Duplicate column name 'createdAt' (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:150)
[2024-12-17 03:53:13,284] ERROR [mysql-sink-connector|task-0] Failed to process record: Failed to process a sink record (io.debezium.connector.jdbc.JdbcSinkConnectorTask:136)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:53:19,702] INFO 172.30.0.4 - - [16/Dec/2024:20:53:19 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:53:23,001] ERROR [mysql-sink-connector|task-0] JDBC sink connector failure (io.debezium.connector.jdbc.JdbcSinkConnectorTask:119)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:53:23,012] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: JDBC sink connector failure (org.apache.kafka.connect.runtime.WorkerSinkTask:634)
org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:53:23,014] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:636)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	... 11 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:53:23,016] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:53:23,017] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:53:23,030] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:53:23,034] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-278a4392-8d45-42e5-a711-e40b87682f66 sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:53:23,042] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:53:23,042] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:53:23,257] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:53:23,258] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:53:23,258] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:53:23,259] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:53:23,269] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:53:43,167] INFO 172.30.0.4 - - [16/Dec/2024:20:53:43 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 4731 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 22 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:55:14,731] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:55:14,734] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:55:14,740] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:55:14,742] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:55:14,756] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=596, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:55:14,767] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=596, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:55:14,769] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:55:14,770] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:55:14,769] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:55:14,787] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:55:14,789] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:55:14,799] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:55:14,800] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 596 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=940, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:55:14,802] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 940 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:55:14,805] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:55:14,805] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:55:14,806] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:55:14,812] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=597, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:55:14,819] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=597, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:55:14,820] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 597 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=941, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:55:14,821] WARN [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-17 03:55:14,822] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Current config state offset 940 is behind group assignment 941, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-17 03:55:14,830] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 941 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-17 03:55:14,830] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 941 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:55:14,831] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:56:20,482] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:56:20,528] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:56:20,531] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:56:20,531] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:56:20,543] INFO 172.30.2.207 - - [16/Dec/2024:20:56:20 +0000] "POST /connectors HTTP/1.1" 201 1253 "-" "curl/7.81.0" 99 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:56:20,549] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=598, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:56:20,559] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=598, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:56:20,560] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 598 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=942, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:56:20,562] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 942 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:56:20,563] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:56:20,564] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:56:20,566] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:56:20,574] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:56:20,582] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:56:20,589] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:56:20,595] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:56:20,601] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:56:20,613] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:56:20,628] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:56:20,643] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:56:20,648] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:56:20,648] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:56:20,651] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=599, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:56:20,655] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=599, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:56:20,657] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 599 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=946, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:56:20,658] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 946 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:56:20,659] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:56:26,505] INFO 172.30.0.4 - - [16/Dec/2024:20:56:26 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 120 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 8 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:56:32,351] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-17 03:56:32,351] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-17 03:56:32,352] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:56:32,352] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:56:32,357] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=600, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:56:32,362] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=600, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:56:32,363] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-17 03:56:32,363] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-17 03:56:32,365] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-17 03:56:32,375] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-17 03:56:32,380] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-17 03:56:32,380] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 600 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=948, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:56:32,386] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 948 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:56:32,387] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:56:32,388] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:56:32,389] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:56:32,406] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=601, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:56:32,417] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=601, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:56:32,418] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 601 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=948, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:56:32,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 948 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:56:32,420] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:58:36,292] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-17 03:58:36,322] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-17 03:58:36,330] INFO 172.30.2.207 - - [16/Dec/2024:20:58:36 +0000] "POST /connectors HTTP/1.1" 201 1253 "-" "curl/7.81.0" 69 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:58:36,331] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:58:36,333] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:58:36,344] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=602, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:58:36,355] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=602, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:58:36,356] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 602 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=949, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:58:36,358] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 949 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:58:36,359] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-17 03:58:36,360] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-17 03:58:36,361] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:58:36,363] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,365] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-17 03:58:36,367] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-17 03:58:36,375] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:58:36,391] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:58:36,396] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,426] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:58:36,432] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:58:36,432] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:58:36,438] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=603, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:58:36,443] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=603, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:58:36,444] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 603 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=951, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:58:36,446] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-17 03:58:36,446] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 951 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:58:36,449] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:58:36,450] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:58:36,452] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:58:36,454] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,454] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:58:36,455] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:58:36,455] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:58:36,456] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:58:36,458] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:58:36,459] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:58:36,459] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:58:36,459] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:58:36,468] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:58:36,488] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:58:36,490] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:58:36,492] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,494] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:58:36,497] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:58:36,531] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:58:36,532] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:58:36,532] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:58:36,533] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734382716532 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:58:36,541] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:58:36,541] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:58:36,542] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-17 03:58:36,542] INFO [mysql-sink-connector|task-0] Stopping task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-17 03:58:36,545] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:58:36,546] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,546] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,546] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,547] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,547] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,547] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,548] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,548] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,548] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,549] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,549] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,549] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,549] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,550] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,550] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,550] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,email,age,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,551] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,551] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,551] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,551] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,552] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,552] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,552] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,552] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,553] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,553] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,553] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,554] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,567] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:58:36,571] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:58:36,572] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:58:36,573] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:58:36,573] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:58:36,574] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:58:36,600] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:58:36,602] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@829b46ff [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@6f5c63c9 [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|3bfb41f6, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@d237c3a4 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|78614ec3, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|25006830, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:58:36,720] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:58:36,721] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:58:36,734] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:58:36,737] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:58:36,738] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:58:36,739] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:58:36,740] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:58:36,743] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:58:36,749] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:58:36,751] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:58:36,752] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:58:36,753] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:58:36,754] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:58:36,754] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:58:36,758] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:58:36,760] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-17 03:58:36,762] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-17 03:58:36,766] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully joined group with generation Generation{generationId=604, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-17 03:58:36,780] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Successfully synced group in generation Generation{generationId=604, memberId='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-17 03:58:36,781] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Joined group at generation 604 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.207:8083-32a35daa-4ae8-4e3a-846b-1d294614d0ac', leaderUrl='http://172.30.2.207:8083/', offset=953, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-17 03:58:36,782] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting connectors and tasks using config offset 953 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-17 03:58:36,784] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-17 03:58:36,785] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-17 03:58:36,786] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-17 03:58:36,787] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,787] INFO [mysql-sink-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.jdbc.JdbcSinkConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] New InternalSinkRecord class found (io.debezium.connector.jdbc.JdbcSinkConnectorTask:74)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] Instantiated task mysql-sink-connector-0 with version 3.0.4.Final of type io.debezium.connector.jdbc.JdbcSinkConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-17 03:58:36,788] INFO [mysql-sink-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mysql-sink-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-17 03:58:36,789] INFO [mysql-sink-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mysql-sink-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-17 03:58:36,789] WARN [mysql-sink-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-17 03:58:36,790] INFO [mysql-sink-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-17 03:58:36,790] INFO [mysql-sink-connector|task-0] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-17 03:58:36,791] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, replaceField]
	transforms.replaceField.blacklist = null
	transforms.replaceField.exclude = []
	transforms.replaceField.include = []
	transforms.replaceField.negate = false
	transforms.replaceField.predicate = null
	transforms.replaceField.renames = []
	transforms.replaceField.replace.null.with.default = true
	transforms.replaceField.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.replaceField.whitelist = null
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-17 03:58:36,792] INFO [mysql-sink-connector|task-0] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.207:9092, 172.30.2.147:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = connector-consumer-mysql-sink-connector-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = connect-mysql-sink-connector
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-17 03:58:36,793] INFO [mysql-sink-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-17 03:58:36,806] INFO [mysql-sink-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-17 03:58:36,806] INFO [mysql-sink-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-17 03:58:36,806] INFO [mysql-sink-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-17 03:58:36,806] INFO [mysql-sink-connector|task-0] Kafka startTimeMs: 1734382716806 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-17 03:58:36,810] INFO [Worker clientId=connect-172.30.2.207:8083, groupId=mysql-sink-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-17 03:58:36,811] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Subscribed to topic(s): fullfillment.test.customers (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:481)
[2024-12-17 03:58:36,812] INFO [mysql-sink-connector|task-0] Starting JdbcSinkConnectorConfig with configuration: (io.debezium.connector.jdbc.JdbcSinkConnectorTask:410)
[2024-12-17 03:58:36,816] INFO [mysql-sink-connector|task-0]    connector.class = io.debezium.connector.jdbc.JdbcSinkConnector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,816] INFO [mysql-sink-connector|task-0]    connection.password = ******** (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,816] INFO [mysql-sink-connector|task-0]    tasks.max = 1 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,816] INFO [mysql-sink-connector|task-0]    transforms = unwrap,replaceField (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,816] INFO [mysql-sink-connector|task-0]    collection.name.format = mongodb_customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    auto.evolve = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    schema.evolution = basic (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    driver.class = com.mysql.cj.jdbc.Driver (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,817] INFO [mysql-sink-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    insert.mode = insert (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    transforms.replaceField.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    primary.key.mode = none (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    transforms.replaceField.whiteList = _id,name,email,age,createdAt (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    topics = fullfillment.test.customers (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    batch.size = 1000 (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,818] INFO [mysql-sink-connector|task-0]    connection.username = phuc (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    delete.enabled = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    task.class = io.debezium.connector.jdbc.JdbcSinkConnectorTask (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    name = mysql-sink-connector (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    auto.create = false (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    connection.url = jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,819] INFO [mysql-sink-connector|task-0]    transforms.unwrap.handle.duplicate.fields = skip (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,820] INFO [mysql-sink-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.jdbc.JdbcSinkConnectorTask:412)
[2024-12-17 03:58:36,856] INFO [mysql-sink-connector|task-0] HHH000026: Second-level cache disabled (org.hibernate.cache.internal.RegionFactoryInitiator:50)
[2024-12-17 03:58:36,875] INFO [mysql-sink-connector|task-0] HHH000130: Instantiating explicit connection provider: org.hibernate.c3p0.internal.C3P0ConnectionProvider (org.hibernate.engine.jdbc.connections.internal.ConnectionProviderInitiator:131)
[2024-12-17 03:58:36,875] INFO [mysql-sink-connector|task-0] HHH010002: C3P0 using driver: null at URL: jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true (org.hibernate.orm.connections.pooling.c3p0:124)
[2024-12-17 03:58:36,876] INFO [mysql-sink-connector|task-0] HHH10001001: Connection properties: {password=****, user=phuc} (org.hibernate.orm.connections.pooling.c3p0:125)
[2024-12-17 03:58:36,876] INFO [mysql-sink-connector|task-0] HHH10001003: Autocommit mode: false (org.hibernate.orm.connections.pooling.c3p0:128)
[2024-12-17 03:58:36,882] WARN [mysql-sink-connector|task-0] HHH10001006: No JDBC Driver class was specified by property `jakarta.persistence.jdbc.driver`, `hibernate.driver` or `javax.persistence.jdbc.driver` (org.hibernate.orm.connections.pooling.c3p0:131)
[2024-12-17 03:58:36,921] INFO [mysql-sink-connector|task-0] HHH10001007: JDBC isolation level: <unknown> (org.hibernate.orm.connections.pooling.c3p0:200)
[2024-12-17 03:58:36,926] INFO [mysql-sink-connector|task-0] Initializing c3p0 pool... com.mchange.v2.c3p0.PoolBackedDataSource@60b3c830 [ connectionPoolDataSource -> com.mchange.v2.c3p0.WrapperConnectionPoolDataSource@8dedc6da [ acquireIncrement -> 32, acquireRetryAttempts -> 30, acquireRetryDelay -> 1000, autoCommitOnClose -> false, automaticTestTable -> null, breakAfterAcquireFailure -> false, checkoutTimeout -> 0, connectionCustomizerClassName -> null, connectionTesterClassName -> com.mchange.v2.c3p0.impl.DefaultConnectionTester, contextClassLoaderSource -> caller, debugUnreturnedConnectionStackTraces -> false, factoryClassLocation -> null, forceIgnoreUnresolvedTransactions -> false, forceSynchronousCheckins -> false, identityToken -> z8kfsxb71lzjtqlvoexde|6b530a9e, idleConnectionTestPeriod -> 0, initialPoolSize -> 5, maxAdministrativeTaskTime -> 0, maxConnectionAge -> 0, maxIdleTime -> 0, maxIdleTimeExcessConnections -> 0, maxPoolSize -> 32, maxStatements -> 0, maxStatementsPerConnection -> 0, minPoolSize -> 5, nestedDataSource -> com.mchange.v2.c3p0.DriverManagerDataSource@8a27d6f2 [ description -> null, driverClass -> null, factoryClassLocation -> null, forceUseNamedDriverClass -> false, identityToken -> z8kfsxb71lzjtqlvoexde|73e6bacf, jdbcUrl -> jdbc:mysql://172.30.2.207:3306/demo?useSSL=false&allowPublicKeyRetrieval=true, properties -> {password=******, user=******} ], preferredTestQuery -> null, privilegeSpawnedThreads -> false, propertyCycle -> 0, statementCacheNumDeferredCloseThreads -> 0, testConnectionOnCheckin -> false, testConnectionOnCheckout -> false, unreturnedConnectionTimeout -> 0, usesTraditionalReflectiveProxies -> false; userOverrides: {} ], dataSourceName -> null, extensions -> {}, factoryClassLocation -> null, identityToken -> z8kfsxb71lzjtqlvoexde|11edb697, numHelperThreads -> 3 ] (com.mchange.v2.c3p0.impl.AbstractPoolBackedDataSource:212)
[2024-12-17 03:58:37,012] INFO [mysql-sink-connector|task-0] HHH000489: No JTA platform available (set 'hibernate.transaction.jta.platform' to enable JTA platform integration) (org.hibernate.engine.transaction.jta.platform.internal.JtaPlatformInitiator:58)
[2024-12-17 03:58:37,014] INFO [mysql-sink-connector|task-0] Using dialect io.debezium.connector.jdbc.dialect.mysql.MySqlDatabaseDialect (io.debezium.connector.jdbc.dialect.DatabaseDialectResolver:44)
[2024-12-17 03:58:37,018] INFO [mysql-sink-connector|task-0] Database TimeZone: SYSTEM (global), SYSTEM (system) (io.debezium.connector.jdbc.dialect.GeneralDatabaseDialect:114)
[2024-12-17 03:58:37,020] INFO [mysql-sink-connector|task-0] Database version 8.0.0 (io.debezium.connector.jdbc.JdbcChangeEventSink:69)
[2024-12-17 03:58:37,020] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Sink task finished initialization and start (org.apache.kafka.connect.runtime.WorkerSinkTask:325)
[2024-12-17 03:58:37,021] INFO [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Executing sink task (org.apache.kafka.connect.runtime.WorkerSinkTask:211)
[2024-12-17 03:58:37,032] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-17 03:58:37,035] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:937)
[2024-12-17 03:58:37,037] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:58:37,047] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: need to re-join with the given member-id: connector-consumer-mysql-sink-connector-0-2e4a1171-b6cc-4255-8544-6fe60ae7d44f (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:58:37,049] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:605)
[2024-12-17 03:58:37,051] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully joined group with generation Generation{generationId=9, memberId='connector-consumer-mysql-sink-connector-0-2e4a1171-b6cc-4255-8544-6fe60ae7d44f', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:666)
[2024-12-17 03:58:37,056] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Finished assignment for group at generation 9: {connector-consumer-mysql-sink-connector-0-2e4a1171-b6cc-4255-8544-6fe60ae7d44f=Assignment(partitions=[fullfillment.test.customers-0])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:664)
[2024-12-17 03:58:37,059] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Successfully synced group in generation Generation{generationId=9, memberId='connector-consumer-mysql-sink-connector-0-2e4a1171-b6cc-4255-8544-6fe60ae7d44f', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:843)
[2024-12-17 03:58:37,060] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Notifying assignor about the new Assignment(partitions=[fullfillment.test.customers-0]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:324)
[2024-12-17 03:58:37,061] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Adding newly assigned partitions: fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:58)
[2024-12-17 03:58:37,064] INFO [mysql-sink-connector|task-0] Setting offset for partition fullfillment.test.customers-0 to the committed offset FetchPosition{offset=19, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}} (org.apache.kafka.clients.consumer.internals.ConsumerUtils:209)
[2024-12-17 03:58:37,110] WARN [mysql-sink-connector|task-0] SQL Error: 1060, SQLState: 42S21 (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:145)
[2024-12-17 03:58:37,111] ERROR [mysql-sink-connector|task-0] Duplicate column name 'createdAt' (org.hibernate.engine.jdbc.spi.SqlExceptionHelper:150)
[2024-12-17 03:58:37,113] ERROR [mysql-sink-connector|task-0] Failed to process record: Failed to process a sink record (io.debezium.connector.jdbc.JdbcSinkConnectorTask:136)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:58:41,558] INFO 172.30.0.4 - - [16/Dec/2024:20:58:41 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 178 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 20 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-17 03:58:46,808] ERROR [mysql-sink-connector|task-0] JDBC sink connector failure (io.debezium.connector.jdbc.JdbcSinkConnectorTask:119)
org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:58:46,810] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted. Error: JDBC sink connector failure (org.apache.kafka.connect.runtime.WorkerSinkTask:634)
org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:58:46,812] ERROR [mysql-sink-connector|task-0] WorkerSinkTask{id=mysql-sink-connector-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:234)
org.apache.kafka.connect.errors.ConnectException: Exiting WorkerSinkTask due to unrecoverable exception.
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:636)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.poll(WorkerSinkTask.java:345)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.iteration(WorkerSinkTask.java:247)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.execute(WorkerSinkTask.java:216)
	at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:226)
	at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:281)
	at org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:238)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.kafka.connect.errors.ConnectException: JDBC sink connector failure
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:120)
	at org.apache.kafka.connect.runtime.WorkerSinkTask.deliverMessages(WorkerSinkTask.java:606)
	... 11 more
Caused by: org.apache.kafka.connect.errors.ConnectException: Failed to process a sink record
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:207)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.lambda$flushBuffers$2(JdbcChangeEventSink.java:178)
	at java.base/java.util.LinkedHashMap.forEach(LinkedHashMap.java:721)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffers(JdbcChangeEventSink.java:178)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.execute(JdbcChangeEventSink.java:157)
	at io.debezium.connector.jdbc.JdbcSinkConnectorTask.put(JdbcSinkConnectorTask.java:127)
	... 12 more
Caused by: org.hibernate.exception.SQLGrammarException: JDBC exception executing SQL [ALTER TABLE mongodb_customers ADD COLUMN ( createdAt longtext NULL)] [Duplicate column name 'createdAt'] [n/a]
	at org.hibernate.exception.internal.SQLExceptionTypeDelegate.convert(SQLExceptionTypeDelegate.java:66)
	at org.hibernate.exception.internal.StandardSQLExceptionConverter.convert(StandardSQLExceptionConverter.java:58)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:108)
	at org.hibernate.engine.jdbc.spi.SqlExceptionHelper.convert(SqlExceptionHelper.java:94)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:101)
	at org.hibernate.query.sql.internal.NativeNonSelectQueryPlanImpl.executeUpdate(NativeNonSelectQueryPlanImpl.java:76)
	at org.hibernate.query.sql.internal.NativeQueryImpl.doExecuteUpdate(NativeQueryImpl.java:833)
	at org.hibernate.query.spi.AbstractQuery.executeUpdate(AbstractQuery.java:650)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.alterTableIfNeeded(JdbcChangeEventSink.java:343)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.checkAndApplyTableChangesIfNeeded(JdbcChangeEventSink.java:265)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBuffer(JdbcChangeEventSink.java:220)
	at io.debezium.connector.jdbc.JdbcChangeEventSink.flushBufferWithRetries(JdbcChangeEventSink.java:198)
	... 17 more
Caused by: java.sql.SQLSyntaxErrorException: Duplicate column name 'createdAt'
	at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:112)
	at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:114)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeInternal(ClientPreparedStatement.java:988)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1166)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdateInternal(ClientPreparedStatement.java:1101)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeLargeUpdate(ClientPreparedStatement.java:1467)
	at com.mysql.cj.jdbc.ClientPreparedStatement.executeUpdate(ClientPreparedStatement.java:1084)
	at com.mchange.v2.c3p0.impl.NewProxyPreparedStatement.executeUpdate(NewProxyPreparedStatement.java:1502)
	at org.hibernate.sql.exec.internal.StandardJdbcMutationExecutor.execute(StandardJdbcMutationExecutor.java:87)
	... 24 more
[2024-12-17 03:58:46,812] INFO [mysql-sink-connector|task-0] Closing session. (io.debezium.connector.jdbc.JdbcChangeEventSink:235)
[2024-12-17 03:58:46,813] INFO [mysql-sink-connector|task-0] Closing the session factory (io.debezium.connector.jdbc.JdbcSinkConnectorTask:186)
[2024-12-17 03:58:46,822] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Revoke previously assigned partitions fullfillment.test.customers-0 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker:80)
[2024-12-17 03:58:46,822] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Member connector-consumer-mysql-sink-connector-0-2e4a1171-b6cc-4255-8544-6fe60ae7d44f sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1174)
[2024-12-17 03:58:46,836] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-17 03:58:46,836] INFO [mysql-sink-connector|task-0] [Consumer clientId=connector-consumer-mysql-sink-connector-0, groupId=connect-mysql-sink-connector] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-17 03:58:47,109] INFO [mysql-sink-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-17 03:58:47,110] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:58:47,110] INFO [mysql-sink-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-17 03:58:47,111] INFO [mysql-sink-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-17 03:58:47,130] INFO [mysql-sink-connector|task-0] App info kafka.consumer for connector-consumer-mysql-sink-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-17 03:58:50,084] INFO 172.30.0.4 - - [16/Dec/2024:20:58:50 +0000] "GET /connectors/mysql-sink-connector/status HTTP/1.1" 200 4731 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 9 (org.apache.kafka.connect.runtime.rest.RestServer:62)
