[2024-12-16 22:00:20,267] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,271] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,272] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,275] INFO Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,282] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@4b21361f}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:00:20,283] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,289] INFO Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=7816449, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037048989934} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,290] INFO Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,296] INFO Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1641773, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037056968318} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,297] INFO Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,299] INFO Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,301] INFO Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,365] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-16 22:00:20,391] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mongodb-source-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:00:20,393] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:00:20,394] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:00:20,400] INFO 172.30.2.147 - - [16/Dec/2024:15:00:20 +0000] "POST /connectors HTTP/1.1" 201 1484 "-" "curl/7.81.0" 162 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:00:20,422] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=315, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:00:20,428] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=315, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:00:20,429] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 315 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=513, connectorIds=[mongodb-source-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:00:20,430] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 513 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:00:20,430] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mongodb-source-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:00:20,431] INFO [mongodb-source-connector|worker] Creating connector mongodb-source-connector of type io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:00:20,433] INFO [mongodb-source-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:00:20,437] INFO [mongodb-source-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,446] INFO [mongodb-source-connector|worker] Instantiated connector mongodb-source-connector with version 3.0.4.Final of type class io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:00:20,447] INFO [mongodb-source-connector|worker] Finished creating connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:00:20,447] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:00:20,448] INFO [mongodb-source-connector|worker] Successfully started MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:67)
[2024-12-16 22:00:20,455] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:00:20,464] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,492] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mongodb-source-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:00:20,506] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:00:20,507] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:00:20,510] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=316, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:00:20,517] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=316, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:00:20,518] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 316 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=515, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:00:20,518] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 515 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:00:20,519] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:00:20,520] INFO [mongodb-source-connector|task-0] Creating task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:00:20,522] INFO [mongodb-source-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:00:20,524] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,525] INFO [mongodb-source-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mongodb.MongoDbConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-16 22:00:20,529] INFO [mongodb-source-connector|task-0] Instantiated task mongodb-source-connector-0 with version 3.0.4.Final of type io.debezium.connector.mongodb.MongoDbConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-16 22:00:20,529] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:00:20,534] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:00:20,535] INFO [mongodb-source-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-16 22:00:20,535] INFO [mongodb-source-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-16 22:00:20,535] INFO [mongodb-source-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-16 22:00:20,546] WARN [mongodb-source-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-16 22:00:20,550] WARN [mongodb-source-connector|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-16 22:00:20,556] INFO [mongodb-source-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-16 22:00:20,557] INFO [mongodb-source-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:00:20,561] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mongodb-source-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:00:20,567] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,569] INFO [mongodb-source-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongodb-source-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:00:20,570] INFO [mongodb-source-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:00:20,611] INFO [mongodb-source-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:00:20,612] INFO [mongodb-source-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:00:20,612] INFO [mongodb-source-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:00:20,612] INFO [mongodb-source-connector|task-0] Kafka startTimeMs: 1734361220612 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:00:20,630] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:00:20,630] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mongodb-source-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:00:20,631] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-16 22:00:20,632] INFO [mongodb-source-connector|task-0] Starting MongoDbConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:245)
[2024-12-16 22:00:20,638] INFO [mongodb-source-connector|task-0]    connector.class = io.debezium.connector.mongodb.MongoDbConnector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,638] INFO [mongodb-source-connector|task-0]    collection.include.list = test.customers (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,639] INFO [mongodb-source-connector|task-0]    transforms.unwrap.delete.handling.mode = drop (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,639] INFO [mongodb-source-connector|task-0]    transforms.unwrap.array.encoding = array (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,639] INFO [mongodb-source-connector|task-0]    mongodb.connection.string = ******** (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,639] INFO [mongodb-source-connector|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,639] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:00:20,645] INFO [mongodb-source-connector|task-0]    transforms = unwrap,extractNew,convertDate (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    transforms.unwrap.add.source.fields = db,collection (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    topic.prefix = fullfillment (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    transforms.convertDate.format = yyyy-MM-dd HH:mm:ss (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    mongodb.server.selection.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,656] INFO [mongodb-source-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,657] INFO [mongodb-source-connector|task-0]    mongodb.connect.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,657] INFO [mongodb-source-connector|task-0]    mongodb.auth.source = admin (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,657] INFO [mongodb-source-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,657] INFO [mongodb-source-connector|task-0]    transforms.convertDate.type = org.apache.kafka.connect.transforms.TimestampConverter$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,658] INFO [mongodb-source-connector|task-0]    transforms.extractNew.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,658] INFO [mongodb-source-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,658] INFO [mongodb-source-connector|task-0]    transforms.extractNew.whitelist = id,name,email,age,createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,658] INFO [mongodb-source-connector|task-0]    task.class = io.debezium.connector.mongodb.MongoDbConnectorTask (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,659] INFO [mongodb-source-connector|task-0]    transforms.convertDate.field = createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,659] INFO [mongodb-source-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,660] INFO [mongodb-source-connector|task-0]    name = mongodb-source-connector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,660] INFO [mongodb-source-connector|task-0]    transforms.convertDate.target.type = string (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,661] INFO [mongodb-source-connector|task-0]    database.include.list = test (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,661] INFO [mongodb-source-connector|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,662] INFO [mongodb-source-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,662] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,664] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,665] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,666] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,667] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,670] INFO [mongodb-source-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2024-12-16 22:00:20,671] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,672] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,673] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,686] INFO [mongodb-source-connector|task-0] Found previous partition offset MongoDbPartition [sourcePartition={server_id=fullfillment}]: {sec=1734294503, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:527)
[2024-12-16 22:00:20,690] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = SignalProcessor (io.debezium.util.Threads:270)
[2024-12-16 22:00:20,694] WARN [mongodb-source-connector|task-0] Found a not connector specific implementation io.debezium.snapshot.lock.NoLockingSupport for lock mode no_locking_support (io.debezium.snapshot.SnapshotLockProvider:82)
[2024-12-16 22:00:20,696] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,697] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,698] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,700] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,701] INFO [mongodb-source-connector|task-0] Found existing offset for at {sec=1734294503, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==} (io.debezium.connector.mongodb.connection.MongoDbConnection:202)
[2024-12-16 22:00:20,702] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,708] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2139737, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037468607150} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,712] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@148038b5}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:00:20,716] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:00:20,730] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,731] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,736] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1216376, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037496269485} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,736] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,737] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,740] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,813] INFO [mongodb-source-connector|task-0] Valid resume token present, so no snapshot will be performed' (io.debezium.connector.mongodb.connection.MongoDbConnection:220)
[2024-12-16 22:00:20,817] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2024-12-16 22:00:20,818] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = blocking-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:00:20,819] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-change-event-source-coordinator (io.debezium.util.Threads:287)
[2024-12-16 22:00:20,819] INFO [mongodb-source-connector|task-0] WorkerSourceTask{id=mongodb-source-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2024-12-16 22:00:20,820] INFO [mongodb-source-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:432)
[2024-12-16 22:00:20,823] INFO [mongodb-source-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2024-12-16 22:00:20,823] INFO [mongodb-source-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2024-12-16 22:00:20,825] INFO [mongodb-source-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:144)
[2024-12-16 22:00:20,825] INFO [mongodb-source-connector|task-0] According to the connector configuration, no snapshot will occur. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:151)
[2024-12-16 22:00:20,826] INFO [mongodb-source-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MongoDbOffsetContext [sourceInfo=SourceInfo [initialSync=false, collectionId=null, position=Position [ts=Timestamp{value=7448738172017573889, seconds=1734294503, inc=1}, changeStreamSessionTxnId=null, resumeToken=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==]]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2024-12-16 22:00:20,827] INFO [mongodb-source-connector|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:00:20,827] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-SignalProcessor (io.debezium.util.Threads:287)
[2024-12-16 22:00:20,828] INFO [mongodb-source-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2024-12-16 22:00:20,829] INFO [mongodb-source-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2024-12-16 22:00:20,829] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:00:20,838] INFO [mongodb-source-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:00:20,838] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:00:20,839] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:00:20,839] INFO [mongodb-source-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:00:20,840] INFO [mongodb-source-connector|task-0] App info kafka.producer for connector-producer-mongodb-source-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:00:20,843] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:00:20,843] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:00:20,847] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=317, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:00:20,851] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=317, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:00:20,851] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 317 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=517, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:00:20,851] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 517 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:00:20,851] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:00:20,852] INFO [mongodb-source-connector|task-0] Creating task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:00:20,853] INFO [mongodb-source-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:00:20,854] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,854] INFO [mongodb-source-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mongodb.MongoDbConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-16 22:00:20,855] INFO [mongodb-source-connector|task-0] Instantiated task mongodb-source-connector-0 with version 3.0.4.Final of type io.debezium.connector.mongodb.MongoDbConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-16 22:00:20,855] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:00:20,856] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:00:20,856] INFO [mongodb-source-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-16 22:00:20,856] INFO [mongodb-source-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-16 22:00:20,856] INFO [mongodb-source-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-16 22:00:20,857] WARN [mongodb-source-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-16 22:00:20,857] WARN [mongodb-source-connector|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-16 22:00:20,858] INFO [mongodb-source-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-16 22:00:20,858] INFO [mongodb-source-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:00:20,859] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:00:20,860] INFO [mongodb-source-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongodb-source-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:00:20,860] INFO [mongodb-source-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:00:20,867] INFO [mongodb-source-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:00:20,867] INFO [mongodb-source-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:00:20,867] INFO [mongodb-source-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:00:20,867] INFO [mongodb-source-connector|task-0] Kafka startTimeMs: 1734361220867 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:00:20,869] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:00:20,873] INFO [mongodb-source-connector|task-0] Starting MongoDbConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:245)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    connector.class = io.debezium.connector.mongodb.MongoDbConnector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    collection.include.list = test.customers (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    transforms.unwrap.delete.handling.mode = drop (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    transforms.unwrap.array.encoding = array (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    mongodb.connection.string = ******** (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,876] INFO [mongodb-source-connector|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    transforms = unwrap,extractNew,convertDate (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    transforms.unwrap.add.source.fields = db,collection (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    topic.prefix = fullfillment (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    transforms.convertDate.format = yyyy-MM-dd HH:mm:ss (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    mongodb.server.selection.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    mongodb.connect.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    mongodb.auth.source = admin (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,877] INFO [mongodb-source-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,883] INFO [mongodb-source-connector|task-0]    transforms.convertDate.type = org.apache.kafka.connect.transforms.TimestampConverter$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,883] INFO [mongodb-source-connector|task-0]    transforms.extractNew.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,883] INFO [mongodb-source-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    transforms.extractNew.whitelist = id,name,email,age,createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    task.class = io.debezium.connector.mongodb.MongoDbConnectorTask (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    transforms.convertDate.field = createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    name = mongodb-source-connector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    transforms.convertDate.target.type = string (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    database.include.list = test (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:00:20,884] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,885] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,885] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,879] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:00:20,886] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,899] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,899] INFO [mongodb-source-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2024-12-16 22:00:20,900] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,900] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,900] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,931] INFO [mongodb-source-connector|task-0] Found previous partition offset MongoDbPartition [sourcePartition={server_id=fullfillment}]: {sec=1734294503, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:527)
[2024-12-16 22:00:20,933] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = SignalProcessor (io.debezium.util.Threads:270)
[2024-12-16 22:00:20,935] WARN [mongodb-source-connector|task-0] Found a not connector specific implementation io.debezium.snapshot.lock.NoLockingSupport for lock mode no_locking_support (io.debezium.snapshot.SnapshotLockProvider:82)
[2024-12-16 22:00:20,936] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,937] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,938] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,939] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:20,940] INFO [mongodb-source-connector|task-0] Found existing offset for at {sec=1734294503, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==} (io.debezium.connector.mongodb.connection.MongoDbConnection:202)
[2024-12-16 22:00:20,941] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,945] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1408680, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037705628540} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,946] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@6d9c4309}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:00:20,948] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:00:20,953] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,955] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,958] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=706723, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037719027130} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,959] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,960] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:20,963] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,022] INFO [mongodb-source-connector|task-0] Valid resume token present, so no snapshot will be performed' (io.debezium.connector.mongodb.connection.MongoDbConnection:220)
[2024-12-16 22:00:21,027] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2024-12-16 22:00:21,027] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = blocking-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:00:21,028] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-change-event-source-coordinator (io.debezium.util.Threads:287)
[2024-12-16 22:00:21,028] INFO [mongodb-source-connector|task-0] WorkerSourceTask{id=mongodb-source-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2024-12-16 22:00:21,031] INFO [mongodb-source-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2024-12-16 22:00:21,031] INFO [mongodb-source-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2024-12-16 22:00:21,032] INFO [mongodb-source-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:144)
[2024-12-16 22:00:21,032] INFO [mongodb-source-connector|task-0] According to the connector configuration, no snapshot will occur. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:151)
[2024-12-16 22:00:21,033] INFO [mongodb-source-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MongoDbOffsetContext [sourceInfo=SourceInfo [initialSync=false, collectionId=null, position=Position [ts=Timestamp{value=7448738172017573889, seconds=1734294503, inc=1}, changeStreamSessionTxnId=null, resumeToken=zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==]]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2024-12-16 22:00:21,033] INFO [mongodb-source-connector|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:00:21,034] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = mongodb named = incremental-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:00:21,034] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,036] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,037] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,038] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,039] INFO [mongodb-source-connector|task-0] No incremental snapshot in progress, no action needed on start (io.debezium.connector.mongodb.snapshot.MongoDbIncrementalSnapshotChangeEventSource:262)
[2024-12-16 22:00:21,040] INFO [mongodb-source-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2024-12-16 22:00:21,041] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-SignalProcessor (io.debezium.util.Threads:287)
[2024-12-16 22:00:21,041] INFO [mongodb-source-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2024-12-16 22:00:21,042] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,042] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,043] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,045] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:00:21,046] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,049] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@6f5a06ec}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:00:21,049] INFO [mongodb-source-connector|task-0] Reading change stream (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:100)
[2024-12-16 22:00:21,050] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:00:21,053] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4940168, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037813780203} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,055] INFO [mongodb-source-connector|task-0] Resuming streaming from token 'zQAAAAJfZGF0YQC9AAAAODI2NzVGM0JFNzAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==' (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:207)
[2024-12-16 22:00:21,055] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = replicator-fetcher (io.debezium.util.Threads:270)
[2024-12-16 22:00:21,056] INFO [mongodb-source-connector|task-0] Fetcher submitted for execution: io.debezium.connector.mongodb.events.BufferingChangeStreamCursor$EventFetcher@76ab6012 @ java.util.concurrent.ThreadPoolExecutor@2c1764ba[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0] (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:367)
[2024-12-16 22:00:21,056] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-replicator-fetcher-0 (io.debezium.util.Threads:287)
[2024-12-16 22:00:21,058] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,061] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,062] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,063] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=2146395, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:00:18 ICT 2024, lastUpdateTimeNanos=68037823370255} (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,065] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:21,066] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:00:25,804] INFO 172.30.0.4 - - [16/Dec/2024:15:00:25 +0000] "GET /connectors/mongodb-source-connector/status HTTP/1.1" 200 184 "-" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36" 28 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:01:02,112] INFO [mongodb-source-connector|task-0] 2 records sent during previous 00:00:41.257, last recorded offset of {server_id=fullfillment} partition is {sec=1734361261, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBBRDAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2NDY1NkM2NTc0NjUwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzVGM0JFNzEzOTgyQTVFQjE5NjQwMzQwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:349)
[2024-12-16 22:01:10,871] INFO [mongodb-source-connector|task-0|offsets] WorkerSourceTask{id=mongodb-source-connector-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2024-12-16 22:01:20,910] INFO [mongodb-source-connector|task-0|offsets] WorkerSourceTask{id=mongodb-source-connector-0} Committing offsets for 1 acknowledged messages (org.apache.kafka.connect.runtime.WorkerSourceTask:236)
[2024-12-16 22:01:33,634] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:01:33,641] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:01:33,642] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:01:33,657] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=318, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:01:33,677] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=318, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:01:33,679] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 318 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=518, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:01:33,681] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 518 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:01:33,682] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:01:33,684] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:01:33,688] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, timestamp]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:01:33,698] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, timestamp]
	transforms.timestamp.field = createdAt
	transforms.timestamp.format = 
	transforms.timestamp.negate = false
	transforms.timestamp.predicate = null
	transforms.timestamp.replace.null.with.default = true
	transforms.timestamp.target.type = Timestamp
	transforms.timestamp.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestamp.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:01:33,708] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:01:33,724] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:01:33,728] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:01:33,730] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, timestamp]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:01:33,734] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap, timestamp]
	transforms.timestamp.field = createdAt
	transforms.timestamp.format = 
	transforms.timestamp.negate = false
	transforms.timestamp.predicate = null
	transforms.timestamp.replace.null.with.default = true
	transforms.timestamp.target.type = Timestamp
	transforms.timestamp.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestamp.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:01:33,745] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:01:33,786] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:01:33,787] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:01:33,788] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:01:33,791] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=319, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:01:33,796] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=319, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:01:33,797] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 319 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=522, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:01:33,798] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 522 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:01:33,799] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:01:33,800] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:01:33,802] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, timestamp]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:01:33,804] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, timestamp]
	transforms.timestamp.field = createdAt
	transforms.timestamp.format = 
	transforms.timestamp.negate = false
	transforms.timestamp.predicate = null
	transforms.timestamp.replace.null.with.default = true
	transforms.timestamp.target.type = Timestamp
	transforms.timestamp.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.timestamp.unix.precision = milliseconds
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.delete.handling.mode = rewrite
	transforms.unwrap.drop.fields.from.key = false
	transforms.unwrap.drop.fields.header.name = null
	transforms.unwrap.drop.fields.keep.schema.compatible = true
	transforms.unwrap.drop.tombstones = true
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.transforms.ExtractNewRecordState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:01:33,815] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:01:33,819] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:03:22,357] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:03:22,358] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:03:22,380] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:03:22,381] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:03:22,384] INFO 172.30.2.207 - - [16/Dec/2024:15:03:22 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 65 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:03:22,389] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=320, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:03:22,404] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=320, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:03:22,406] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:03:22,407] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:03:22,442] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:03:22,445] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:03:22,447] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:03:22,449] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:03:22,451] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:03:22,452] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 320 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=524, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:03:22,453] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 524 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:03:22,454] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:03:22,455] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:03:22,455] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:03:22,461] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=321, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:03:22,467] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=321, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:03:22,468] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 321 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=524, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:03:22,468] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 524 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:03:22,469] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:04:34,228] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:04:34,238] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:04:34,239] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:04:34,258] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=322, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:04:34,268] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=322, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:04:34,269] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 322 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=525, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:04:34,270] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 525 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:04:34,271] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:04:34,273] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:04:34,280] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:04:34,291] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:04:34,295] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:04:34,298] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:04:34,299] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:04:34,305] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:04:34,306] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:04:34,339] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:04:34,344] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:04:34,345] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:04:34,353] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=323, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:04:34,354] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:04:34,359] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=323, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:04:34,360] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 323 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=528, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:04:34,361] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 528 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:04:34,362] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:04:34,363] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:04:34,365] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:04:34,367] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:04:34,375] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:04:34,387] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:04:34,388] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:04:34,388] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:04:34,388] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:04:34,388] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:04:34,390] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:04:34,395] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=324, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:04:34,409] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=324, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:04:34,410] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 324 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=529, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:04:34,411] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 529 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:04:34,412] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:04:34,414] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:04:34,416] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:04:34,424] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = []
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:04:34,427] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:04:34,432] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:06:50,578] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:06:50,580] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:06:50,581] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:06:50,581] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:06:50,587] INFO 172.30.2.207 - - [16/Dec/2024:15:06:50 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 55 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:06:50,594] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=325, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:06:50,605] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=325, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:06:50,606] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:06:50,607] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:06:50,607] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:06:50,608] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:06:50,617] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:06:50,621] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:06:50,627] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:06:50,628] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 325 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=531, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:06:50,631] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 531 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:06:50,632] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:06:50,633] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:06:50,635] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:06:50,641] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=326, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:06:50,647] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=326, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:06:50,647] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 326 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=531, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:06:50,648] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 531 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:06:50,648] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:07:15,413] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:07:15,416] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:07:15,417] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:07:15,426] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=327, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:07:15,433] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=327, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:07:15,434] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 327 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=532, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:07:15,434] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 532 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:07:15,435] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:07:15,436] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:07:15,439] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:07:15,450] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:07:15,460] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:07:15,469] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:07:15,469] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:07:15,471] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:07:15,483] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:07:15,517] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:07:15,542] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:07:15,547] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:07:15,547] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:07:15,551] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=328, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:07:15,559] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=328, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:07:15,560] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 328 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=536, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:07:15,560] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 536 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:07:15,561] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:07:15,563] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:07:15,566] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:07:15,569] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:07:15,577] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:07:15,588] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:07:58,884] INFO [AdminClient clientId=mongo-source-cluster-shared-admin] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,884] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,884] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,889] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Cancelled in-flight FETCH request with correlation id 52610 due to node 0 being disconnected (elapsed time since creation: 222ms, elapsed time since send: 222ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient:364)
[2024-12-16 22:07:58,890] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=52383) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:58,893] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,894] INFO [Producer clientId=mongo-source-cluster-configs] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,895] INFO [Producer clientId=mongo-source-cluster-offsets] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,898] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,898] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Cancelled in-flight FETCH request with correlation id 52539 due to node 0 being disconnected (elapsed time since creation: 199ms, elapsed time since send: 199ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient:364)
[2024-12-16 22:07:58,902] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=52321) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:58,901] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 2147483647 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,909] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:58,915] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Cancelled in-flight FETCH request with correlation id 52431 due to node 0 being disconnected (elapsed time since creation: 83ms, elapsed time since send: 83ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient:364)
[2024-12-16 22:07:58,916] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=52211) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:58,920] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:58,921] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:58,922] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:58,936] INFO [Producer clientId=mongo-source-cluster-statuses] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:58,936] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Node -2 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,007] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,008] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,008] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,013] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,014] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,015] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,025] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,025] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,025] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,033] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,037] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 2147483647 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,037] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connection to node 2147483647 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,038] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,039] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,040] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,040] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,128] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,128] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,129] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,160] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,162] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 2147483647 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,162] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connection to node 2147483647 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,163] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,165] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,166] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,167] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,213] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,213] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,214] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,218] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,219] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,220] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,262] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,263] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,263] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,333] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,333] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,333] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,471] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,473] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 2147483647 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,474] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connection to node 2147483647 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,474] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,477] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,477] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,477] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,488] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,489] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,489] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,517] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,517] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,517] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,570] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,571] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,571] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,806] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,807] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,807] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:07:59,812] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:07:59,813] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:07:59,814] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:07:59,995] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:07:59,996] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:07:59,996] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:00,021] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:00,021] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:00,021] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:00,624] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:00,625] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:00,625] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:00,767] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:00,767] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:00,767] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:00,836] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:00,836] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:00,837] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:01,664] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:01,665] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:01,665] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:01,844] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:01,845] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:01,846] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:01,844] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:01,846] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:01,847] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:02,475] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Broker coordinator was unreachable for 3000ms. Revoking previous assignment Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=536, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} to avoid running tasks while not being a member the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:147)
[2024-12-16 22:08:02,478] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:08:02,478] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:08:02,480] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:08:02,480] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:08:02,481] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-16 22:08:02,478] INFO [mongodb-source-connector|worker] Stopping connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:08:02,483] INFO [mongodb-source-connector|worker] Scheduled shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:08:02,484] INFO [mongodb-source-connector|worker] Stopping MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:82)
[2024-12-16 22:08:02,484] INFO [mongodb-source-connector|worker] Stopped MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:86)
[2024-12-16 22:08:02,485] INFO [mongodb-source-connector|worker] Completed shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:08:02,487] INFO [Producer clientId=mongo-source-cluster-statuses] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:02,488] WARN [Producer clientId=mongo-source-cluster-statuses] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:02,488] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:08:02,557] INFO [mongodb-source-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:432)
[2024-12-16 22:08:02,672] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:02,672] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:02,672] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:02,850] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:02,851] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:02,852] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:02,855] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:02,855] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:02,856] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:03,187] INFO [mongodb-source-connector|task-0] Awaiting fetcher thread termination (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:457)
[2024-12-16 22:08:03,679] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:03,682] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:03,682] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:03,759] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:03,759] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:03,759] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:03,860] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:03,860] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:03,860] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:03,992] INFO [mongodb-source-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2024-12-16 22:08:03,993] INFO [mongodb-source-connector|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:08:03,995] INFO [mongodb-source-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2024-12-16 22:08:03,996] INFO [mongodb-source-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2024-12-16 22:08:03,997] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:08:04,013] INFO [mongodb-source-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:04,014] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:04,015] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:04,015] INFO [mongodb-source-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:04,017] INFO [mongodb-source-connector|task-0] App info kafka.producer for connector-producer-mongodb-source-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:04,025] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:08:04,719] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:04,719] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:04,719] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:04,762] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:04,762] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:04,762] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:04,862] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:04,863] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:04,864] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:05,628] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:05,629] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:05,629] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:05,769] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:05,770] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:05,770] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:05,847] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:05,849] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:05,849] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:06,562] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:06,563] WARN [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:06,564] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Error sending fetch request (sessionId=67364303, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:06,774] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:06,776] WARN [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:06,776] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Error sending fetch request (sessionId=667950765, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:06,860] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:06,861] WARN [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:06,861] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Error sending fetch request (sessionId=1160988539, epoch=INITIAL) to node 0: (org.apache.kafka.clients.FetchSessionHandler:618)
org.apache.kafka.common.errors.DisconnectException
[2024-12-16 22:08:06,912] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,013] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 2147483647 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:07,013] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connection to node 2147483647 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:07,014] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: null. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:07,036] INFO [Producer clientId=mongo-source-cluster-statuses] Node 0 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:07,036] WARN [Producer clientId=mongo-source-cluster-statuses] Connection to node 0 (/172.30.2.207:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient:849)
[2024-12-16 22:08:07,215] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,216] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:07,412] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,413] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,413] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:08:07,415] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,415] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,417] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,418] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:07,418] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:07,425] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,426] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,426] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,429] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,432] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,433] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,434] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,440] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,440] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,441] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,446] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,447] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,448] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,453] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,454] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,455] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,459] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,460] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,464] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,464] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,481] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,482] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,485] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,489] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,490] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,491] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,494] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,495] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,496] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,500] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,501] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,501] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,505] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,506] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,507] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,510] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,511] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,512] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,516] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,517] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,517] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,520] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,520] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,533] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,535] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:07,535] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:07,553] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,553] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,554] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,567] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,572] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,573] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,574] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,578] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,579] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,580] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,584] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,585] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,586] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,589] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,590] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,591] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,595] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,596] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,597] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,601] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,602] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,603] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,607] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,608] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,609] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,619] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,620] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,621] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,625] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,626] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,627] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,636] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,636] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,637] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,641] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,642] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,642] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,646] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,646] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,647] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,654] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,655] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,655] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,659] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,659] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,660] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,663] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,664] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,664] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,669] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,670] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,671] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,675] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,676] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,676] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,680] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,681] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,681] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,684] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,685] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,685] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,695] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,695] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,698] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,702] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,705] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,706] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,709] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,710] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,710] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,717] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,718] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,719] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,723] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,724] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,725] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,728] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,729] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,730] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,734] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,735] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,735] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,739] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,740] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,743] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,748] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,749] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,751] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:07,751] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:07,752] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:07,752] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,766] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,766] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,766] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,773] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,774] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,775] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,777] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,778] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,779] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,782] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,782] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,783] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,787] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,788] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,788] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,791] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,795] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,796] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,797] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,804] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,805] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,806] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,811] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,817] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,818] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,823] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,824] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,825] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,829] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,831] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,832] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,835] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,837] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,838] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,841] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,842] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,843] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,847] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,848] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,849] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,854] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,855] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,857] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,861] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,862] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,864] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,868] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,869] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,871] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,878] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,879] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,880] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,885] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,886] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,886] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,891] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,892] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,893] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,899] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,899] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,900] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,905] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,905] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,906] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,909] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,910] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,911] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,915] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,916] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,917] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,921] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,922] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,925] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,926] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,927] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,931] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,932] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,934] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,938] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,939] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,940] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,946] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,947] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,948] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,958] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,959] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,963] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,987] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,989] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,990] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,995] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,996] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:07,997] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,005] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,006] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,007] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,013] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,014] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,014] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,020] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,020] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,021] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,025] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,025] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,026] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,029] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,030] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,031] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,035] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,036] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,037] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,042] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,044] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,044] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,048] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,049] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,049] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,055] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,056] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,057] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,060] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,060] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,061] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,065] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,066] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,067] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,071] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-2 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,072] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,072] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,157] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:08,159] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:08:08,159] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,193] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Attempt to heartbeat failed since coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is either not started or not valid (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1247)
[2024-12-16 22:08:08,193] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:08,193] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:08,194] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Client requested disconnect from node 2147483647 (org.apache.kafka.clients.NetworkClient:344)
[2024-12-16 22:08:08,194] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Cancelled in-flight JOIN_GROUP request with correlation id 9111 due to node 2147483647 being disconnected (elapsed time since creation: 33ms, elapsed time since send: 22ms, throttle time: 0ms, request timeout: 65000ms) (org.apache.kafka.clients.NetworkClient:364)
[2024-12-16 22:08:08,195] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'null' (DisconnectException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,262] WARN [Producer clientId=mongo-source-cluster-statuses] Received invalid metadata error in produce request on partition connect-status-3 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now (org.apache.kafka.clients.producer.internals.Sender:711)
[2024-12-16 22:08:08,297] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:08,298] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,302] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:08,302] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:08,302] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=328, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:695)
[2024-12-16 22:08:08,303] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,403] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Client requested disconnect from node 2147483647 (org.apache.kafka.clients.NetworkClient:344)
[2024-12-16 22:08:08,406] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:08,406] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1000)
[2024-12-16 22:08:08,406] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Requesting disconnect from last known coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1013)
[2024-12-16 22:08:08,491] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:08,492] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,519] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] JoinGroup failed: Coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is loading the group. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:679)
[2024-12-16 22:08:08,519] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'The coordinator is loading and hence can't process requests.' (CoordinatorLoadInProgressException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,620] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,627] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] JoinGroup failed: Coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is loading the group. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:679)
[2024-12-16 22:08:08,628] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'The coordinator is loading and hence can't process requests.' (CoordinatorLoadInProgressException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,728] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,731] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] JoinGroup failed: Coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is loading the group. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:679)
[2024-12-16 22:08:08,732] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'The coordinator is loading and hence can't process requests.' (CoordinatorLoadInProgressException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,832] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,834] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] JoinGroup failed: Coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) is loading the group. (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:679)
[2024-12-16 22:08:08,834] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Request joining group due to: rebalance failed due to 'The coordinator is loading and hence can't process requests.' (CoordinatorLoadInProgressException) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1103)
[2024-12-16 22:08:08,935] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:08,983] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=329, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:08:09,028] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=329, memberId='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:08:09,029] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 329 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5', leaderUrl='http://172.30.2.147:8083/', offset=536, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:08:09,030] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 536 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:08:09,031] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:08:09,032] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:08:09,033] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:08:09,034] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:08:09,031] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mongodb-source-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:08:09,036] INFO [mongodb-source-connector|worker] Creating connector mongodb-source-connector of type io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:08:09,034] INFO [mongodb-source-connector|task-0] Creating task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:08:09,038] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:08:09,041] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,045] INFO [mongodb-source-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:08:09,034] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:08:09,049] INFO [mongodb-source-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:09,052] INFO [mongodb-source-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,057] INFO [mongodb-source-connector|worker] Instantiated connector mongodb-source-connector with version 3.0.4.Final of type class io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:08:09,053] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:08:09,059] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,062] INFO [mongodb-source-connector|worker] Successfully started MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:67)
[2024-12-16 22:08:09,053] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:08:09,067] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:08:09,068] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,069] INFO [mongodb-source-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mongodb.MongoDbConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-16 22:08:09,071] INFO [mongodb-source-connector|worker] Finished creating connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:08:09,062] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:08:09,088] INFO [mongodb-source-connector|task-0] Instantiated task mongodb-source-connector-0 with version 3.0.4.Final of type io.debezium.connector.mongodb.MongoDbConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-16 22:08:09,094] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:09,094] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:09,096] INFO [mongodb-source-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-16 22:08:09,096] INFO [mongodb-source-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-16 22:08:09,096] INFO [mongodb-source-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-16 22:08:09,109] WARN [mongodb-source-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-16 22:08:09,110] WARN [mongodb-source-connector|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-16 22:08:09,112] INFO [mongodb-source-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-16 22:08:09,113] INFO [mongodb-source-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:09,115] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,117] INFO [mongodb-source-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongodb-source-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:08:09,118] INFO [mongodb-source-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:09,151] INFO [mongodb-source-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:08:09,151] INFO [mongodb-source-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:09,152] INFO [mongodb-source-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:09,152] INFO [mongodb-source-connector|task-0] Kafka startTimeMs: 1734361689151 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:09,156] INFO [mongodb-source-connector|task-0] Starting MongoDbConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:245)
[2024-12-16 22:08:09,158] INFO [mongodb-source-connector|task-0]    connector.class = io.debezium.connector.mongodb.MongoDbConnector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,158] INFO [mongodb-source-connector|task-0]    collection.include.list = test.customers (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,158] INFO [mongodb-source-connector|task-0]    transforms.unwrap.delete.handling.mode = drop (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,158] INFO [mongodb-source-connector|task-0]    transforms.unwrap.array.encoding = array (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,158] INFO [mongodb-source-connector|task-0]    mongodb.connection.string = ******** (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,160] INFO [mongodb-source-connector|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,160] INFO [mongodb-source-connector|task-0]    transforms = unwrap,extractNew,convertDate (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,160] INFO [mongodb-source-connector|task-0]    transforms.unwrap.add.source.fields = db,collection (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,160] INFO [mongodb-source-connector|task-0]    topic.prefix = fullfillment (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,160] INFO [mongodb-source-connector|task-0]    transforms.convertDate.format = yyyy-MM-dd HH:mm:ss (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    mongodb.server.selection.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    mongodb.connect.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,161] INFO [mongodb-source-connector|task-0]    mongodb.auth.source = admin (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    transforms.convertDate.type = org.apache.kafka.connect.transforms.TimestampConverter$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    transforms.extractNew.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    transforms.extractNew.whitelist = id,name,email,age,createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    task.class = io.debezium.connector.mongodb.MongoDbConnectorTask (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    transforms.convertDate.field = createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    name = mongodb-source-connector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    transforms.convertDate.target.type = string (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,162] INFO [mongodb-source-connector|task-0]    database.include.list = test (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,163] INFO [mongodb-source-connector|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,163] INFO [mongodb-source-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:09,163] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:08:09,165] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:08:09,166] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,168] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:09,175] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:09,176] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,177] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,178] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,178] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,189] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,197] INFO [mongodb-source-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2024-12-16 22:08:09,197] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,198] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:09,201] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,202] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,252] INFO [mongodb-source-connector|task-0] Found previous partition offset MongoDbPartition [sourcePartition={server_id=fullfillment}]: {sec=1734361270, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:527)
[2024-12-16 22:08:09,254] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = SignalProcessor (io.debezium.util.Threads:270)
[2024-12-16 22:08:09,257] WARN [mongodb-source-connector|task-0] Found a not connector specific implementation io.debezium.snapshot.lock.NoLockingSupport for lock mode no_locking_support (io.debezium.snapshot.SnapshotLockProvider:82)
[2024-12-16 22:08:09,258] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,263] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,264] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,265] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,266] INFO [mongodb-source-connector|task-0] Found existing offset for at {sec=1734361270, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==} (io.debezium.connector.mongodb.connection.MongoDbConnection:202)
[2024-12-16 22:08:09,267] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,277] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=3540708, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:08 ICT 2024, lastUpdateTimeNanos=68506037255003} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,281] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@68c730c8}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:08:09,287] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:08:09,300] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,307] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,309] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,310] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1062589, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:08 ICT 2024, lastUpdateTimeNanos=68506071036904} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,311] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,314] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,365] INFO [mongodb-source-connector|task-0] Valid resume token present, so no snapshot will be performed' (io.debezium.connector.mongodb.connection.MongoDbConnection:220)
[2024-12-16 22:08:09,369] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2024-12-16 22:08:09,369] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = blocking-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:08:09,370] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-change-event-source-coordinator (io.debezium.util.Threads:287)
[2024-12-16 22:08:09,370] INFO [mongodb-source-connector|task-0] WorkerSourceTask{id=mongodb-source-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2024-12-16 22:08:09,372] INFO [mongodb-source-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2024-12-16 22:08:09,372] INFO [mongodb-source-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2024-12-16 22:08:09,373] INFO [mongodb-source-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:144)
[2024-12-16 22:08:09,373] INFO [mongodb-source-connector|task-0] According to the connector configuration, no snapshot will occur. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:151)
[2024-12-16 22:08:09,374] INFO [mongodb-source-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MongoDbOffsetContext [sourceInfo=SourceInfo [initialSync=false, collectionId=null, position=Position [ts=Timestamp{value=7449024934099025921, seconds=1734361270, inc=1}, changeStreamSessionTxnId=null, resumeToken=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==]]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2024-12-16 22:08:09,374] INFO [mongodb-source-connector|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:08:09,375] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = mongodb named = incremental-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:08:09,375] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,376] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,384] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,385] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,388] INFO [mongodb-source-connector|task-0] No incremental snapshot in progress, no action needed on start (io.debezium.connector.mongodb.snapshot.MongoDbIncrementalSnapshotChangeEventSource:262)
[2024-12-16 22:08:09,389] INFO [mongodb-source-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2024-12-16 22:08:09,389] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-SignalProcessor (io.debezium.util.Threads:287)
[2024-12-16 22:08:09,390] INFO [mongodb-source-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2024-12-16 22:08:09,390] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,391] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,392] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,393] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:09,394] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,395] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@6636d676, com.mongodb.Jep395RecordCodecProvider@5faedab5, com.mongodb.KotlinCodecProvider@384c4466]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@55f15438}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:08:09,397] INFO [mongodb-source-connector|task-0] Reading change stream (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:100)
[2024-12-16 22:08:09,398] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"event": "$$ROOT", "namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:08:09,398] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=964163, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:08 ICT 2024, lastUpdateTimeNanos=68506158645719} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,401] INFO [mongodb-source-connector|task-0] Resuming streaming from token 'zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==' (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:207)
[2024-12-16 22:08:09,402] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = replicator-fetcher (io.debezium.util.Threads:270)
[2024-12-16 22:08:09,402] INFO [mongodb-source-connector|task-0] Fetcher submitted for execution: io.debezium.connector.mongodb.events.BufferingChangeStreamCursor$EventFetcher@635296b2 @ java.util.concurrent.ThreadPoolExecutor@3bc8537a[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0] (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:367)
[2024-12-16 22:08:09,402] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-replicator-fetcher-0 (io.debezium.util.Threads:287)
[2024-12-16 22:08:09,404] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,404] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,406] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,406] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,408] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=999930, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:08 ICT 2024, lastUpdateTimeNanos=68506168961113} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:09,409] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:13,948] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,950] INFO [Producer clientId=mongo-source-cluster-configs] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,953] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,955] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,956] INFO [Producer clientId=mongo-source-cluster-offsets] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,960] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,961] INFO [Producer clientId=mongo-source-cluster-statuses] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,962] INFO [AdminClient clientId=mongo-source-cluster-shared-admin] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:13,972] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient:1021)
[2024-12-16 22:08:23,989] INFO Kafka Connect stopping (org.apache.kafka.connect.runtime.Connect:87)
[2024-12-16 22:08:23,992] INFO Stopping REST server (org.apache.kafka.connect.runtime.rest.RestServer:358)
[2024-12-16 22:08:24,021] INFO Stopped http_0.0.0.08083@50195abf{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:383)
[2024-12-16 22:08:24,022] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session:149)
[2024-12-16 22:08:24,054] INFO Stopped o.e.j.s.ServletContextHandler@75d773fa{/,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler:1159)
[2024-12-16 22:08:24,055] INFO REST server stopped (org.apache.kafka.connect.runtime.rest.RestServer:387)
[2024-12-16 22:08:24,055] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Herder stopping (org.apache.kafka.connect.runtime.distributed.DistributedHerder:851)
[2024-12-16 22:08:24,056] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Stopping connectors and tasks that are still assigned to this worker. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:808)
[2024-12-16 22:08:24,056] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:08:24,057] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:08:24,057] INFO [mongodb-source-connector|worker] Stopping connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:08:24,057] INFO [mongodb-source-connector|worker] Scheduled shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:08:24,058] INFO [mongodb-source-connector|worker] Stopping MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:82)
[2024-12-16 22:08:24,059] INFO [mongodb-source-connector|worker] Stopped MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:86)
[2024-12-16 22:08:24,061] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:08:24,064] INFO [mongodb-source-connector|worker] Completed shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:08:24,065] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-16 22:08:24,401] INFO [mongodb-source-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:432)
[2024-12-16 22:08:24,645] INFO [mongodb-source-connector|task-0] Awaiting fetcher thread termination (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:457)
[2024-12-16 22:08:26,006] INFO [mongodb-source-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2024-12-16 22:08:26,009] INFO [mongodb-source-connector|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:08:26,010] INFO [mongodb-source-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2024-12-16 22:08:26,011] INFO [mongodb-source-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2024-12-16 22:08:26,012] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:08:26,034] INFO [mongodb-source-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,035] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,035] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,036] INFO [mongodb-source-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,040] INFO [mongodb-source-connector|task-0] App info kafka.producer for connector-producer-mongodb-source-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,046] INFO Stopping KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2024-12-16 22:08:26,048] INFO [Producer clientId=mongo-source-cluster-statuses] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:08:26,068] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,068] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,069] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,070] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,071] INFO App info kafka.producer for mongo-source-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,077] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-16 22:08:26,078] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-16 22:08:26,088] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Node 0 sent an invalid full fetch response with extraIds=(BvVnx0t_SlGXkZscTz3wEQ), response=() (org.apache.kafka.clients.FetchSessionHandler:556)
[2024-12-16 22:08:26,092] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,092] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,092] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,092] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,101] INFO App info kafka.consumer for mongo-source-cluster-statuses unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,101] INFO Stopped KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2024-12-16 22:08:26,103] INFO Closing KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:407)
[2024-12-16 22:08:26,104] INFO Stopping KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2024-12-16 22:08:26,105] INFO [Producer clientId=mongo-source-cluster-configs] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:08:26,116] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,116] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,116] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,116] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,117] INFO App info kafka.producer for mongo-source-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,117] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-16 22:08:26,117] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-16 22:08:26,281] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,281] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,282] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,282] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,289] INFO App info kafka.consumer for mongo-source-cluster-configs unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,289] INFO Stopped KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2024-12-16 22:08:26,289] INFO Closed KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:412)
[2024-12-16 22:08:26,289] INFO Worker stopping (org.apache.kafka.connect.runtime.Worker:250)
[2024-12-16 22:08:26,292] INFO Stopping KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:261)
[2024-12-16 22:08:26,293] INFO Stopping KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:317)
[2024-12-16 22:08:26,294] INFO [Producer clientId=mongo-source-cluster-offsets] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:08:26,303] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,303] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,304] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,304] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,304] INFO App info kafka.producer for mongo-source-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,305] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1056)
[2024-12-16 22:08:26,305] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator:1103)
[2024-12-16 22:08:26,497] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,497] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,497] INFO Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,498] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,504] INFO App info kafka.consumer for mongo-source-cluster-offsets unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,504] INFO Stopped KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:341)
[2024-12-16 22:08:26,504] INFO Stopped KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:263)
[2024-12-16 22:08:26,504] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,504] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,504] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,505] INFO App info kafka.connect for 172.30.2.147:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,505] INFO Worker stopped (org.apache.kafka.connect.runtime.Worker:271)
[2024-12-16 22:08:26,509] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Member connect-172.30.2.147:8083-f57c66f8-f42c-4f98-9089-a6a91f5635e5 sending LeaveGroup request to coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) due to the consumer is being closed (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1174)
[2024-12-16 22:08:26,512] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1056)
[2024-12-16 22:08:26,512] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Close timed out with 1 pending requests to coordinator, terminating client connections (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:1141)
[2024-12-16 22:08:26,512] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,513] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,513] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,517] INFO App info kafka.connect for connect-172.30.2.147:8083 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,520] INFO App info kafka.admin.client for mongo-source-cluster-shared-admin unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:26,523] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:26,523] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:26,523] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:26,524] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:394)
[2024-12-16 22:08:26,529] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Herder stopped (org.apache.kafka.connect.runtime.distributed.DistributedHerder:858)
[2024-12-16 22:08:26,530] INFO Kafka Connect stopped (org.apache.kafka.connect.runtime.Connect:92)
[2024-12-16 22:08:28,703] INFO Kafka Connect worker initializing ... (org.apache.kafka.connect.cli.AbstractConnectCli:114)
[2024-12-16 22:08:28,709] INFO WorkerInfo values: 
	jvm.args = -Xms256M, -Xmx2G, -XX:+UseG1GC, -XX:MaxGCPauseMillis=20, -XX:InitiatingHeapOccupancyPercent=35, -XX:+ExplicitGCInvokesConcurrent, -XX:MaxInlineLevel=15, -Djava.awt.headless=true, -Dcom.sun.management.jmxremote=true, -Dcom.sun.management.jmxremote.authenticate=false, -Dcom.sun.management.jmxremote.ssl=false, -Dkafka.logs.dir=/opt/kafka/bin/../logs, -Dlog4j.configuration=file:/opt/kafka/bin/../config/connect-log4j.properties
	jvm.spec = Ubuntu, OpenJDK 64-Bit Server VM, 17.0.13, 17.0.13+11-Ubuntu-2ubuntu122.04
	jvm.classpath = /opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-sync-4.11.0.jar:/opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar:/opt/kafka/plugins/debezium-connector-mongodb/bson-record-codec-4.11.0.jar:/opt/kafka/plugins/debezium-connector-mongodb/bson-4.11.0.jar:/opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-core-4.11.0.jar:/opt/kafka/plugins/debezium-connector-mongodb/debezium-sink-3.0.4.Final.jar:/opt/kafka/plugins/debezium-connector-mongodb/debezium-api-3.0.4.Final.jar:/opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar:/opt/kafka/bin/../libs/activation-1.1.1.jar:/opt/kafka/bin/../libs/aopalliance-repackaged-2.6.1.jar:/opt/kafka/bin/../libs/argparse4j-0.7.0.jar:/opt/kafka/bin/../libs/audience-annotations-0.12.0.jar:/opt/kafka/bin/../libs/caffeine-2.9.3.jar:/opt/kafka/bin/../libs/commons-beanutils-1.9.4.jar:/opt/kafka/bin/../libs/commons-cli-1.4.jar:/opt/kafka/bin/../libs/commons-collections-3.2.2.jar:/opt/kafka/bin/../libs/commons-digester-2.1.jar:/opt/kafka/bin/../libs/commons-io-2.14.0.jar:/opt/kafka/bin/../libs/commons-lang3-3.12.0.jar:/opt/kafka/bin/../libs/commons-logging-1.2.jar:/opt/kafka/bin/../libs/commons-validator-1.7.jar:/opt/kafka/bin/../libs/connect-api-3.9.0.jar:/opt/kafka/bin/../libs/connect-basic-auth-extension-3.9.0.jar:/opt/kafka/bin/../libs/connect-json-3.9.0.jar:/opt/kafka/bin/../libs/connect-mirror-3.9.0.jar:/opt/kafka/bin/../libs/connect-mirror-client-3.9.0.jar:/opt/kafka/bin/../libs/connect-runtime-3.9.0.jar:/opt/kafka/bin/../libs/connect-transforms-3.9.0.jar:/opt/kafka/bin/../libs/error_prone_annotations-2.10.0.jar:/opt/kafka/bin/../libs/hk2-api-2.6.1.jar:/opt/kafka/bin/../libs/hk2-locator-2.6.1.jar:/opt/kafka/bin/../libs/hk2-utils-2.6.1.jar:/opt/kafka/bin/../libs/jackson-annotations-2.16.2.jar:/opt/kafka/bin/../libs/jackson-core-2.16.2.jar:/opt/kafka/bin/../libs/jackson-databind-2.16.2.jar:/opt/kafka/bin/../libs/jackson-dataformat-csv-2.16.2.jar:/opt/kafka/bin/../libs/jackson-datatype-jdk8-2.16.2.jar:/opt/kafka/bin/../libs/jackson-jaxrs-base-2.16.2.jar:/opt/kafka/bin/../libs/jackson-jaxrs-json-provider-2.16.2.jar:/opt/kafka/bin/../libs/jackson-module-afterburner-2.16.2.jar:/opt/kafka/bin/../libs/jackson-module-jaxb-annotations-2.16.2.jar:/opt/kafka/bin/../libs/jackson-module-scala_2.13-2.16.2.jar:/opt/kafka/bin/../libs/jakarta.activation-api-1.2.2.jar:/opt/kafka/bin/../libs/jakarta.annotation-api-1.3.5.jar:/opt/kafka/bin/../libs/jakarta.inject-2.6.1.jar:/opt/kafka/bin/../libs/jakarta.validation-api-2.0.2.jar:/opt/kafka/bin/../libs/jakarta.ws.rs-api-2.1.6.jar:/opt/kafka/bin/../libs/jakarta.xml.bind-api-2.3.3.jar:/opt/kafka/bin/../libs/javassist-3.29.2-GA.jar:/opt/kafka/bin/../libs/javax.activation-api-1.2.0.jar:/opt/kafka/bin/../libs/javax.annotation-api-1.3.2.jar:/opt/kafka/bin/../libs/javax.servlet-api-3.1.0.jar:/opt/kafka/bin/../libs/javax.ws.rs-api-2.1.1.jar:/opt/kafka/bin/../libs/jaxb-api-2.3.1.jar:/opt/kafka/bin/../libs/jersey-client-2.39.1.jar:/opt/kafka/bin/../libs/jersey-common-2.39.1.jar:/opt/kafka/bin/../libs/jersey-container-servlet-2.39.1.jar:/opt/kafka/bin/../libs/jersey-container-servlet-core-2.39.1.jar:/opt/kafka/bin/../libs/jersey-hk2-2.39.1.jar:/opt/kafka/bin/../libs/jersey-server-2.39.1.jar:/opt/kafka/bin/../libs/jetty-client-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-continuation-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-http-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-io-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-security-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-server-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-servlet-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-servlets-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-util-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jetty-util-ajax-9.4.56.v20240826.jar:/opt/kafka/bin/../libs/jline-3.25.1.jar:/opt/kafka/bin/../libs/jopt-simple-5.0.4.jar:/opt/kafka/bin/../libs/jose4j-0.9.4.jar:/opt/kafka/bin/../libs/jsr305-3.0.2.jar:/opt/kafka/bin/../libs/kafka-clients-3.9.0.jar:/opt/kafka/bin/../libs/kafka-group-coordinator-3.9.0.jar:/opt/kafka/bin/../libs/kafka-group-coordinator-api-3.9.0.jar:/opt/kafka/bin/../libs/kafka-metadata-3.9.0.jar:/opt/kafka/bin/../libs/kafka-raft-3.9.0.jar:/opt/kafka/bin/../libs/kafka-server-3.9.0.jar:/opt/kafka/bin/../libs/kafka-server-common-3.9.0.jar:/opt/kafka/bin/../libs/kafka-shell-3.9.0.jar:/opt/kafka/bin/../libs/kafka-storage-3.9.0.jar:/opt/kafka/bin/../libs/kafka-storage-api-3.9.0.jar:/opt/kafka/bin/../libs/kafka-streams-3.9.0.jar:/opt/kafka/bin/../libs/kafka-streams-examples-3.9.0.jar:/opt/kafka/bin/../libs/kafka-streams-scala_2.13-3.9.0.jar:/opt/kafka/bin/../libs/kafka-streams-test-utils-3.9.0.jar:/opt/kafka/bin/../libs/kafka-tools-3.9.0.jar:/opt/kafka/bin/../libs/kafka-tools-api-3.9.0.jar:/opt/kafka/bin/../libs/kafka-transaction-coordinator-3.9.0.jar:/opt/kafka/bin/../libs/kafka_2.13-3.9.0.jar:/opt/kafka/bin/../libs/lz4-java-1.8.0.jar:/opt/kafka/bin/../libs/maven-artifact-3.9.6.jar:/opt/kafka/bin/../libs/metrics-core-2.2.0.jar:/opt/kafka/bin/../libs/metrics-core-4.1.12.1.jar:/opt/kafka/bin/../libs/netty-buffer-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-codec-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-common-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-handler-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-resolver-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-transport-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-transport-classes-epoll-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-epoll-4.1.111.Final.jar:/opt/kafka/bin/../libs/netty-transport-native-unix-common-4.1.111.Final.jar:/opt/kafka/bin/../libs/opentelemetry-proto-1.0.0-alpha.jar:/opt/kafka/bin/../libs/osgi-resource-locator-1.0.3.jar:/opt/kafka/bin/../libs/paranamer-2.8.jar:/opt/kafka/bin/../libs/pcollections-4.0.1.jar:/opt/kafka/bin/../libs/plexus-utils-3.5.1.jar:/opt/kafka/bin/../libs/protobuf-java-3.25.5.jar:/opt/kafka/bin/../libs/reflections-0.10.2.jar:/opt/kafka/bin/../libs/reload4j-1.2.25.jar:/opt/kafka/bin/../libs/rocksdbjni-7.9.2.jar:/opt/kafka/bin/../libs/scala-collection-compat_2.13-2.10.0.jar:/opt/kafka/bin/../libs/scala-java8-compat_2.13-1.0.2.jar:/opt/kafka/bin/../libs/scala-library-2.13.14.jar:/opt/kafka/bin/../libs/scala-logging_2.13-3.9.5.jar:/opt/kafka/bin/../libs/scala-reflect-2.13.14.jar:/opt/kafka/bin/../libs/slf4j-api-1.7.36.jar:/opt/kafka/bin/../libs/slf4j-reload4j-1.7.36.jar:/opt/kafka/bin/../libs/snappy-java-1.1.10.5.jar:/opt/kafka/bin/../libs/swagger-annotations-2.2.8.jar:/opt/kafka/bin/../libs/trogdor-3.9.0.jar:/opt/kafka/bin/../libs/zookeeper-3.8.4.jar:/opt/kafka/bin/../libs/zookeeper-jute-3.8.4.jar:/opt/kafka/bin/../libs/zstd-jni-1.5.6-4.jar
	os.spec = Linux, amd64, 5.15.0-125-generic
	os.vcpus = 2
 (org.apache.kafka.connect.runtime.WorkerInfo:72)
[2024-12-16 22:08:28,710] INFO Scanning for plugin classes. This might take a moment ... (org.apache.kafka.connect.cli.AbstractConnectCli:120)
[2024-12-16 22:08:28,840] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:28,986] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2024-12-16 22:08:29,556] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:29,561] INFO Loading plugin from: /opt/kafka/plugins/hibernate-core-6.4.1.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:29,578] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2024-12-16 22:08:29,641] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/hibernate-core-6.4.1.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:29,642] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:29,681] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2024-12-16 22:08:29,688] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-jdbc (org.apache.kafka.connect.runtime.isolation.PluginScanner:139)
java.util.ServiceConfigurationError: org.apache.kafka.connect.storage.Converter: Provider io.debezium.converters.CloudEventsConverter could not be instantiated
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:813)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	... 13 more
[2024-12-16 22:08:29,740] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:29,980] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-sync-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,018] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-sync-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,019] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,062] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,064] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/bson-record-codec-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,086] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/bson-record-codec-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,086] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/bson-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,108] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/bson-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,109] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-core-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,136] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-core-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,137] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-sink-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,152] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-sink-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,153] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-api-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,167] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-api-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,168] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,178] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2024-12-16 22:08:30,180] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:139)
java.util.ServiceConfigurationError: org.apache.kafka.connect.storage.Converter: Provider io.debezium.converters.CloudEventsConverter could not be instantiated
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:813)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	... 13 more
[2024-12-16 22:08:30,239] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,240] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/waffle-jna-3.2.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,250] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/waffle-jna-3.2.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,250] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jaxb-core-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,260] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jaxb-core-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,261] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/txw2-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,270] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/txw2-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,271] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/ojdbc11-21.15.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,281] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/ojdbc11-21.15.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,319] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/c3p0-0.9.5.5.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,343] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/c3p0-0.9.5.5.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,344] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/angus-activation-2.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,369] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/angus-activation-2.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,370] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jandex-3.1.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,381] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jandex-3.1.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,382] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-c3p0-6.4.8.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,394] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-c3p0-6.4.8.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,395] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mysql-connector-j-9.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,403] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mysql-connector-j-9.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,408] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/error_prone_annotations-2.10.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,416] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/error_prone_annotations-2.10.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,417] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.activation-api-2.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,426] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.activation-api-2.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,427] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/byte-buddy-1.14.11.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,435] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/byte-buddy-1.14.11.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,436] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/istack-commons-runtime-4.1.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,444] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/istack-commons-runtime-4.1.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,444] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mssql-jdbc-12.4.2.jre8.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,453] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mssql-jdbc-12.4.2.jre8.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,509] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mchange-commons-java-0.2.19.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,518] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mchange-commons-java-0.2.19.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,518] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/caffeine-2.9.3.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,527] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/caffeine-2.9.3.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,528] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jaxb-runtime-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,536] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jaxb-runtime-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,536] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jboss-logging-3.5.0.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,544] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jboss-logging-3.5.0.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,545] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.persistence-api-3.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,553] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.persistence-api-3.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,553] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.xml.bind-api-4.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,561] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.xml.bind-api-4.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,562] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.inject-api-2.0.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,570] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.inject-api-2.0.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,570] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mariadb-java-client-3.2.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,579] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mariadb-java-client-3.2.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,581] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jna-platform-5.12.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,594] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jna-platform-5.12.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,595] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/antlr4-runtime-4.10.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,606] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/antlr4-runtime-4.10.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,607] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jcl-over-slf4j-1.7.36.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,623] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jcl-over-slf4j-1.7.36.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,623] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-commons-annotations-6.0.6.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,634] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-commons-annotations-6.0.6.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,634] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/postgresql-42.6.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,650] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/postgresql-42.6.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,656] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.transaction-api-2.0.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,667] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.transaction-api-2.0.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,667] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-sink-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,677] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-sink-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,678] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-api-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,690] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-api-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,691] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/classmate-1.5.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,701] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/classmate-1.5.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,701] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-connector-jdbc-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,717] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-connector-jdbc-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,718] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,730] INFO Using up-to-date JsonConverter implementation (io.debezium.converters.CloudEventsConverter:120)
[2024-12-16 22:08:30,732] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:139)
java.util.ServiceConfigurationError: org.apache.kafka.connect.storage.Converter: Provider io.debezium.converters.CloudEventsConverter could not be instantiated
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:586)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:813)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	... 13 more
[2024-12-16 22:08:30,795] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,796] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jna-5.12.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,817] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jna-5.12.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,817] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-core-6.4.8.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,830] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-core-6.4.8.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,830] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:30,849] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@1dbd16a6 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:30,849] INFO Scanning plugins with ServiceLoaderScanner took 2012 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2024-12-16 22:08:30,854] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:31,387] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:31,387] INFO Loading plugin from: /opt/kafka/plugins/hibernate-core-6.4.1.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:32,366] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/hibernate-core-6.4.1.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:32,368] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,307] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-jdbc: Unable to instantiate CloudEventsConverter: Plugin class has a dependency which is missing or invalid (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:139)
java.lang.NoClassDefFoundError: Could not initialize class io.debezium.converters.CloudEventsConverter
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.versionFor(ReflectionScanner.java:74)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.getPluginDesc(ReflectionScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.scanPlugins(ReflectionScanner.java:90)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:92)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.lang.ExceptionInInitializerError: Exception java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype [in thread "main"]
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	... 5 more
[2024-12-16 22:08:34,319] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,336] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-sync-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,352] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-sync-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,353] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,388] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,389] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/bson-record-codec-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,392] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/bson-record-codec-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,392] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/bson-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,427] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/bson-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,428] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-core-4.11.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,610] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/mongodb-driver-core-4.11.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,610] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-sink-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,615] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-sink-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,616] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-api-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,620] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-api-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,621] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,721] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar: Unable to instantiate CloudEventsConverter: Plugin class has a dependency which is missing or invalid (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:139)
java.lang.NoClassDefFoundError: Could not initialize class io.debezium.converters.CloudEventsConverter
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.versionFor(ReflectionScanner.java:74)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.getPluginDesc(ReflectionScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.scanPlugins(ReflectionScanner.java:90)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:92)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.lang.ExceptionInInitializerError: Exception java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype [in thread "main"]
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	... 5 more
[2024-12-16 22:08:34,738] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-core-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,739] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/waffle-jna-3.2.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,746] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/waffle-jna-3.2.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,746] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jaxb-core-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,755] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jaxb-core-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,755] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/txw2-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,761] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/txw2-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,762] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/ojdbc11-21.15.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:34,988] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/ojdbc11-21.15.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:34,989] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/c3p0-0.9.5.5.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,030] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/c3p0-0.9.5.5.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,031] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/angus-activation-2.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,034] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/angus-activation-2.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,034] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jandex-3.1.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,051] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jandex-3.1.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,051] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-c3p0-6.4.8.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,055] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-c3p0-6.4.8.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,055] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mysql-connector-j-9.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,163] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mysql-connector-j-9.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,164] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/error_prone_annotations-2.10.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,167] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/error_prone_annotations-2.10.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,167] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.activation-api-2.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,172] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.activation-api-2.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,173] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/byte-buddy-1.14.11.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,383] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/byte-buddy-1.14.11.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,385] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/istack-commons-runtime-4.1.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,389] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/istack-commons-runtime-4.1.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,389] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mssql-jdbc-12.4.2.jre8.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,477] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mssql-jdbc-12.4.2.jre8.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,488] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mchange-commons-java-0.2.19.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,523] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mchange-commons-java-0.2.19.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,525] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/caffeine-2.9.3.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,563] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/caffeine-2.9.3.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,563] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jaxb-runtime-4.0.2.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,604] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jaxb-runtime-4.0.2.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,604] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jboss-logging-3.5.0.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,609] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jboss-logging-3.5.0.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,610] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.persistence-api-3.1.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,618] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.persistence-api-3.1.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,619] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.xml.bind-api-4.0.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,628] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.xml.bind-api-4.0.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,628] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.inject-api-2.0.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,633] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.inject-api-2.0.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,633] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/mariadb-java-client-3.2.0.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,661] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/mariadb-java-client-3.2.0.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,662] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jna-platform-5.12.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,726] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jna-platform-5.12.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,727] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/antlr4-runtime-4.10.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,752] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/antlr4-runtime-4.10.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,753] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jcl-over-slf4j-1.7.36.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,756] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jcl-over-slf4j-1.7.36.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,757] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-commons-annotations-6.0.6.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,763] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-commons-annotations-6.0.6.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,763] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/postgresql-42.6.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,809] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/postgresql-42.6.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,810] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jakarta.transaction-api-2.0.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,813] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jakarta.transaction-api-2.0.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,813] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-sink-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,818] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-sink-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,818] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-api-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,822] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-api-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,822] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/classmate-1.5.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,828] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/classmate-1.5.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,834] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-connector-jdbc-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,848] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-connector-jdbc-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,848] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,914] ERROR Failed to discover Converter in /opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar: Unable to instantiate CloudEventsConverter: Plugin class has a dependency which is missing or invalid (org.apache.kafka.connect.runtime.isolation.ReflectionScanner:139)
java.lang.NoClassDefFoundError: Could not initialize class io.debezium.converters.CloudEventsConverter
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.versionFor(ReflectionScanner.java:74)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.getPluginDesc(ReflectionScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ReflectionScanner.scanPlugins(ReflectionScanner.java:90)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:92)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:76)
	at org.apache.kafka.connect.runtime.isolation.Plugins.<init>(Plugins.java:65)
	at org.apache.kafka.connect.cli.AbstractConnectCli.startConnect(AbstractConnectCli.java:121)
	at org.apache.kafka.connect.cli.AbstractConnectCli.run(AbstractConnectCli.java:95)
	at org.apache.kafka.connect.cli.ConnectDistributed.main(ConnectDistributed.java:112)
Caused by: java.lang.ExceptionInInitializerError: Exception java.util.ServiceConfigurationError: io.debezium.converters.spi.CloudEventsProvider: io.debezium.connector.mongodb.converters.MongoDbCloudEventsProvider not a subtype [in thread "main"]
	at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:593)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1244)
	at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1273)
	at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1309)
	at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1393)
	at io.debezium.converters.CloudEventsConverter.<clinit>(CloudEventsConverter.java:135)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)
	at java.base/java.util.ServiceLoader$ProviderImpl.newInstance(ServiceLoader.java:789)
	at java.base/java.util.ServiceLoader$ProviderImpl.get(ServiceLoader.java:729)
	at java.base/java.util.ServiceLoader$3.next(ServiceLoader.java:1403)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.handleLinkageError(PluginScanner.java:177)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.getServiceLoaderPluginDesc(PluginScanner.java:137)
	at org.apache.kafka.connect.runtime.isolation.ServiceLoaderScanner.scanPlugins(ServiceLoaderScanner.java:61)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.scanUrlsAndAddPlugins(PluginScanner.java:80)
	at org.apache.kafka.connect.runtime.isolation.PluginScanner.discoverPlugins(PluginScanner.java:68)
	at org.apache.kafka.connect.runtime.isolation.Plugins.initLoaders(Plugins.java:84)
	... 5 more
[2024-12-16 22:08:35,920] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/debezium-core-3.0.4.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,921] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/jna-5.12.1.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:35,933] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/jna-5.12.1.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:35,934] INFO Loading plugin from: /opt/kafka/plugins/debezium-connector-jdbc/hibernate-core-6.4.8.Final.jar (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:36,470] INFO Registered loader: PluginClassLoader{pluginLocation=file:/opt/kafka/plugins/debezium-connector-jdbc/hibernate-core-6.4.8.Final.jar} (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:36,472] INFO Loading plugin from: classpath (org.apache.kafka.connect.runtime.isolation.PluginScanner:76)
[2024-12-16 22:08:39,048] INFO Registered loader: jdk.internal.loader.ClassLoaders$AppClassLoader@1dbd16a6 (org.apache.kafka.connect.runtime.isolation.PluginScanner:81)
[2024-12-16 22:08:39,049] INFO Scanning plugins with ReflectionScanner took 8195 ms (org.apache.kafka.connect.runtime.isolation.PluginScanner:71)
[2024-12-16 22:08:39,060] WARN One or more plugins are missing ServiceLoader manifests may not be usable with plugin.discovery=service_load: [
file:/opt/kafka/plugins/debezium-connector-mongodb/	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.4.Final
file:/opt/kafka/plugins/debezium-connector-mongodb/debezium-connector-mongodb-3.0.4.Final.jar	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.4.Final
classpath	io.debezium.connector.mongodb.MongoDbSinkConnector	sink	3.0.4.Final
]
Read the documentation at https://kafka.apache.org/documentation.html#connect_plugindiscovery for instructions on migrating your plugins to take advantage of the performance improvements of service_load mode. To silence this warning, set plugin.discovery=only_scan in the worker config. (org.apache.kafka.connect.runtime.isolation.Plugins:123)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,061] INFO Added plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.connector.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.ReplaceField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,062] INFO Added plugin 'org.apache.kafka.connect.transforms.SetSchemaMetadata$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.converters.ByteArrayConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.Cast$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.Flatten$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,063] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampConverter$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,071] INFO Added plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,071] INFO Added plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,071] INFO Added plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,071] INFO Added plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.connector.jdbc.transforms.ConvertCloudEventToSaveableForm' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.HeaderFrom$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Value' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,072] INFO Added plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.transforms.MaskField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.transforms.ExtractField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.transforms.InsertField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.transforms.HoistField$Key' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,073] INFO Added plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:105)
[2024-12-16 22:08:39,075] INFO Added alias 'ExtractNewDocumentState' to plugin 'io.debezium.connector.mongodb.transforms.ExtractNewDocumentState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'RecordIsTombstone' to plugin 'org.apache.kafka.connect.transforms.predicates.RecordIsTombstone' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'CloudEventsConverter' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'String' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'EnvVar' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'EnvVarConfigProvider' to plugin 'org.apache.kafka.common.config.provider.EnvVarConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'MirrorCheckpointConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'Boolean' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'HeaderToValue' to plugin 'io.debezium.transforms.HeaderToValue' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'PartitionRouting' to plugin 'io.debezium.transforms.partitions.PartitionRouting' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'MongoDbSink' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'Json' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'StringConverter' to plugin 'org.apache.kafka.connect.storage.StringConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'MongoDbConnector' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'IntegerConverter' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,076] INFO Added alias 'Float' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'LongConverter' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'DropHeaders' to plugin 'org.apache.kafka.connect.transforms.DropHeaders' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'SimpleHeaderConverter' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'ExtractSchemaToNewRecord' to plugin 'io.debezium.transforms.ExtractSchemaToNewRecord' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'BinaryData' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'DirectoryConfigProvider' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'ShortConverter' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'CloudEvents' to plugin 'io.debezium.converters.CloudEventsConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'TimezoneConverter' to plugin 'io.debezium.transforms.TimezoneConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'BasicAuthSecurityRestExtension' to plugin 'org.apache.kafka.connect.rest.basic.auth.extension.BasicAuthSecurityRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'Simple' to plugin 'org.apache.kafka.connect.storage.SimpleHeaderConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'AllConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'ExtractNewRecordState' to plugin 'io.debezium.transforms.ExtractNewRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'MirrorSource' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,077] INFO Added alias 'DebeziumMongoDb' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'Directory' to plugin 'org.apache.kafka.common.config.provider.DirectoryConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'MirrorHeartbeat' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'DebeziumMongoDbConnectRestExtension' to plugin 'io.debezium.connector.mongodb.rest.DebeziumMongoDbConnectRestExtension' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'BooleanConverter' to plugin 'org.apache.kafka.connect.converters.BooleanConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'HasHeaderKey' to plugin 'org.apache.kafka.connect.transforms.predicates.HasHeaderKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'MirrorCheckpoint' to plugin 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'ExtractChangedRecordState' to plugin 'io.debezium.transforms.ExtractChangedRecordState' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'None' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'TimestampRouter' to plugin 'org.apache.kafka.connect.transforms.TimestampRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'Principal' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'All' to plugin 'org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'JsonConverter' to plugin 'org.apache.kafka.connect.json.JsonConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'JdbcSinkConnector' to plugin 'io.debezium.connector.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'JdbcSink' to plugin 'io.debezium.connector.jdbc.JdbcSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'RegexRouter' to plugin 'org.apache.kafka.connect.transforms.RegexRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'MongoDbSinkConnector' to plugin 'io.debezium.connector.mongodb.MongoDbSinkConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'MongoDb' to plugin 'io.debezium.connector.mongodb.MongoDbConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,078] INFO Added alias 'NoneConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.NoneConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'Short' to plugin 'org.apache.kafka.connect.converters.ShortConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'ByLogicalTableRouter' to plugin 'io.debezium.transforms.ByLogicalTableRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'Double' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'FileConfigProvider' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'EventRouter' to plugin 'io.debezium.transforms.outbox.EventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'SchemaChangeEventFilter' to plugin 'io.debezium.transforms.SchemaChangeEventFilter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'ConvertCloudEventToSaveableForm' to plugin 'io.debezium.connector.jdbc.transforms.ConvertCloudEventToSaveableForm' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'Long' to plugin 'org.apache.kafka.connect.converters.LongConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'File' to plugin 'org.apache.kafka.common.config.provider.FileConfigProvider' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,079] INFO Added alias 'FloatConverter' to plugin 'org.apache.kafka.connect.converters.FloatConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,084] INFO Added alias 'ActivateTracingSpan' to plugin 'io.debezium.transforms.tracing.ActivateTracingSpan' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,084] INFO Added alias 'DoubleConverter' to plugin 'org.apache.kafka.connect.converters.DoubleConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,084] INFO Added alias 'BinaryDataConverter' to plugin 'io.debezium.converters.BinaryDataConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,084] INFO Added alias 'TopicNameMatches' to plugin 'org.apache.kafka.connect.transforms.predicates.TopicNameMatches' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,084] INFO Added alias 'MirrorHeartbeatConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorHeartbeatConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,085] INFO Added alias 'InsertHeader' to plugin 'org.apache.kafka.connect.transforms.InsertHeader' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,085] INFO Added alias 'MirrorSourceConnector' to plugin 'org.apache.kafka.connect.mirror.MirrorSourceConnector' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,086] INFO Added alias 'PrincipalConnectorClientConfigOverridePolicy' to plugin 'org.apache.kafka.connect.connector.policy.PrincipalConnectorClientConfigOverridePolicy' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,086] INFO Added alias 'ValueToKey' to plugin 'org.apache.kafka.connect.transforms.ValueToKey' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,086] INFO Added alias 'MongoEventRouter' to plugin 'io.debezium.connector.mongodb.transforms.outbox.MongoEventRouter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,086] INFO Added alias 'Integer' to plugin 'org.apache.kafka.connect.converters.IntegerConverter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,086] INFO Added alias 'Filter' to plugin 'org.apache.kafka.connect.transforms.Filter' (org.apache.kafka.connect.runtime.isolation.DelegatingClassLoader:109)
[2024-12-16 22:08:39,130] INFO DistributedConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	auto.include.jmx.reporter = true
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	config.providers = []
	config.storage.replication.factor = 1
	config.storage.topic = connect-configs
	connect.protocol = sessioned
	connections.max.idle.ms = 540000
	connector.client.config.override.policy = All
	exactly.once.source.support = disabled
	group.id = mongo-source-cluster
	header.converter = class org.apache.kafka.connect.storage.SimpleHeaderConverter
	heartbeat.interval.ms = 3000
	inter.worker.key.generation.algorithm = HmacSHA256
	inter.worker.key.size = null
	inter.worker.key.ttl.ms = 3600000
	inter.worker.signature.algorithm = HmacSHA256
	inter.worker.verification.algorithms = [HmacSHA256]
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	listeners = [HTTP://0.0.0.0:8083]
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	offset.flush.interval.ms = 10000
	offset.flush.timeout.ms = 5000
	offset.storage.partitions = 25
	offset.storage.replication.factor = 1
	offset.storage.topic = connect-offsets
	plugin.discovery = hybrid_warn
	plugin.path = [/opt/kafka/plugins, /opt/kafka/plugins/debezium-connector-mongodb, /opt/kafka/plugins/debezium-connector-jdbc]
	rebalance.timeout.ms = 60000
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 40000
	response.http.headers.config = 
	rest.advertised.host.name = 172.30.2.147
	rest.advertised.listener = null
	rest.advertised.port = 8083
	rest.extension.classes = []
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	scheduled.rebalance.max.delay.ms = 300000
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	status.storage.partitions = 5
	status.storage.replication.factor = 1
	status.storage.topic = connect-status
	task.shutdown.graceful.timeout.ms = 5000
	topic.creation.enable = true
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
	value.converter = class org.apache.kafka.connect.json.JsonConverter
	worker.sync.timeout.ms = 3000
	worker.unsync.backoff.ms = 300000
 (org.apache.kafka.connect.runtime.distributed.DistributedConfig:371)
[2024-12-16 22:08:39,132] INFO Creating Kafka admin client (org.apache.kafka.connect.runtime.WorkerConfig:281)
[2024-12-16 22:08:39,135] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2024-12-16 22:08:39,231] INFO These configurations '[config.storage.topic, listeners, rest.advertised.host.name, status.storage.topic, group.id, rest.advertised.port, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2024-12-16 22:08:39,232] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:39,232] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:39,232] INFO Kafka startTimeMs: 1734361719231 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:39,697] INFO Kafka cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.connect.runtime.WorkerConfig:298)
[2024-12-16 22:08:39,698] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:08:39,710] INFO Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:08:39,711] INFO Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:08:39,711] INFO Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:08:39,718] INFO PublicConfig values: 
	access.control.allow.methods = 
	access.control.allow.origin = 
	admin.listeners = null
	listeners = [HTTP://0.0.0.0:8083]
	response.http.headers.config = 
	rest.advertised.host.name = 172.30.2.147
	rest.advertised.listener = null
	rest.advertised.port = 8083
	rest.extension.classes = []
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	topic.tracking.allow.reset = true
	topic.tracking.enable = true
 (org.apache.kafka.connect.runtime.rest.RestServerConfig$PublicConfig:371)
[2024-12-16 22:08:39,733] INFO Logging initialized @11978ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log:170)
[2024-12-16 22:08:39,824] INFO Added connector for HTTP://0.0.0.0:8083 (org.apache.kafka.connect.runtime.rest.RestServer:125)
[2024-12-16 22:08:39,825] INFO Initializing REST server (org.apache.kafka.connect.runtime.rest.RestServer:196)
[2024-12-16 22:08:39,863] INFO jetty-9.4.56.v20240826; built: 2024-08-26T17:15:05.868Z; git: ec6782ff5ead824dabdcf47fa98f90a4aedff401; jvm 17.0.13+11-Ubuntu-2ubuntu122.04 (org.eclipse.jetty.server.Server:375)
[2024-12-16 22:08:39,893] INFO Started http_0.0.0.08083@68ff1d6e{HTTP/1.1, (http/1.1)}{0.0.0.0:8083} (org.eclipse.jetty.server.AbstractConnector:333)
[2024-12-16 22:08:39,894] INFO Started @12139ms (org.eclipse.jetty.server.Server:415)
[2024-12-16 22:08:39,915] INFO Advertised URI: http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2024-12-16 22:08:39,915] INFO REST server listening at http://0.0.0.0:8083/, advertising URL http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:216)
[2024-12-16 22:08:39,915] INFO Advertised URI: http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2024-12-16 22:08:39,915] INFO REST admin endpoints at http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:219)
[2024-12-16 22:08:39,916] INFO Advertised URI: http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2024-12-16 22:08:39,916] INFO Setting up All Policy for ConnectorClientConfigOverride. This will allow all client configurations to be overridden (org.apache.kafka.connect.connector.policy.AllConnectorClientConfigOverridePolicy:45)
[2024-12-16 22:08:39,921] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:39,950] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:39,950] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:39,950] INFO Kafka startTimeMs: 1734361719950 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:39,960] INFO JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:39,960] INFO JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = false
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:39,981] INFO Advertised URI: http://172.30.2.147:8083/ (org.apache.kafka.connect.runtime.rest.RestServer:416)
[2024-12-16 22:08:40,014] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,015] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,015] INFO Kafka startTimeMs: 1734361720014 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,018] INFO Kafka Connect worker initialization took 11312ms (org.apache.kafka.connect.cli.AbstractConnectCli:141)
[2024-12-16 22:08:40,018] INFO Kafka Connect starting (org.apache.kafka.connect.runtime.Connect:67)
[2024-12-16 22:08:40,021] INFO Initializing REST resources (org.apache.kafka.connect.runtime.rest.RestServer:223)
[2024-12-16 22:08:40,023] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Herder starting (org.apache.kafka.connect.runtime.distributed.DistributedHerder:375)
[2024-12-16 22:08:40,030] INFO Worker starting (org.apache.kafka.connect.runtime.Worker:233)
[2024-12-16 22:08:40,030] INFO Starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:232)
[2024-12-16 22:08:40,030] INFO Starting KafkaBasedLog with topic connect-offsets reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2024-12-16 22:08:40,036] INFO AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.controllers = []
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-shared-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	enable.metrics.push = true
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
 (org.apache.kafka.clients.admin.AdminClientConfig:371)
[2024-12-16 22:08:40,061] INFO Adding admin resources to main listener (org.apache.kafka.connect.runtime.rest.RestServer:238)
[2024-12-16 22:08:40,063] INFO These configurations '[config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, group.id, rest.advertised.port, plugin.path, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, value.converter.schemas.enable, offset.storage.replication.factor, offset.storage.topic, value.converter, key.converter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig:380)
[2024-12-16 22:08:40,070] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,070] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,070] INFO Kafka startTimeMs: 1734361720070 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,126] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session:334)
[2024-12-16 22:08:40,126] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session:339)
[2024-12-16 22:08:40,127] INFO node0 Scavenging every 600000ms (org.eclipse.jetty.server.session:132)
[2024-12-16 22:08:40,148] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-offsets
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:08:40,179] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:40,243] INFO These configurations '[group.id, rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:08:40,244] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,245] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,245] INFO Kafka startTimeMs: 1734361720244 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,262] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-offsets
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = mongo-source-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-16 22:08:40,288] INFO [Producer clientId=mongo-source-cluster-offsets] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:40,299] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:40,376] INFO These configurations '[rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-16 22:08:40,381] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,381] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,381] INFO Kafka startTimeMs: 1734361720377 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,407] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:40,417] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Assigned to partition(s): connect-offsets-0, connect-offsets-5, connect-offsets-10, connect-offsets-20, connect-offsets-15, connect-offsets-9, connect-offsets-11, connect-offsets-4, connect-offsets-16, connect-offsets-17, connect-offsets-3, connect-offsets-24, connect-offsets-23, connect-offsets-13, connect-offsets-18, connect-offsets-22, connect-offsets-8, connect-offsets-2, connect-offsets-12, connect-offsets-19, connect-offsets-14, connect-offsets-1, connect-offsets-6, connect-offsets-7, connect-offsets-21 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2024-12-16 22:08:40,422] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,422] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-5 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,423] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-10 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-20 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-15 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-9 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-11 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,425] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-16 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,426] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-17 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,426] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,426] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-24 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,426] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-23 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,426] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-13 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,427] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-18 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,427] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-22 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,428] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-8 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,435] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,435] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-12 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,436] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-19 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,438] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-14 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,438] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,439] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-6 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,439] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-7 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,439] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-offsets-21 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,527] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-10 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,528] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-8 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,529] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-14 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,529] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-12 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,529] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,530] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,530] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-6 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,530] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,531] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-24 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,531] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-18 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,531] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-16 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,535] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-22 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,537] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-20 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,537] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-9 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,537] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-7 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,538] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-13 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,538] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-11 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,538] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,538] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-5 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,539] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,539] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-23 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,539] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-17 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,540] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-15 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,540] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-21 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,540] INFO [Consumer clientId=mongo-source-cluster-offsets, groupId=mongo-source-cluster] Resetting offset for partition connect-offsets-19 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,755] INFO Finished reading KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2024-12-16 22:08:40,755] INFO Started KafkaBasedLog for topic connect-offsets (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2024-12-16 22:08:40,756] INFO Finished reading offsets topic and starting KafkaOffsetBackingStore (org.apache.kafka.connect.storage.KafkaOffsetBackingStore:249)
[2024-12-16 22:08:40,757] INFO Worker started (org.apache.kafka.connect.runtime.Worker:243)
[2024-12-16 22:08:40,757] INFO Starting KafkaBasedLog with topic connect-status reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2024-12-16 22:08:40,769] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-statuses
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:08:40,770] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:40,788] INFO These configurations '[group.id, rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:08:40,788] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,788] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,788] INFO Kafka startTimeMs: 1734361720788 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,789] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-statuses
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = mongo-source-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-16 22:08:40,790] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:40,812] INFO [Producer clientId=mongo-source-cluster-statuses] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:40,817] INFO These configurations '[rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-16 22:08:40,834] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:40,834] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:40,834] INFO Kafka startTimeMs: 1734361720817 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:40,851] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:40,864] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Assigned to partition(s): connect-status-0, connect-status-4, connect-status-1, connect-status-2, connect-status-3 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2024-12-16 22:08:40,865] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-status-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,865] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-status-4 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,865] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-status-1 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,865] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-status-2 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,865] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-status-3 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:40,889] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting offset for partition connect-status-1 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,889] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting offset for partition connect-status-2 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,890] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting offset for partition connect-status-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,890] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting offset for partition connect-status-3 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:40,890] INFO [Consumer clientId=mongo-source-cluster-statuses, groupId=mongo-source-cluster] Resetting offset for partition connect-status-4 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:41,058] INFO Finished reading KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2024-12-16 22:08:41,058] INFO Started KafkaBasedLog for topic connect-status (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2024-12-16 22:08:41,062] INFO Starting KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:378)
[2024-12-16 22:08:41,062] INFO Starting KafkaBasedLog with topic connect-configs reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog:254)
[2024-12-16 22:08:41,147] INFO ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-configs
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:08:41,149] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:41,167] INFO These configurations '[group.id, rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:08:41,168] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:41,168] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:41,168] INFO Kafka startTimeMs: 1734361721167 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:41,168] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = earliest
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = mongo-source-cluster-configs
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	enable.metrics.push = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = mongo-source-cluster
	group.instance.id = null
	group.protocol = classic
	group.remote.assignor = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
 (org.apache.kafka.clients.consumer.ConsumerConfig:371)
[2024-12-16 22:08:41,169] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:41,184] INFO These configurations '[rest.advertised.port, plugin.path, metrics.context.connect.kafka.cluster.id, status.storage.replication.factor, offset.storage.topic, value.converter, key.converter, config.storage.topic, listeners, metrics.context.connect.group.id, rest.advertised.host.name, status.storage.topic, config.storage.replication.factor, offset.flush.interval.ms, key.converter.schemas.enable, value.converter.schemas.enable, offset.storage.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig:380)
[2024-12-16 22:08:41,185] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:41,190] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:41,190] INFO Kafka startTimeMs: 1734361721184 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:41,193] INFO Started o.e.j.s.ServletContextHandler@62b056cb{/,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler:921)
[2024-12-16 22:08:41,196] INFO REST resources initialized; server is started and ready to handle requests (org.apache.kafka.connect.runtime.rest.RestServer:303)
[2024-12-16 22:08:41,197] INFO Kafka Connect started (org.apache.kafka.connect.runtime.Connect:77)
[2024-12-16 22:08:41,201] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:41,202] INFO [Producer clientId=mongo-source-cluster-configs] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:41,204] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Assigned to partition(s): connect-configs-0 (org.apache.kafka.clients.consumer.internals.ClassicKafkaConsumer:579)
[2024-12-16 22:08:41,205] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Seeking to earliest offset of partition connect-configs-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState:715)
[2024-12-16 22:08:41,222] INFO [Consumer clientId=mongo-source-cluster-configs, groupId=mongo-source-cluster] Resetting offset for partition connect-configs-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[172.30.2.207:9092 (id: 0 rack: null)], epoch=6}}. (org.apache.kafka.clients.consumer.internals.SubscriptionState:407)
[2024-12-16 22:08:41,245] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,248] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,248] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,249] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,253] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,255] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,256] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,261] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,263] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,264] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,264] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,265] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,266] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,266] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,267] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,267] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,268] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,269] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,269] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,270] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,270] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,271] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,272] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,272] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,273] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,273] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,274] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,275] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,276] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,276] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,277] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,278] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,278] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,279] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,279] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,280] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,281] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,281] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,282] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,282] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,283] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,283] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,284] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,284] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,285] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,286] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,286] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,287] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,287] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,288] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,289] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,290] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,290] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,291] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,292] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,292] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,293] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,293] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,294] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,295] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,295] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,296] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,297] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,298] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,298] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,299] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,299] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,302] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,302] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,303] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,304] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,305] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:08:41,305] INFO Finished reading KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:311)
[2024-12-16 22:08:41,306] INFO Started KafkaBasedLog for topic connect-configs (org.apache.kafka.connect.util.KafkaBasedLog:313)
[2024-12-16 22:08:41,306] INFO Started KafkaConfigBackingStore (org.apache.kafka.connect.storage.KafkaConfigBackingStore:402)
[2024-12-16 22:08:41,317] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:41,318] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Discovered group coordinator 172.30.2.207:9092 (id: 2147483647 rack: null) (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:937)
[2024-12-16 22:08:41,320] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:08:41,321] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:41,332] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:08:41,336] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=331, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:08:41,364] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=331, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:08:41,364] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 331 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=536, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:08:41,366] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Herder started (org.apache.kafka.connect.runtime.distributed.DistributedHerder:387)
[2024-12-16 22:08:41,366] WARN [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Catching up to assignment's config offset. (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1811)
[2024-12-16 22:08:41,366] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Current config state offset -1 is behind group assignment 536, reading to end of config log (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1884)
[2024-12-16 22:08:41,371] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished reading to end of log and updated config snapshot, new config log offset: 536 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1911)
[2024-12-16 22:08:41,371] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 536 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:08:41,373] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:08:41,378] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mongodb-source-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:08:41,380] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:08:41,381] INFO [mongodb-source-connector|worker] Creating connector mongodb-source-connector of type io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:08:41,385] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:08:41,394] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,381] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:08:41,381] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:08:41,389] INFO [mongodb-source-connector|worker] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:41,403] INFO [mongodb-source-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,405] INFO [mongodb-source-connector|task-0] Creating task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:08:41,410] INFO [mongodb-source-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:08:41,404] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:08:41,413] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:08:41,414] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,416] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,420] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:08:41,423] INFO [mongodb-source-connector|task-0] TaskConfig values: 
	task.class = class io.debezium.connector.mongodb.MongoDbConnectorTask
 (org.apache.kafka.connect.runtime.TaskConfig:371)
[2024-12-16 22:08:41,474] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:08:41,474] INFO [mongodb-source-connector|worker] Instantiated connector mongodb-source-connector with version 3.0.4.Final of type class io.debezium.connector.mongodb.MongoDbConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:08:41,480] INFO [mongodb-source-connector|task-0] Instantiated task mongodb-source-connector-0 with version 3.0.4.Final of type io.debezium.connector.mongodb.MongoDbConnectorTask (org.apache.kafka.connect.runtime.Worker:665)
[2024-12-16 22:08:41,481] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = key
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:41,481] INFO [mongodb-source-connector|task-0] JsonConverterConfig values: 
	converter.type = value
	decimal.format = BASE64
	replace.null.with.default = true
	schemas.cache.size = 1000
	schemas.enable = true
 (org.apache.kafka.connect.json.JsonConverterConfig:371)
[2024-12-16 22:08:41,481] INFO [mongodb-source-connector|task-0] Set up the key converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:680)
[2024-12-16 22:08:41,482] INFO [mongodb-source-connector|task-0] Set up the value converter class org.apache.kafka.connect.json.JsonConverter for task mongodb-source-connector-0 using the connector config (org.apache.kafka.connect.runtime.Worker:686)
[2024-12-16 22:08:41,482] INFO [mongodb-source-connector|task-0] Set up the header converter class org.apache.kafka.connect.storage.SimpleHeaderConverter for task mongodb-source-connector-0 using the worker config (org.apache.kafka.connect.runtime.Worker:691)
[2024-12-16 22:08:41,495] INFO [mongodb-source-connector|worker] Finished creating connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:08:41,499] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:08:41,535] WARN [mongodb-source-connector|task-0] The deleted record handling configs "drop.tombstones" and "delete.handling.mode" have been deprecated, please use "delete.tombstone.handling.mode" instead. (io.debezium.transforms.AbstractExtractNewRecordState:110)
[2024-12-16 22:08:41,563] WARN [mongodb-source-connector|task-0] Configuration key whitelist is deprecated and may be removed in the future.  Please update your configuration to use include instead. (org.apache.kafka.common.utils.ConfigUtils:114)
[2024-12-16 22:08:41,565] INFO [mongodb-source-connector|task-0] Initializing: org.apache.kafka.connect.runtime.TransformationChain{io.debezium.connector.mongodb.transforms.ExtractNewDocumentState, org.apache.kafka.connect.transforms.ReplaceField$Value, org.apache.kafka.connect.transforms.TimestampConverter$Value} (org.apache.kafka.connect.runtime.Worker:1795)
[2024-12-16 22:08:41,576] INFO [mongodb-source-connector|task-0] SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:41,579] INFO [mongodb-source-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,584] INFO [mongodb-source-connector|task-0] ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [172.30.2.147:9092, 172.30.2.207:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = connector-producer-mongodb-source-connector-0
	compression.gzip.level = -1
	compression.lz4.level = 9
	compression.type = none
	compression.zstd.level = 3
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 2147483647
	enable.idempotence = false
	enable.metrics.push = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 9223372036854775807
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metadata.recovery.strategy = none
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.max.ms = 1000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.header.urlencode = false
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
 (org.apache.kafka.clients.producer.ProducerConfig:371)
[2024-12-16 22:08:41,589] INFO [mongodb-source-connector|task-0] initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector:270)
[2024-12-16 22:08:41,659] INFO [mongodb-source-connector|worker] Successfully started MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:67)
[2024-12-16 22:08:41,672] INFO [mongodb-source-connector|task-0] These configurations '[metrics.context.connect.kafka.cluster.id, metrics.context.connect.group.id]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig:380)
[2024-12-16 22:08:41,673] INFO [mongodb-source-connector|task-0] Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser:125)
[2024-12-16 22:08:41,673] INFO [mongodb-source-connector|task-0] Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser:126)
[2024-12-16 22:08:41,674] INFO [mongodb-source-connector|task-0] Kafka startTimeMs: 1734361721673 (org.apache.kafka.common.utils.AppInfoParser:127)
[2024-12-16 22:08:41,706] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Cluster ID: 63zujGc1TGyE2LbXVuVaSg (org.apache.kafka.clients.Metadata:365)
[2024-12-16 22:08:41,719] INFO [mongodb-source-connector|task-0] Starting MongoDbConnectorTask with configuration: (io.debezium.connector.common.BaseSourceTask:245)
[2024-12-16 22:08:41,720] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    connector.class = io.debezium.connector.mongodb.MongoDbConnector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    collection.include.list = test.customers (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    transforms.unwrap.delete.handling.mode = drop (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    transforms.unwrap.array.encoding = array (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    mongodb.connection.string = ******** (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,721] INFO [mongodb-source-connector|task-0]    tasks.max = 1 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms = unwrap,extractNew,convertDate (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.unwrap.add.source.fields = db,collection (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    topic.prefix = fullfillment (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.convertDate.format = yyyy-MM-dd HH:mm:ss (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    mongodb.server.selection.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.unwrap.drop.tombstones = false (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.unwrap.type = io.debezium.connector.mongodb.transforms.ExtractNewDocumentState (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    value.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    mongodb.connect.timeout.ms = 10000 (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    mongodb.auth.source = admin (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    key.converter = org.apache.kafka.connect.json.JsonConverter (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.convertDate.type = org.apache.kafka.connect.transforms.TimestampConverter$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.extractNew.type = org.apache.kafka.connect.transforms.ReplaceField$Value (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    key.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.extractNew.whitelist = id,name,email,age,createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    task.class = io.debezium.connector.mongodb.MongoDbConnectorTask (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    transforms.convertDate.field = createdAt (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,722] INFO [mongodb-source-connector|task-0]    value.converter.schemas.enable = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,723] INFO [mongodb-source-connector|task-0]    name = mongodb-source-connector (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,723] INFO [mongodb-source-connector|task-0]    transforms.convertDate.target.type = string (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,723] INFO [mongodb-source-connector|task-0]    database.include.list = test (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,723] INFO [mongodb-source-connector|task-0]    snapshot.mode = initial (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,723] INFO [mongodb-source-connector|task-0]    transforms.unwrap.operation.header = true (io.debezium.connector.common.BaseSourceTask:247)
[2024-12-16 22:08:41,725] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:08:41,729] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,727] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,736] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,738] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,738] INFO SourceConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SourceConnectorConfig:371)
[2024-12-16 22:08:41,744] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.mongodb.MongoDbConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	exactly.once.support = requested
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mongodb-source-connector
	offsets.storage.topic = null
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topic.creation.groups = []
	transaction.boundary = poll
	transaction.boundary.interval.ms = null
	transforms = [unwrap, extractNew, convertDate]
	transforms.convertDate.field = createdAt
	transforms.convertDate.format = yyyy-MM-dd HH:mm:ss
	transforms.convertDate.negate = false
	transforms.convertDate.predicate = null
	transforms.convertDate.replace.null.with.default = true
	transforms.convertDate.target.type = string
	transforms.convertDate.type = class org.apache.kafka.connect.transforms.TimestampConverter$Value
	transforms.convertDate.unix.precision = milliseconds
	transforms.extractNew.blacklist = null
	transforms.extractNew.exclude = []
	transforms.extractNew.include = []
	transforms.extractNew.negate = false
	transforms.extractNew.predicate = null
	transforms.extractNew.renames = []
	transforms.extractNew.replace.null.with.default = true
	transforms.extractNew.type = class org.apache.kafka.connect.transforms.ReplaceField$Value
	transforms.extractNew.whitelist = [id, name, email, age, createdAt]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:08:41,749] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,756] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,759] INFO [mongodb-source-connector|task-0] Loading the custom topic naming strategy plugin: io.debezium.schema.DefaultTopicNamingStrategy (io.debezium.config.CommonConnectorConfig:1401)
[2024-12-16 22:08:41,759] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,761] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,762] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,890] INFO [mongodb-source-connector|task-0] Found previous partition offset MongoDbPartition [sourcePartition={server_id=fullfillment}]: {sec=1734361270, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==} (io.debezium.connector.common.BaseSourceTask:527)
[2024-12-16 22:08:41,903] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = SignalProcessor (io.debezium.util.Threads:270)
[2024-12-16 22:08:41,918] WARN [mongodb-source-connector|task-0] Found a not connector specific implementation io.debezium.snapshot.lock.NoLockingSupport for lock mode no_locking_support (io.debezium.snapshot.SnapshotLockProvider:82)
[2024-12-16 22:08:41,929] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,931] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,931] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,934] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:41,935] INFO [mongodb-source-connector|task-0] Found existing offset for at {sec=1734361270, ord=1, resume_token=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==} (io.debezium.connector.mongodb.connection.MongoDbConnection:202)
[2024-12-16 22:08:41,961] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,022] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5691a171, com.mongodb.Jep395RecordCodecProvider@415feae4, com.mongodb.KotlinCodecProvider@26839f41]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@5597f160}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:08:42,076] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}, "event": "$$ROOT"}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:08:42,082] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=30929313, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:38 ICT 2024, lastUpdateTimeNanos=68538819527406} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,097] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,108] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,114] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,116] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=6611449, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:38 ICT 2024, lastUpdateTimeNanos=68538876353400} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,121] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,252] INFO [mongodb-source-connector|task-0] Valid resume token present, so no snapshot will be performed' (io.debezium.connector.mongodb.connection.MongoDbConnection:220)
[2024-12-16 22:08:42,269] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = change-event-source-coordinator (io.debezium.util.Threads:270)
[2024-12-16 22:08:42,269] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = blocking-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:08:42,271] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-change-event-source-coordinator (io.debezium.util.Threads:287)
[2024-12-16 22:08:42,271] INFO [mongodb-source-connector|task-0] WorkerSourceTask{id=mongodb-source-connector-0} Source task finished initialization and start (org.apache.kafka.connect.runtime.AbstractWorkerSourceTask:280)
[2024-12-16 22:08:42,277] INFO [mongodb-source-connector|task-0] Metrics registered (io.debezium.pipeline.ChangeEventSourceCoordinator:137)
[2024-12-16 22:08:42,278] INFO [mongodb-source-connector|task-0] Context created (io.debezium.pipeline.ChangeEventSourceCoordinator:140)
[2024-12-16 22:08:42,280] INFO [mongodb-source-connector|task-0] A previous offset indicating a completed snapshot has been found. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:144)
[2024-12-16 22:08:42,280] INFO [mongodb-source-connector|task-0] According to the connector configuration, no snapshot will occur. (io.debezium.connector.mongodb.MongoDbSnapshotChangeEventSource:151)
[2024-12-16 22:08:42,291] INFO [mongodb-source-connector|task-0] Snapshot ended with SnapshotResult [status=SKIPPED, offset=MongoDbOffsetContext [sourceInfo=SourceInfo [initialSync=false, collectionId=null, position=Position [ts=Timestamp{value=7449024934099025921, seconds=1734361270, inc=1}, changeStreamSessionTxnId=null, resumeToken=zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==]]]] (io.debezium.pipeline.ChangeEventSourceCoordinator:298)
[2024-12-16 22:08:42,292] INFO [mongodb-source-connector|task-0] Connected metrics set to 'true' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:08:42,295] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = mongodb named = incremental-snapshot (io.debezium.util.Threads:270)
[2024-12-16 22:08:42,295] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,296] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,296] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,298] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,298] INFO [mongodb-source-connector|task-0] No incremental snapshot in progress, no action needed on start (io.debezium.connector.mongodb.snapshot.MongoDbIncrementalSnapshotChangeEventSource:262)
[2024-12-16 22:08:42,306] INFO [mongodb-source-connector|task-0] SignalProcessor started. Scheduling it every 5000ms (io.debezium.pipeline.signal.SignalProcessor:105)
[2024-12-16 22:08:42,307] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-SignalProcessor (io.debezium.util.Threads:287)
[2024-12-16 22:08:42,307] INFO [mongodb-source-connector|task-0] Starting streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:323)
[2024-12-16 22:08:42,307] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,308] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,308] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,309] INFO [mongodb-source-connector|task-0] Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:42,311] INFO [mongodb-source-connector|task-0] Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,317] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1610761, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:38 ICT 2024, lastUpdateTimeNanos=68539077888910} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,320] INFO [mongodb-source-connector|task-0] MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5691a171, com.mongodb.Jep395RecordCodecProvider@415feae4, com.mongodb.KotlinCodecProvider@26839f41]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@76455860}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:08:42,321] INFO [mongodb-source-connector|task-0] Reading change stream (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:100)
[2024-12-16 22:08:42,325] INFO [mongodb-source-connector|task-0] Effective change stream pipeline: [{"$replaceRoot": {"newRoot": {"namespace": {"$concat": ["$ns.db", ".", "$ns.coll"]}, "event": "$$ROOT"}}}, {"$match": {"$and": [{"$and": [{"event.ns.db": {"$regularExpression": {"pattern": "test", "options": "i"}}}, {"namespace": {"$regularExpression": {"pattern": "test.customers", "options": "i"}}}]}, {"event.operationType": {"$in": ["insert", "update", "replace", "delete"]}}]}}, {"$replaceRoot": {"newRoot": "$event"}}] (io.debezium.connector.mongodb.ChangeStreamPipelineFactory:56)
[2024-12-16 22:08:42,330] INFO [mongodb-source-connector|task-0] Resuming streaming from token 'zQAAAAJfZGF0YQC9AAAAODI2NzYwNDBCNjAwMDAwMDAxMkIwNDJDMDEwMDI5NkU1QTEwMDRGNTNEREUwQzM0RkQ0QzE4QjFFNkYzMjIyREI2OTI3NTQ2M0M2RjcwNjU3MjYxNzQ2OTZGNkU1NDc5NzA2NTAwM0M2OTZFNzM2NTcyNzQwMDQ2NjQ2RjYzNzU2RDY1NkU3NDRCNjU3OTAwNDY2NDVGNjk2NDAwNjQ2NzYwNDBCNjQ5NkY2QjQ1OTk5NjQwMzMwMDAwMDQAAA==' (io.debezium.connector.mongodb.MongoDbStreamingChangeEventSource:207)
[2024-12-16 22:08:42,334] INFO [mongodb-source-connector|task-0] Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,340] INFO [mongodb-source-connector|task-0] Requested thread factory for component MongoDbConnector, id = fullfillment named = replicator-fetcher (io.debezium.util.Threads:270)
[2024-12-16 22:08:42,342] INFO [mongodb-source-connector|task-0] Fetcher submitted for execution: io.debezium.connector.mongodb.events.BufferingChangeStreamCursor$EventFetcher@1ef4168b @ java.util.concurrent.ThreadPoolExecutor@1288201[Running, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0] (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:367)
[2024-12-16 22:08:42,343] INFO [mongodb-source-connector|task-0] Creating thread debezium-mongodbconnector-fullfillment-replicator-fetcher-0 (io.debezium.util.Threads:287)
[2024-12-16 22:08:42,346] INFO [mongodb-source-connector|task-0] Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=1447217, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:38 ICT 2024, lastUpdateTimeNanos=68539106320153} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,347] INFO [mongodb-source-connector|task-0] Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,347] INFO [mongodb-source-connector|task-0] Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,351] INFO [mongodb-source-connector|task-0] No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:42,352] INFO [mongodb-source-connector|task-0] Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,341] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:49,343] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:49,344] INFO Loading the custom source info struct maker plugin: io.debezium.connector.mongodb.MongoDbSourceInfoStructMaker (io.debezium.config.CommonConnectorConfig:1701)
[2024-12-16 22:08:49,345] INFO Adding discovered server 172.30.2.147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,355] INFO MongoClient with metadata {"driver": {"name": "mongo-java-driver|sync", "version": "4.11.0"}, "os": {"type": "Linux", "name": "Linux", "architecture": "amd64", "version": "5.15.0-125-generic"}, "platform": "Java/Ubuntu/17.0.13+11-Ubuntu-2ubuntu122.04"} created with settings MongoClientSettings{readPreference=primary, writeConcern=WriteConcern{w=null, wTimeout=null ms, journal=null}, retryWrites=true, retryReads=true, readConcern=ReadConcern{level=null}, credential=MongoCredential{mechanism=null, userName='root', source='admin', password=<hidden>, mechanismProperties=<hidden>}, transportSettings=null, streamFactoryFactory=null, commandListeners=[], codecRegistry=ProvidersCodecRegistry{codecProviders=[ValueCodecProvider{}, BsonValueCodecProvider{}, DBRefCodecProvider{}, DBObjectCodecProvider{}, DocumentCodecProvider{}, CollectionCodecProvider{}, IterableCodecProvider{}, MapCodecProvider{}, GeoJsonCodecProvider{}, GridFSFileCodecProvider{}, Jsr310CodecProvider{}, JsonObjectCodecProvider{}, BsonCodecProvider{}, EnumCodecProvider{}, com.mongodb.client.model.mql.ExpressionCodecProvider@5691a171, com.mongodb.Jep395RecordCodecProvider@415feae4, com.mongodb.KotlinCodecProvider@26839f41]}, loggerSettings=LoggerSettings{maxDocumentLength=1000}, clusterSettings={hosts=[172.30.2.147:27017], srvServiceName=mongodb, mode=MULTIPLE, requiredClusterType=REPLICA_SET, requiredReplicaSetName='rs0', serverSelector='null', clusterListeners='[]', serverSelectionTimeout='10000 ms', localThreshold='15 ms'}, socketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=0, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, heartbeatSocketSettings=SocketSettings{connectTimeoutMS=10000, readTimeoutMS=10000, receiveBufferSize=0, proxySettings=ProxySettings{host=null, port=null, username=null, password=null}}, connectionPoolSettings=ConnectionPoolSettings{maxSize=100, minSize=0, maxWaitTimeMS=120000, maxConnectionLifeTimeMS=0, maxConnectionIdleTimeMS=0, maintenanceInitialDelayMS=0, maintenanceFrequencyMS=60000, connectionPoolListeners=[], maxConnecting=2}, serverSettings=ServerSettings{heartbeatFrequencyMS=10000, minHeartbeatFrequencyMS=500, serverListeners='[]', serverMonitorListeners='[]'}, sslSettings=SslSettings{enabled=false, invalidHostNameAllowed=false, context=javax.net.ssl.SSLContext@1d0822c6}, applicationName='null', compressorList=[], uuidRepresentation=STANDARD, serverApi=null, autoEncryptionSettings=null, dnsClient=null, inetAddressResolver=null, contextProvider=null} (org.mongodb.driver.client:71)
[2024-12-16 22:08:49,359] INFO Monitor thread successfully connected to server with description ServerDescription{address=172.30.2.147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=8375179, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:48 ICT 2024, lastUpdateTimeNanos=68546119149577} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,362] INFO No server chosen by ReadPreferenceServerSelector{readPreference=primary} from cluster description ClusterDescription{type=REPLICA_SET, connectionMode=MULTIPLE, serverDescriptions=[ServerDescription{address=172.30.2.147:27017, type=UNKNOWN, state=CONNECTING}]}. Waiting for 10000 ms before timing out (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,363] INFO Adding discovered server linux-ip-147:27017 to client view of cluster (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,367] INFO Server 172.30.2.147:27017 is no longer a member of the replica set.  Removing from client view of cluster. (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,369] INFO Discovered replica set primary 172.30.2.147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,371] INFO Monitor thread successfully connected to server with description ServerDescription{address=linux-ip-147:27017, type=REPLICA_SET_PRIMARY, state=CONNECTED, ok=true, minWireVersion=0, maxWireVersion=21, maxDocumentSize=16777216, logicalSessionTimeoutMinutes=30, roundTripTimeNanos=4041058, setName='rs0', canonicalAddress=linux-ip-147:27017, hosts=[linux-ip-147:27017], passives=[], arbiters=[], primary='linux-ip-147:27017', tagSet=TagSet{[]}, electionId=7fffffff0000000000000004, setVersion=1, topologyVersion=TopologyVersion{processId=675f36d97bcf1da49879e510, counter=6}, lastWriteDate=Mon Dec 16 22:08:48 ICT 2024, lastUpdateTimeNanos=68546131519364} (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,374] INFO Discovered replica set primary linux-ip-147:27017 with max election id 7fffffff0000000000000004 and max set version 1 (org.mongodb.driver.cluster:71)
[2024-12-16 22:08:49,418] INFO AbstractConfig values: 
 (org.apache.kafka.common.config.AbstractConfig:371)
[2024-12-16 22:08:49,474] INFO 172.30.2.147 - - [16/Dec/2024:15:08:49 +0000] "POST /connectors HTTP/1.1" 409 80 "-" "curl/7.81.0" 355 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:09:19,644] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2024-12-16 22:09:19,646] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Session key updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2510)
[2024-12-16 22:11:21,890] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:11:21,893] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:11:21,894] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:11:21,895] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:11:21,908] INFO 172.30.2.207 - - [16/Dec/2024:15:11:21 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 112 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:11:21,914] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=332, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:11:21,947] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=332, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:11:21,956] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:11:21,957] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:11:21,961] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:11:21,966] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:11:21,970] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:11:21,970] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:11:21,990] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:11:21,991] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 332 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=540, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:11:22,015] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 540 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:11:22,016] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:11:22,017] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:11:22,019] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:11:22,029] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=333, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:11:22,037] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=333, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:11:22,040] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 333 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=540, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:11:22,041] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 540 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:11:22,042] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:11:30,254] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:11:30,262] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:11:30,262] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:11:30,270] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=334, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:11:30,280] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=334, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:11:30,280] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 334 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=541, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:11:30,282] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 541 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:11:30,285] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:11:30,286] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:11:30,289] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:11:30,292] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:11:30,293] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:11:30,294] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:11:30,306] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:11:30,312] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:11:30,315] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:11:30,371] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:11:30,382] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:11:30,383] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:11:30,390] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=335, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:11:30,400] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=335, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:11:30,400] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 335 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=543, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:11:30,401] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 543 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:11:30,402] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:11:30,403] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:11:30,406] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:11:30,410] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:11:30,413] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:11:30,428] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:11:30,480] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:11:30,483] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:11:30,484] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:11:30,484] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:11:30,484] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:11:30,485] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:11:30,492] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=336, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:11:30,497] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=336, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:11:30,497] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 336 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=545, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:11:30,498] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 545 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:11:30,499] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:11:30,500] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:11:30,502] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:11:30,508] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:11:30,511] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:11:30,523] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:29:16,551] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:29:16,553] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:29:16,558] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:29:16,559] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:29:16,562] INFO 172.30.2.207 - - [16/Dec/2024:15:29:16 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 81 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:29:16,570] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=337, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:29:16,580] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=337, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:29:16,581] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:29:16,582] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:29:16,582] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:29:16,583] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:29:16,591] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:29:16,596] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:29:16,605] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:29:16,606] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 337 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=547, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:29:16,612] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 547 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:29:16,615] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:29:16,616] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:29:16,617] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:29:16,626] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=338, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:29:16,639] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=338, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:29:16,640] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 338 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=547, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:29:16,643] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 547 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:29:16,643] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:29:59,473] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:29:59,484] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:29:59,485] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:29:59,502] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=339, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:29:59,525] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=339, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:29:59,526] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 339 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=548, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:29:59,527] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 548 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:29:59,528] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:29:59,530] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:29:59,538] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:29:59,543] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:29:59,548] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:29:59,549] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:29:59,557] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:29:59,560] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:29:59,568] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:29:59,627] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:29:59,657] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:29:59,658] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:29:59,659] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:29:59,672] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=340, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:29:59,677] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=340, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:29:59,678] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 340 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=552, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:29:59,679] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 552 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:29:59,680] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:29:59,681] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:29:59,683] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:29:59,685] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:29:59,689] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:29:59,694] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:38:25,098] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:38:25,104] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:38:25,105] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:38:25,106] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:38:25,113] INFO 172.30.2.207 - - [16/Dec/2024:15:38:25 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 82 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:38:25,116] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=341, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:38:25,123] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=341, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:38:25,124] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:38:25,124] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:38:25,125] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:38:25,125] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:38:25,245] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:38:25,249] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:38:25,250] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:38:25,252] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 341 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=554, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:38:25,252] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 554 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:38:25,253] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:38:25,253] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:38:25,253] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:38:25,260] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=342, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:38:25,266] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=342, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:38:25,267] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 342 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=554, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:38:25,270] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 554 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:38:25,270] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:38:27,015] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:38:27,016] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:38:27,017] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:38:27,025] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=343, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:38:27,034] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=343, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:38:27,035] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 343 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=555, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:38:27,037] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 555 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:38:27,038] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:38:27,040] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:38:27,050] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:38:27,054] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:38:27,061] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:38:27,066] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:38:27,068] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:38:27,070] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:38:27,073] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:38:27,134] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:38:27,138] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:38:27,138] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:38:27,155] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=344, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:38:27,155] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:38:27,165] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=344, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:38:27,166] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 344 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=558, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:38:27,167] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 558 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:38:27,167] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:38:27,168] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:38:27,171] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:38:27,175] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:38:27,187] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:38:27,199] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:38:27,200] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:38:27,201] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:38:27,201] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:38:27,202] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:38:27,202] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:38:27,211] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=345, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:38:27,224] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=345, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:38:27,225] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 345 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=559, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:38:27,226] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 559 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:38:27,226] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:38:27,227] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:38:27,229] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:38:27,233] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:38:27,241] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:38:27,254] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:48:25,374] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:48:25,376] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:48:25,382] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:48:25,383] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:48:25,383] INFO 172.30.2.207 - - [16/Dec/2024:15:48:25 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 86 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:48:25,393] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=346, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:48:25,405] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=346, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:48:25,406] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:48:25,407] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:48:25,407] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:48:25,408] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:48:25,417] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:48:25,418] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:48:25,436] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:48:25,437] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 346 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=561, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:48:25,439] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 561 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:48:25,439] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:48:25,439] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:48:25,440] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:48:25,445] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=347, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:48:25,453] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=347, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:48:25,454] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 347 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=561, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:48:25,457] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 561 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:48:25,457] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:48:27,418] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:48:27,421] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:48:27,421] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:48:27,428] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=348, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:48:27,439] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=348, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:48:27,440] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 348 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=562, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:48:27,440] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 562 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:48:27,441] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:48:27,442] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:48:27,446] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:48:27,457] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:48:27,465] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:48:27,467] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:48:27,470] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:48:27,472] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:48:27,477] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:48:27,537] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:48:27,538] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:48:27,538] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:48:27,546] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=349, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:48:27,551] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=349, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:48:27,552] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 349 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=564, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:48:27,553] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 564 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:48:27,553] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:48:27,555] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:48:27,557] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:48:27,560] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:48:27,564] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:48:27,570] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:48:27,587] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:48:27,588] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:48:27,588] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:48:27,588] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:48:27,588] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:48:27,589] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:48:27,594] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=350, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:48:27,601] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=350, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:48:27,602] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 350 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=566, connectorIds=[mysql-sink-connector, mongodb-source-connector], taskIds=[mysql-sink-connector-0, mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:48:27,603] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 566 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:48:27,604] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:48:27,605] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:48:27,607] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:48:27,614] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:48:27,622] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:48:27,627] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:16,900] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:56:16,902] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:56:16,902] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:16,908] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:16,908] INFO 172.30.2.207 - - [16/Dec/2024:15:56:16 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 73 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:56:16,920] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=351, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:16,940] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=351, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:16,941] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:56:16,942] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:56:16,947] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:56:16,952] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:56:16,952] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:56:16,953] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:56:16,956] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:56:16,958] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 351 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=568, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:16,964] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 568 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:16,965] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:16,966] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:16,967] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:16,982] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=352, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:16,988] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=352, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:16,990] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 352 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=568, connectorIds=[mongodb-source-connector], taskIds=[mongodb-source-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:16,991] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 568 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:16,992] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:26,840] INFO Successfully processed removal of connector 'mongodb-source-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:56:26,841] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mongodb-source-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:56:26,868] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:26,868] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:26,874] INFO 172.30.2.147 - - [16/Dec/2024:15:56:26 +0000] "DELETE /connectors/mongodb-source-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 56 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:56:26,877] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=353, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:26,884] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=353, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:26,885] INFO [mongodb-source-connector|worker] Stopping connector mongodb-source-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:56:26,885] INFO [mongodb-source-connector|worker] Scheduled shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:56:26,886] INFO [mongodb-source-connector|worker] Stopping MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:82)
[2024-12-16 22:56:26,887] INFO [mongodb-source-connector|worker] Stopped MongoDB connector (io.debezium.connector.mongodb.MongoDbConnector:86)
[2024-12-16 22:56:26,888] INFO [mongodb-source-connector|task-0] Stopping task mongodb-source-connector-0 (org.apache.kafka.connect.runtime.Worker:1048)
[2024-12-16 22:56:26,891] INFO [mongodb-source-connector|worker] Completed shutdown for WorkerConnector{id=mongodb-source-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:56:27,111] INFO [mongodb-source-connector|task-0] Stopping down connector (io.debezium.connector.common.BaseSourceTask:432)
[2024-12-16 22:56:27,536] INFO [mongodb-source-connector|task-0] Awaiting fetcher thread termination (io.debezium.connector.mongodb.events.BufferingChangeStreamCursor:457)
[2024-12-16 22:56:28,479] INFO [mongodb-source-connector|task-0] Finished streaming (io.debezium.pipeline.ChangeEventSourceCoordinator:325)
[2024-12-16 22:56:28,480] INFO [mongodb-source-connector|task-0] Connected metrics set to 'false' (io.debezium.pipeline.ChangeEventSourceCoordinator:477)
[2024-12-16 22:56:28,486] INFO [mongodb-source-connector|task-0] SignalProcessor stopped (io.debezium.pipeline.signal.SignalProcessor:127)
[2024-12-16 22:56:28,488] INFO [mongodb-source-connector|task-0] Debezium ServiceRegistry stopped. (io.debezium.service.DefaultServiceRegistry:105)
[2024-12-16 22:56:28,491] INFO [mongodb-source-connector|task-0] [Producer clientId=connector-producer-mongodb-source-connector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1382)
[2024-12-16 22:56:28,516] INFO [mongodb-source-connector|task-0] Metrics scheduler closed (org.apache.kafka.common.metrics.Metrics:685)
[2024-12-16 22:56:28,517] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.metrics.JmxReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:56:28,518] INFO [mongodb-source-connector|task-0] Closing reporter org.apache.kafka.common.telemetry.internals.ClientTelemetryReporter (org.apache.kafka.common.metrics.Metrics:689)
[2024-12-16 22:56:28,518] INFO [mongodb-source-connector|task-0] Metrics reporters closed (org.apache.kafka.common.metrics.Metrics:695)
[2024-12-16 22:56:28,521] INFO [mongodb-source-connector|task-0] App info kafka.producer for connector-producer-mongodb-source-connector-0 unregistered (org.apache.kafka.common.utils.AppInfoParser:89)
[2024-12-16 22:56:28,533] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:56:28,537] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:56:28,538] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 353 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=570, connectorIds=[], taskIds=[], revokedConnectorIds=[mongodb-source-connector], revokedTaskIds=[mongodb-source-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:28,539] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 570 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:28,540] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:28,540] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:28,541] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:28,546] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=354, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:28,556] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=354, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:28,556] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 354 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=570, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:28,558] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 570 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:28,558] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:39,392] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2448)
[2024-12-16 22:56:39,400] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:39,400] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:39,420] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=355, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:39,432] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=355, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:39,433] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 355 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=571, connectorIds=[mysql-sink-connector], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:39,435] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 571 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:39,438] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connector mysql-sink-connector (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2097)
[2024-12-16 22:56:39,440] INFO [mysql-sink-connector|worker] Creating connector mysql-sink-connector of type io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:313)
[2024-12-16 22:56:39,445] INFO [mysql-sink-connector|worker] SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:56:39,449] INFO [mysql-sink-connector|worker] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:56:39,451] INFO [mysql-sink-connector|worker] Instantiated connector mysql-sink-connector with version 3.0.4.Final of type class io.debezium.connector.jdbc.JdbcSinkConnector (org.apache.kafka.connect.runtime.Worker:335)
[2024-12-16 22:56:39,451] INFO [mysql-sink-connector|worker] Finished creating connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:356)
[2024-12-16 22:56:39,453] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:39,461] INFO SinkConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.SinkConnectorConfig:371)
[2024-12-16 22:56:39,466] INFO EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.deadletterqueue.context.headers.enable = false
	errors.deadletterqueue.topic.name = 
	errors.deadletterqueue.topic.replication.factor = 3
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	topics = [fullfillment.test.customers]
	topics.regex = 
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:56:39,524] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:56:39,534] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:39,534] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:39,537] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=356, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:39,542] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=356, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:39,543] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 356 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=574, connectorIds=[mysql-sink-connector], taskIds=[mysql-sink-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:39,543] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 574 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:39,544] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:56:39,545] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:56:39,547] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:56:39,548] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:56:39,555] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:56:39,557] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Tasks [mysql-sink-connector-0] configs updated (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2467)
[2024-12-16 22:56:39,565] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:56:39,565] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Handling task config update by stopping tasks [mysql-sink-connector-0], which will be restarted after rebalance if still assigned to this worker (org.apache.kafka.connect.runtime.distributed.DistributedHerder:794)
[2024-12-16 22:56:39,566] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:56:39,566] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:56:39,566] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:56:39,566] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:56:39,572] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=357, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:56:39,580] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=357, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:56:39,580] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 357 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=575, connectorIds=[mysql-sink-connector], taskIds=[mysql-sink-connector-0], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:56:39,581] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 575 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:56:39,581] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2022)
[2024-12-16 22:56:39,582] INFO [mysql-sink-connector|task-0] Creating task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:646)
[2024-12-16 22:56:39,584] INFO [mysql-sink-connector|task-0] ConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig:371)
[2024-12-16 22:56:39,585] INFO [mysql-sink-connector|task-0] EnrichedConnectorConfig values: 
	config.action.reload = restart
	connector.class = io.debezium.connector.jdbc.JdbcSinkConnector
	errors.log.enable = false
	errors.log.include.messages = false
	errors.retry.delay.max.ms = 60000
	errors.retry.timeout = 0
	errors.tolerance = none
	header.converter = null
	key.converter = class org.apache.kafka.connect.json.JsonConverter
	name = mysql-sink-connector
	predicates = []
	tasks.max = 1
	tasks.max.enforce = true
	transforms = [unwrap]
	transforms.unwrap.add.fields = []
	transforms.unwrap.add.fields.prefix = __
	transforms.unwrap.add.headers = []
	transforms.unwrap.add.headers.prefix = __
	transforms.unwrap.array.encoding = array
	transforms.unwrap.delete.handling.mode = drop
	transforms.unwrap.drop.tombstones = false
	transforms.unwrap.flatten.struct = false
	transforms.unwrap.flatten.struct.delimiter = _
	transforms.unwrap.negate = false
	transforms.unwrap.predicate = null
	transforms.unwrap.route.by.field = 
	transforms.unwrap.type = class io.debezium.connector.mongodb.transforms.ExtractNewDocumentState
	value.converter = class org.apache.kafka.connect.json.JsonConverter
 (org.apache.kafka.connect.runtime.ConnectorConfig$EnrichedConnectorConfig:371)
[2024-12-16 22:56:39,586] ERROR [mysql-sink-connector|task-0] Failed to start task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:707)
java.lang.NoClassDefFoundError: org/hibernate/SharedSessionContract
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:467)
	at org.apache.kafka.common.utils.Utils.loadClass(Utils.java:435)
	at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:769)
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:146)
	at org.apache.kafka.connect.runtime.TaskConfig.<init>(TaskConfig.java:51)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:662)
	at org.apache.kafka.connect.runtime.Worker.startSinkTask(Worker.java:569)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:2026)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.lambda$getTaskStartingCallable$40(DistributedHerder.java:2076)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.lang.ClassNotFoundException: org.hibernate.SharedSessionContract
	at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)
	at org.apache.kafka.connect.runtime.isolation.PluginClassLoader.loadClass(PluginClassLoader.java:124)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)
	... 17 more
[2024-12-16 22:56:39,595] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:57:24,488] INFO Successfully processed removal of connector 'mysql-sink-connector' (org.apache.kafka.connect.storage.KafkaConfigBackingStore:1005)
[2024-12-16 22:57:24,490] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Connector mysql-sink-connector config removed (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2432)
[2024-12-16 22:57:24,490] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:57:24,491] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:57:24,498] INFO 172.30.2.207 - - [16/Dec/2024:15:57:24 +0000] "DELETE /connectors/mysql-sink-connector HTTP/1.1" 204 0 "-" "curl/7.81.0" 74 (org.apache.kafka.connect.runtime.rest.RestServer:62)
[2024-12-16 22:57:24,498] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=358, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:57:24,509] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=358, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:57:24,510] INFO [mysql-sink-connector|worker] Stopping connector mysql-sink-connector (org.apache.kafka.connect.runtime.Worker:452)
[2024-12-16 22:57:24,511] INFO [mysql-sink-connector|worker] Scheduled shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:295)
[2024-12-16 22:57:24,510] WARN [mysql-sink-connector|task-0] Ignoring stop request for unowned task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1044)
[2024-12-16 22:57:24,512] WARN [mysql-sink-connector|task-0] Ignoring await stop request for non-present task mysql-sink-connector-0 (org.apache.kafka.connect.runtime.Worker:1070)
[2024-12-16 22:57:24,516] INFO [mysql-sink-connector|worker] Completed shutdown for WorkerConnector{id=mysql-sink-connector} (org.apache.kafka.connect.runtime.WorkerConnector:315)
[2024-12-16 22:57:24,517] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished stopping tasks in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2735)
[2024-12-16 22:57:24,519] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished flushing status backing store in preparation for rebalance (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2756)
[2024-12-16 22:57:24,519] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 358 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=577, connectorIds=[], taskIds=[], revokedConnectorIds=[mysql-sink-connector], revokedTaskIds=[mysql-sink-connector-0], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:57:24,522] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 577 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:57:24,522] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
[2024-12-16 22:57:24,522] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Rebalance started (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:243)
[2024-12-16 22:57:24,523] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] (Re-)joining group (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:605)
[2024-12-16 22:57:24,531] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully joined group with generation Generation{generationId=359, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:666)
[2024-12-16 22:57:24,547] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Successfully synced group in generation Generation{generationId=359, memberId='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', protocol='sessioned'} (org.apache.kafka.connect.runtime.distributed.WorkerCoordinator:843)
[2024-12-16 22:57:24,548] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Joined group at generation 359 with protocol version 2 and got assignment: Assignment{error=0, leader='connect-172.30.2.147:8083-423e807c-b1a6-4424-b8a4-454b167b0254', leaderUrl='http://172.30.2.147:8083/', offset=577, connectorIds=[], taskIds=[], revokedConnectorIds=[], revokedTaskIds=[], delay=0} with rebalance delay: 0 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2648)
[2024-12-16 22:57:24,549] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Starting connectors and tasks using config offset 577 (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1979)
[2024-12-16 22:57:24,557] INFO [Worker clientId=connect-172.30.2.147:8083, groupId=mongo-source-cluster] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:2008)
